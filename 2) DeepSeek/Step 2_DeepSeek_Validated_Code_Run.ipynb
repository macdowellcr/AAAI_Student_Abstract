{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a04e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'is_sm_ips_ports' unique values: [0 1]\n",
      "Column 'is_ftp_login' unique values: [0 1 2 4]\n",
      "Warning: Column 'is_ftp_login' contains unexpected values: [0 1 2 4]\n",
      "Column 'ct_ftp_cmd' unique values: [0 1 2 4]\n",
      "Warning: Column 'ct_ftp_cmd' contains unexpected values: [0 1 2 4]\n",
      "Column 'dbytes' has 120288 zero values (46.68%)\n",
      "Column 'dload' has 1 zero values (0.00%)\n",
      "\n",
      "Final dataset shape: (257673, 40)\n",
      "\n",
      "Column dtypes:\n",
      " dur                  float64\n",
      "spkts                  int64\n",
      "dpkts                  int64\n",
      "sbytes                 int64\n",
      "dbytes                 int64\n",
      "rate                 float64\n",
      "sttl                   int64\n",
      "dttl                   int64\n",
      "sload                float64\n",
      "dload                float64\n",
      "sloss                  int64\n",
      "dloss                  int64\n",
      "sinpkt               float64\n",
      "dinpkt               float64\n",
      "sjit                 float64\n",
      "djit                 float64\n",
      "swin                   int64\n",
      "stcpb                float64\n",
      "dtcpb                float64\n",
      "dwin                   int64\n",
      "tcprtt               float64\n",
      "synack               float64\n",
      "ackdat               float64\n",
      "smean                  int64\n",
      "dmean                  int64\n",
      "trans_depth            int64\n",
      "response_body_len      int64\n",
      "ct_srv_src             int64\n",
      "ct_state_ttl           int64\n",
      "ct_dst_ltm             int64\n",
      "ct_src_dport_ltm       int64\n",
      "ct_dst_sport_ltm       int64\n",
      "ct_dst_src_ltm         int64\n",
      "is_ftp_login           int64\n",
      "ct_ftp_cmd             int64\n",
      "ct_flw_http_mthd       int64\n",
      "ct_src_ltm             int64\n",
      "ct_srv_dst             int64\n",
      "is_sm_ips_ports        int64\n",
      "label                  int64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      " dur                  0\n",
      "spkts                0\n",
      "dpkts                0\n",
      "sbytes               0\n",
      "dbytes               0\n",
      "rate                 0\n",
      "sttl                 0\n",
      "dttl                 0\n",
      "sload                0\n",
      "dload                0\n",
      "sloss                0\n",
      "dloss                0\n",
      "sinpkt               0\n",
      "dinpkt               0\n",
      "sjit                 0\n",
      "djit                 0\n",
      "swin                 0\n",
      "stcpb                0\n",
      "dtcpb                0\n",
      "dwin                 0\n",
      "tcprtt               0\n",
      "synack               0\n",
      "ackdat               0\n",
      "smean                0\n",
      "dmean                0\n",
      "trans_depth          0\n",
      "response_body_len    0\n",
      "ct_srv_src           0\n",
      "ct_state_ttl         0\n",
      "ct_dst_ltm           0\n",
      "ct_src_dport_ltm     0\n",
      "ct_dst_sport_ltm     0\n",
      "ct_dst_src_ltm       0\n",
      "is_ftp_login         0\n",
      "ct_ftp_cmd           0\n",
      "ct_flw_http_mthd     0\n",
      "ct_src_ltm           0\n",
      "ct_srv_dst           0\n",
      "is_sm_ips_ports      0\n",
      "label                0\n",
      "dtype: int64\n",
      "\n",
      "First 5 rows of cleaned data:\n",
      "        dur  spkts  dpkts  sbytes  dbytes      rate  sttl  dttl     sload  \\\n",
      "0 -0.006232      2      0     496       0  0.703801   254     0  2.245592   \n",
      "1 -0.006237      2      0    1762       0  0.976595   254     0  9.992239   \n",
      "2 -0.006241      2      0    1068       0  1.576743   254     0  9.992239   \n",
      "3 -0.006240      2      0     900       0  1.310011   254     0  7.491854   \n",
      "4 -0.006234      2      0    2126       0  0.776546   254     0  9.992239   \n",
      "\n",
      "     dload  ...  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \\\n",
      "0 -0.07905  ...                 1                 1               2   \n",
      "1 -0.07905  ...                 1                 1               2   \n",
      "2 -0.07905  ...                 1                 1               3   \n",
      "3 -0.07905  ...                 2                 1               3   \n",
      "4 -0.07905  ...                 2                 1               3   \n",
      "\n",
      "   is_ftp_login  ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  \\\n",
      "0             0           0                 0           1           2   \n",
      "1             0           0                 0           1           2   \n",
      "2             0           0                 0           1           3   \n",
      "3             0           0                 0           2           3   \n",
      "4             0           0                 0           2           3   \n",
      "\n",
      "   is_sm_ips_ports  label  \n",
      "0                0      0  \n",
      "1                0      0  \n",
      "2                0      0  \n",
      "3                0      0  \n",
      "4                0      0  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from scipy.stats import mstats  # For winsorization (capping extreme values)\n",
    "\n",
    "# Load the dataset from the specified path\n",
    "df = pd.read_csv(r\"E:\\Datasets\\UNSW-NB15\\Training and Testing Sets\\UNSW_NB15_concatenated_dropped.csv\")\n",
    "\n",
    "# Create a copy of the dataframe to preserve original data\n",
    "df_clean = df.copy()\n",
    "\n",
    "### Step 1: Drop the 'id' column\n",
    "# Rationale: The 'id' column is a near-unique identifier that doesn't contribute to predictive power\n",
    "# and could cause model overfitting to specific observations. This is a standard preprocessing step\n",
    "# for ML datasets with record identifiers.\n",
    "df_clean.drop(columns=['id'], inplace=True)\n",
    "\n",
    "### Step 2: Harmonize target variables\n",
    "# Rationale: Prevent data leakage by keeping only one target variable. 'attack_cat' contains\n",
    "# multiclass attack categories while 'label' contains binary classification (normal vs attack).\n",
    "# We choose 'label' for binary classification to maintain consistency with common network\n",
    "# intrusion detection approaches.\n",
    "df_clean.drop(columns=['attack_cat'], inplace=True)\n",
    "\n",
    "### Step 3: Handle extreme values using winsorization\n",
    "# Rationale: Cap extreme values to reduce skewness while preserving outlier information.\n",
    "# Winsorization is preferred over removal for network traffic data where extreme values\n",
    "# may represent legitimate but rare network events. Top and bottom 1% capping is a common\n",
    "# approach that balances outlier mitigation with data preservation.\n",
    "# Define columns with extreme values based on EDA findings\n",
    "extreme_value_cols = ['sload', 'dload', 'dur', 'rate', 'sinpkt', 'dinpkt', \n",
    "                     'sjit', 'djit', 'stcpb', 'dtcpb']\n",
    "\n",
    "# Winsorize top and bottom 1% of values for each specified column\n",
    "# Note: mstats.winsorize returns a masked array, so we extract the underlying data\n",
    "for col in extreme_value_cols:\n",
    "    df_clean[col] = mstats.winsorize(df_clean[col], limits=[0.01, 0.01]).data\n",
    "\n",
    "### Step 4: Scale high-cardinality continuous features\n",
    "# Rationale: Standardize feature scales to prevent dominance by large-magnitude features.\n",
    "# Use RobustScaler due to presence of remaining outliers after winsorization. RobustScaler\n",
    "# uses median and IQR instead of mean and standard deviation, making it more resistant to\n",
    "# outliers that may still exist after winsorization.\n",
    "high_cardinality_features = [\n",
    "    'dur', 'rate', 'sload', 'dload', 'sinpkt', 'dinpkt',\n",
    "    'sjit', 'djit', 'stcpb', 'dtcpb'\n",
    "]\n",
    "\n",
    "# Initialize RobustScaler (less sensitive to outliers than StandardScaler)\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Apply scaling to specified features\n",
    "df_clean[high_cardinality_features] = scaler.fit_transform(df_clean[high_cardinality_features])\n",
    "\n",
    "### Step 5: Validate binary columns\n",
    "# Rationale: Ensure binary columns contain expected values and are properly formatted.\n",
    "# Network traffic binary flags should only contain 0/1 values. Converting to integer\n",
    "# ensures proper handling by machine learning algorithms.\n",
    "binary_columns = ['is_sm_ips_ports', 'is_ftp_login', 'ct_ftp_cmd']\n",
    "\n",
    "for col in binary_columns:\n",
    "    # Check unique values and data type\n",
    "    unique_vals = df_clean[col].unique()\n",
    "    print(f\"Column '{col}' unique values: {unique_vals}\")\n",
    "    \n",
    "    # Validate that only 0 and 1 values are present\n",
    "    if set(unique_vals).issubset({0, 1}):\n",
    "        # Convert to integer if not already (ensures numerical representation)\n",
    "        df_clean[col] = df_clean[col].astype(int)\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' contains unexpected values: {unique_vals}\")\n",
    "\n",
    "### Step 6: Validate zero-value handling\n",
    "# Rationale: Confirm zeros are legitimate values (not missing data) in network metrics.\n",
    "# In network traffic data, zero values often represent one-way communication or missing\n",
    "# responses, which are legitimate patterns in network behavior.\n",
    "zero_value_columns = ['dbytes', 'dload']\n",
    "\n",
    "for col in zero_value_columns:\n",
    "    zero_count = (df_clean[col] == 0).sum()\n",
    "    print(f\"Column '{col}' has {zero_count} zero values ({(zero_count/len(df_clean))*100:.2f}%)\")\n",
    "\n",
    "# No imputation needed - zeros represent valid one-way network traffic patterns\n",
    "\n",
    "### Final Data Validation\n",
    "print(\"\\nFinal dataset shape:\", df_clean.shape)\n",
    "print(\"\\nColumn dtypes:\\n\", df_clean.dtypes)\n",
    "print(\"\\nMissing values per column:\\n\", df_clean.isnull().sum())\n",
    "print(\"\\nFirst 5 rows of cleaned data:\")\n",
    "print(df_clean.head())\n",
    "\n",
    "# The dataframe df_clean is now prepared for machine learning\n",
    "# Features are scaled, extreme values are capped, and data types are validated\n",
    "# Note: Target variable 'label' remains unchanged for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e19eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned DataFrame exported to 'cleaned_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Export the cleaned dataframe to a new CSV file\n",
    "df_final.to_csv('cleaned_data.csv', index=False)\n",
    "\n",
    "print(\"Cleaned DataFrame exported to 'cleaned_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_supervised_20250810",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

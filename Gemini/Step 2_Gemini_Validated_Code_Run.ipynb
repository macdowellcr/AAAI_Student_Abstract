{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a186bdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Data Load ---\n",
      "Successfully loaded the dataset from: E:\\Datasets\\UNSW-NB15\\Training and Testing Sets\\UNSW_NB15_concatenated_dropped.csv\n",
      "Initial dataset shape: (257673, 42) (rows, columns)\n",
      "\n",
      "\n",
      "--- Step 1: Handling Duplicate Records ---\n",
      "Identified and removed 0 fully duplicate rows.\n",
      "Dataset shape after removing duplicates: (257673, 42)\n",
      "\n",
      "\n",
      "--- Step 2: Selecting Target Variable ---\n",
      "Dropped the redundant 'label' column to prevent data leakage.\n",
      "Dataset shape after dropping 'label': (257673, 41)\n",
      "\n",
      "\n",
      "--- Step 3: Removing Identifier Column ---\n",
      "Dropped the 'id' column as it has no predictive value.\n",
      "Dataset shape after dropping 'id': (257673, 40)\n",
      "\n",
      "\n",
      "--- Step 4: Identifying Feature Types ---\n",
      "Identified 32 numerical features.\n",
      "Identified 7 categorical features.\n",
      "\n",
      "\n",
      "--- Steps 5 & 6: Encoding Categorical and Scaling Numerical Features ---\n",
      "Original number of features: 39\n",
      "Number of features after one-hot encoding and scaling: 82\n",
      "\n",
      "\n",
      "--- Finalizing the Preprocessed DataFrame ---\n",
      "Successfully created the final preprocessed DataFrame.\n",
      "Final DataFrame shape: (257673, 83)\n",
      "\n",
      "--- Sample of the Final Preprocessed Data ---\n",
      "        dur     spkts     dpkts    sbytes    dbytes      rate     sload  \\\n",
      "0 -0.208678 -0.130765 -0.165331 -0.046480 -0.098409 -0.002151  0.590935   \n",
      "1 -0.208679 -0.130765 -0.165331 -0.039194 -0.098409  0.210460  4.363255   \n",
      "2 -0.208679 -0.130765 -0.165331 -0.043188 -0.098409  0.678204  4.220037   \n",
      "3 -0.208679 -0.130765 -0.165331 -0.044155 -0.098409  0.470318  2.850314   \n",
      "4 -0.208678 -0.130765 -0.165331 -0.037100 -0.098409  0.054546  4.198501   \n",
      "\n",
      "     dload     sloss     dloss  ...  ct_flw_http_mthd_2  ct_flw_http_mthd_3  \\\n",
      "0 -0.27285 -0.074561 -0.125576  ...                 0.0                 0.0   \n",
      "1 -0.27285 -0.074561 -0.125576  ...                 0.0                 0.0   \n",
      "2 -0.27285 -0.074561 -0.125576  ...                 0.0                 0.0   \n",
      "3 -0.27285 -0.074561 -0.125576  ...                 0.0                 0.0   \n",
      "4 -0.27285 -0.074561 -0.125576  ...                 0.0                 0.0   \n",
      "\n",
      "   ct_flw_http_mthd_4  ct_flw_http_mthd_6  ct_flw_http_mthd_9  \\\n",
      "0                 0.0                 0.0                 0.0   \n",
      "1                 0.0                 0.0                 0.0   \n",
      "2                 0.0                 0.0                 0.0   \n",
      "3                 0.0                 0.0                 0.0   \n",
      "4                 0.0                 0.0                 0.0   \n",
      "\n",
      "   ct_flw_http_mthd_12  ct_flw_http_mthd_16  ct_flw_http_mthd_25  \\\n",
      "0                  0.0                  0.0                  0.0   \n",
      "1                  0.0                  0.0                  0.0   \n",
      "2                  0.0                  0.0                  0.0   \n",
      "3                  0.0                  0.0                  0.0   \n",
      "4                  0.0                  0.0                  0.0   \n",
      "\n",
      "   ct_flw_http_mthd_30  attack_cat  \n",
      "0                  0.0      Normal  \n",
      "1                  0.0      Normal  \n",
      "2                  0.0      Normal  \n",
      "3                  0.0      Normal  \n",
      "4                  0.0      Normal  \n",
      "\n",
      "[5 rows x 83 columns]\n",
      "\n",
      "--- Info of the Final Preprocessed Data ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 257673 entries, 0 to 257672\n",
      "Data columns (total 83 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   dur                  257673 non-null  float64\n",
      " 1   spkts                257673 non-null  float64\n",
      " 2   dpkts                257673 non-null  float64\n",
      " 3   sbytes               257673 non-null  float64\n",
      " 4   dbytes               257673 non-null  float64\n",
      " 5   rate                 257673 non-null  float64\n",
      " 6   sload                257673 non-null  float64\n",
      " 7   dload                257673 non-null  float64\n",
      " 8   sloss                257673 non-null  float64\n",
      " 9   dloss                257673 non-null  float64\n",
      " 10  sinpkt               257673 non-null  float64\n",
      " 11  dinpkt               257673 non-null  float64\n",
      " 12  sjit                 257673 non-null  float64\n",
      " 13  djit                 257673 non-null  float64\n",
      " 14  swin                 257673 non-null  float64\n",
      " 15  stcpb                257673 non-null  float64\n",
      " 16  dtcpb                257673 non-null  float64\n",
      " 17  dwin                 257673 non-null  float64\n",
      " 18  tcprtt               257673 non-null  float64\n",
      " 19  synack               257673 non-null  float64\n",
      " 20  ackdat               257673 non-null  float64\n",
      " 21  smean                257673 non-null  float64\n",
      " 22  dmean                257673 non-null  float64\n",
      " 23  trans_depth          257673 non-null  float64\n",
      " 24  response_body_len    257673 non-null  float64\n",
      " 25  ct_srv_src           257673 non-null  float64\n",
      " 26  ct_dst_ltm           257673 non-null  float64\n",
      " 27  ct_src_dport_ltm     257673 non-null  float64\n",
      " 28  ct_dst_sport_ltm     257673 non-null  float64\n",
      " 29  ct_dst_src_ltm       257673 non-null  float64\n",
      " 30  ct_src_ltm           257673 non-null  float64\n",
      " 31  ct_srv_dst           257673 non-null  float64\n",
      " 32  sttl_0               257673 non-null  float64\n",
      " 33  sttl_1               257673 non-null  float64\n",
      " 34  sttl_29              257673 non-null  float64\n",
      " 35  sttl_30              257673 non-null  float64\n",
      " 36  sttl_31              257673 non-null  float64\n",
      " 37  sttl_32              257673 non-null  float64\n",
      " 38  sttl_60              257673 non-null  float64\n",
      " 39  sttl_62              257673 non-null  float64\n",
      " 40  sttl_63              257673 non-null  float64\n",
      " 41  sttl_64              257673 non-null  float64\n",
      " 42  sttl_252             257673 non-null  float64\n",
      " 43  sttl_254             257673 non-null  float64\n",
      " 44  sttl_255             257673 non-null  float64\n",
      " 45  dttl_0               257673 non-null  float64\n",
      " 46  dttl_29              257673 non-null  float64\n",
      " 47  dttl_30              257673 non-null  float64\n",
      " 48  dttl_31              257673 non-null  float64\n",
      " 49  dttl_32              257673 non-null  float64\n",
      " 50  dttl_60              257673 non-null  float64\n",
      " 51  dttl_252             257673 non-null  float64\n",
      " 52  dttl_253             257673 non-null  float64\n",
      " 53  dttl_254             257673 non-null  float64\n",
      " 54  ct_state_ttl_0       257673 non-null  float64\n",
      " 55  ct_state_ttl_1       257673 non-null  float64\n",
      " 56  ct_state_ttl_2       257673 non-null  float64\n",
      " 57  ct_state_ttl_3       257673 non-null  float64\n",
      " 58  ct_state_ttl_4       257673 non-null  float64\n",
      " 59  ct_state_ttl_5       257673 non-null  float64\n",
      " 60  ct_state_ttl_6       257673 non-null  float64\n",
      " 61  is_sm_ips_ports_0    257673 non-null  float64\n",
      " 62  is_sm_ips_ports_1    257673 non-null  float64\n",
      " 63  is_ftp_login_0       257673 non-null  float64\n",
      " 64  is_ftp_login_1       257673 non-null  float64\n",
      " 65  is_ftp_login_2       257673 non-null  float64\n",
      " 66  is_ftp_login_4       257673 non-null  float64\n",
      " 67  ct_ftp_cmd_0         257673 non-null  float64\n",
      " 68  ct_ftp_cmd_1         257673 non-null  float64\n",
      " 69  ct_ftp_cmd_2         257673 non-null  float64\n",
      " 70  ct_ftp_cmd_4         257673 non-null  float64\n",
      " 71  ct_flw_http_mthd_0   257673 non-null  float64\n",
      " 72  ct_flw_http_mthd_1   257673 non-null  float64\n",
      " 73  ct_flw_http_mthd_2   257673 non-null  float64\n",
      " 74  ct_flw_http_mthd_3   257673 non-null  float64\n",
      " 75  ct_flw_http_mthd_4   257673 non-null  float64\n",
      " 76  ct_flw_http_mthd_6   257673 non-null  float64\n",
      " 77  ct_flw_http_mthd_9   257673 non-null  float64\n",
      " 78  ct_flw_http_mthd_12  257673 non-null  float64\n",
      " 79  ct_flw_http_mthd_16  257673 non-null  float64\n",
      " 80  ct_flw_http_mthd_25  257673 non-null  float64\n",
      " 81  ct_flw_http_mthd_30  257673 non-null  float64\n",
      " 82  attack_cat           257673 non-null  object \n",
      "dtypes: float64(82), object(1)\n",
      "memory usage: 163.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the file path for the dataset.\n",
    "# IMPORTANT: This path must be updated to match the location of the dataset on your local machine.\n",
    "FILE_PATH = \"E:\\\\Datasets\\\\UNSW-NB15\\\\Training and Testing Sets\\\\UNSW_NB15_concatenated_dropped.csv\"\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Load the dataset from the specified CSV file into a pandas DataFrame.\n",
    "# A try-except block is used to gracefully handle the case where the file is not found,\n",
    "# preventing the script from crashing and providing a clear error message to the user.\n",
    "try:\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "    print(\"--- Initial Data Load ---\")\n",
    "    print(f\"Successfully loaded the dataset from: {FILE_PATH}\")\n",
    "    print(f\"Initial dataset shape: {df.shape} (rows, columns)\")\n",
    "    print(\"\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at the specified path: {FILE_PATH}\")\n",
    "    print(\"Please update the FILE_PATH variable with the correct location of your dataset.\")\n",
    "    # Exit the script if the file cannot be loaded, as subsequent steps cannot proceed.\n",
    "    exit()\n",
    "\n",
    "# --- Data Cleaning and Preprocessing Plan Implementation ---\n",
    "\n",
    "# **Step 1: Handle Duplicate Records**\n",
    "# Objective: Remove rows that are exact copies of each other.\n",
    "# Rationale: Duplicate records can introduce bias into a machine learning model, causing it\n",
    "# to overweight the patterns present in those duplicated samples. Removing them ensures that\n",
    "# each data point is unique and contributes independently to the model's training.\n",
    "print(\"--- Step 1: Handling Duplicate Records ---\")\n",
    "initial_rows = df.shape[0]\n",
    "# The pandas `drop_duplicates()` method identifies and removes rows that are identical across all columns.\n",
    "# `inplace=True` modifies the DataFrame directly, saving memory by not creating a new object.\n",
    "df.drop_duplicates(inplace=True)\n",
    "remaining_rows = df.shape[0]\n",
    "print(f\"Identified and removed {initial_rows - remaining_rows} fully duplicate rows.\")\n",
    "print(f\"Dataset shape after removing duplicates: {df.shape}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# **Step 2: Select and Prepare the Target Variable**\n",
    "# Objective: Isolate the desired target variable ('attack_cat') and remove redundant or leaky columns.\n",
    "# Rationale: The 'label' column (binary: 0 or 1) is a direct derivative of 'attack_cat' (multi-class).\n",
    "# For a multi-class classification task, 'attack_cat' is the correct target. Keeping 'label' in the\n",
    "# feature set would represent a \"data leak,\" giving the model a perfect predictor and leading to\n",
    "# unrealistically high performance that would not generalize to new data.\n",
    "print(\"--- Step 2: Selecting Target Variable ---\")\n",
    "if 'label' in df.columns:\n",
    "    # Drop the 'label' column from the DataFrame.\n",
    "    df.drop(columns=['label'], inplace=True)\n",
    "    print(\"Dropped the redundant 'label' column to prevent data leakage.\")\n",
    "    print(f\"Dataset shape after dropping 'label': {df.shape}\")\n",
    "else:\n",
    "    print(\"'label' column not found, skipping.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# **Step 3: Remove Identifier Column**\n",
    "# Objective: Remove columns that serve as identifiers but have no predictive value.\n",
    "# Rationale: The 'id' column is a unique identifier for each row (like a primary key). It contains\n",
    "# no information about the nature of the network traffic itself and would only add noise if included\n",
    "# as a feature in the model. It is standard practice to remove such identifiers.\n",
    "print(\"--- Step 3: Removing Identifier Column ---\")\n",
    "if 'id' in df.columns:\n",
    "    # Drop the 'id' column from the DataFrame.\n",
    "    df.drop(columns=['id'], inplace=True)\n",
    "    print(\"Dropped the 'id' column as it has no predictive value.\")\n",
    "    print(f\"Dataset shape after dropping 'id': {df.shape}\")\n",
    "else:\n",
    "    print(\"'id' column not found, skipping.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# **Step 4: Re-evaluate and Correct Data Types (Feature Identification)**\n",
    "# Objective: Systematically identify and separate features into numerical and categorical types.\n",
    "# Rationale: Machine learning algorithms require numerical input. Therefore, we must apply different\n",
    "# preprocessing techniques based on a feature's data type. Numerical features need scaling, while\n",
    "# categorical features need encoding. This step is crucial for setting up the transformation pipeline.\n",
    "print(\"--- Step 4: Identifying Feature Types ---\")\n",
    "\n",
    "# Separate the features (independent variables, X) from the target (dependent variable, y).\n",
    "# This is a standard and necessary step before any feature transformation.\n",
    "X = df.drop(columns=['attack_cat'])\n",
    "y = df['attack_cat']\n",
    "\n",
    "# Define which columns are categorical based on their data type ('object') and domain knowledge.\n",
    "# Some features are encoded as integers but represent distinct categories rather than a continuous\n",
    "# scale (e.g., 'sttl', 'dttl'). These must be explicitly identified and treated as categorical.\n",
    "categorical_features = [\n",
    "    'proto', 'service', 'state',  # Object types, clearly categorical\n",
    "    'sttl', 'dttl', 'ct_state_ttl', 'is_sm_ips_ports', 'is_ftp_login',\n",
    "    'ct_ftp_cmd', 'ct_flw_http_mthd' # Integer types that represent categories\n",
    "]\n",
    "# A robust check to ensure all features listed above are actually present in the DataFrame's columns.\n",
    "# This prevents errors if the input data changes or a column was already removed.\n",
    "categorical_features = [col for col in categorical_features if col in X.columns]\n",
    "\n",
    "# Identify numerical features by taking all columns from X that were NOT identified as categorical.\n",
    "# This is an efficient way to partition the feature set.\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "print(f\"Identified {len(numerical_features)} numerical features.\")\n",
    "print(f\"Identified {len(categorical_features)} categorical features.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# **Steps 5 & 6: Encode Categorical Features and Scale Numerical Features**\n",
    "# Objective: Apply transformations to convert all features into a suitable numerical format for ML models.\n",
    "# Rationale: We use scikit-learn's ColumnTransformer to create a single, unified preprocessing\n",
    "# pipeline. This is highly efficient and less error-prone than transforming feature subsets manually.\n",
    "#   - One-Hot Encoding (Step 5): Converts categorical features into a binary vector format. This\n",
    "#     prevents the model from assuming any ordinal relationship between categories.\n",
    "#   - Standard Scaling (Step 6): Transforms numerical features to have a mean of 0 and a standard\n",
    "#     deviation of 1. This is crucial for algorithms sensitive to feature scales, such as SVMs,\n",
    "#     Logistic Regression, and Neural Networks, ensuring all features contribute fairly to the result.\n",
    "print(\"--- Steps 5 & 6: Encoding Categorical and Scaling Numerical Features ---\")\n",
    "\n",
    "# Define the transformer for numerical features. StandardScaler is a robust choice.\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# Define the transformer for categorical features.\n",
    "# `handle_unknown='ignore'` is a critical parameter that prevents errors if the model encounters\n",
    "# a category in new data that it didn't see during training. It will encode that new category as all zeros.\n",
    "# `sparse_output=False` ensures the output is a standard NumPy array, which is easier to work with.\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Create the ColumnTransformer. This object applies specified transformers to specified columns.\n",
    "# The 'remainder' parameter is set to 'passthrough', which ensures that any columns not explicitly\n",
    "# handled by the transformers are kept in the dataset. In our case, all columns are handled.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Apply the defined transformations to the feature set X.\n",
    "# The `fit_transform` method first learns the parameters from the data (e.g., mean/std for scaling,\n",
    "# unique categories for encoding) and then applies the transformation.\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# --- Reconstruct the Processed DataFrame ---\n",
    "# The output of the ColumnTransformer is a NumPy array, which lacks column names.\n",
    "# We reconstruct a pandas DataFrame to maintain readability and for easier analysis.\n",
    "\n",
    "# Get the new column names generated by the OneHotEncoder.\n",
    "# `get_feature_names_out` creates meaningful names like 'proto_tcp', 'proto_udp', etc.\n",
    "encoded_cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine the original numerical feature names with the new one-hot encoded feature names.\n",
    "all_feature_names = numerical_features + list(encoded_cat_feature_names)\n",
    "\n",
    "# Create the new DataFrame containing the processed features.\n",
    "# We use the original index from X to ensure correct alignment when we later combine it with the target series y.\n",
    "X_processed_df = pd.DataFrame(X_processed, columns=all_feature_names, index=X.index)\n",
    "\n",
    "print(f\"Original number of features: {len(X.columns)}\")\n",
    "print(f\"Number of features after one-hot encoding and scaling: {len(X_processed_df.columns)}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Final Preprocessed DataFrame ---\n",
    "# Combine the processed features (X_processed_df) and the original target (y)\n",
    "# into a single, fully preprocessed DataFrame ready for machine learning.\n",
    "print(\"--- Finalizing the Preprocessed DataFrame ---\")\n",
    "# `pd.concat` is used to join the two DataFrames column-wise (`axis=1`).\n",
    "# Because we preserved the index throughout the process, we can be confident that\n",
    "# each row of features correctly aligns with its corresponding target value.\n",
    "df_processed = pd.concat([X_processed_df, y], axis=1)\n",
    "\n",
    "print(\"Successfully created the final preprocessed DataFrame.\")\n",
    "print(f\"Final DataFrame shape: {df_processed.shape}\")\n",
    "print(\"\\n--- Sample of the Final Preprocessed Data ---\")\n",
    "print(df_processed.head())\n",
    "print(\"\\n--- Info of the Final Preprocessed Data ---\")\n",
    "# .info() provides a concise summary, confirming data types are all numeric and there are no missing values.\n",
    "df_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12e1b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned DataFrame exported to 'cleaned_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Export the cleaned dataframe to a new CSV file\n",
    "df_processed.to_csv('cleaned_data.csv', index=False)\n",
    "\n",
    "print(\"Cleaned DataFrame exported to 'cleaned_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_supervised_20250810",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

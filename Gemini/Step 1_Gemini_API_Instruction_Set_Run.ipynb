{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6166e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini API configured successfully.\n",
      "Dataset loaded successfully. Shape: (257673, 42)\n",
      "Models with generateContent capability (truncated list): ['models/gemini-1.5-pro-latest', 'models/gemini-1.5-pro-002', 'models/gemini-1.5-pro', 'models/gemini-1.5-flash-latest', 'models/gemini-1.5-flash', 'models/gemini-1.5-flash-002', 'models/gemini-1.5-flash-8b', 'models/gemini-1.5-flash-8b-001'] ...\n",
      "--- Chain Step 1: Initial Analysis ---\n",
      "[analysis] Attempting with model: gemini-2.5-pro\n",
      "[analysis] Success model=gemini-2.5-pro attempt=1 time=23.85s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Analysis Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Analytical Insights\n",
       "### Initial Exploratory Data Analysis (EDA)\n",
       "\n",
       "The dataset represents network traffic data, likely for an intrusion detection system. It contains 42 columns and 257,673 rows.\n",
       "\n",
       "-   **Target Variables**: The dataset includes two potential target variables: `label` (binary, 0 for normal, 1 for attack) and `attack_cat` (a multi-class categorical variable specifying the type of attack, e.g., 'Normal', 'Generic', 'Exploits'). `label` appears to be a direct derivative of `attack_cat`.\n",
       "-   **Data Types**: The data is predominantly numerical (`int64`, `float64`), with `attack_cat` being the sole `object` type. There are no missing values across the entire dataset.\n",
       "-   **Identifier Column**: The `id` column is intended as an identifier but is not unique, with 175,341 unique values for 257,673 records. This indicates the presence of over 82,000 duplicate `id`s, which requires investigation.\n",
       "-   **Feature Categories**:\n",
       "    -   **Continuous Features**: Many columns like `dur` (duration), `rate`, `sload` (source load), and `dload` (destination load) are continuous and exhibit a wide range of values, including scientific notation in the sample, suggesting high variance and potential skewness.\n",
       "    -   **Count/Discrete Features**: Columns like `spkts` (source packets), `sbytes` (source bytes), and various `ct_*` (count) features are discrete numerical values.\n",
       "    -   **Categorical & Binary Features**: Several integer columns have a very low number of unique values, indicating they are categorical or binary. Examples include `sttl` (13 unique values), `dttl` (9), `is_sm_ips_ports` (2), and `is_ftp_login` (4). The `is_ftp_login` having 4 unique values is unusual for a binary-sounding name and may represent different states of a login attempt.\n",
       "-   **Data Distribution**: The sample data suggests a potential pattern where attack traffic (`label`=1) often has zero values for destination-related metrics (`dpkts`, `dbytes`, `dload`, `dinpkt`), which could be a strong predictive signal.\n",
       "\n",
       "### Unique Value Counts\n",
       "\n",
       "| Column | Unique Values | Column | Unique Values |\n",
       "| :--- | :--- | :--- | :--- |\n",
       "| id | 175341 | dtcpb | 114187 |\n",
       "| dur | 109945 | dwin | 19 |\n",
       "| spkts | 646 | tcprtt | 63878 |\n",
       "| dpkts | 627 | synack | 57366 |\n",
       "| sbytes | 9382 | ackdat | 53248 |\n",
       "| dbytes | 8653 | smean | 1377 |\n",
       "| rate | 115763 | dmean | 1362 |\n",
       "| sttl | 13 | trans_depth | 14 |\n",
       "| dttl | 9 | response_body_len | 2819 |\n",
       "| sload | 121356 | ct_srv_src | 57 |\n",
       "| dload | 116380 | ct_state_ttl | 7 |\n",
       "| sloss | 490 | ct_dst_ltm | 52 |\n",
       "| dloss | 476 | ct_src_dport_ltm | 52 |\n",
       "| sinpkt | 114318 | ct_dst_sport_ltm | 35 |\n",
       "| dinpkt | 110270 | ct_dst_src_ltm | 58 |\n",
       "| sjit | 117101 | is_ftp_login | 4 |\n",
       "| djit | 114861 | ct_ftp_cmd | 4 |\n",
       "| swin | 22 | ct_flw_http_mthd | 11 |\n",
       "| stcpb | 114473 | ct_src_ltm | 52 |\n",
       "| is_sm_ips_ports | 2 | ct_srv_dst | 57 |\n",
       "| attack_cat | 10 | label | 2 |\n",
       "\n",
       "### High Cardinality Columns\n",
       "\n",
       "High cardinality columns are identified as those with a very large number of unique values, making them unsuitable for direct use as categorical features. These are typically identifiers or continuous variables.\n",
       "\n",
       "| Column | Unique Values | Justification |\n",
       "| :--- | :--- | :--- |\n",
       "| id | 175341 | Identifier column, not a feature for modeling. |\n",
       "| dur | 109945 | Continuous feature representing time duration. |\n",
       "| rate | 115763 | Continuous feature representing packet rate. |\n",
       "| sload | 121356 | Continuous feature representing source load. |\n",
       "| dload | 116380 | Continuous feature representing destination load. |\n",
       "| sinpkt | 114318 | Continuous feature (source inter-packet arrival time). |\n",
       "| dinpkt | 110270 | Continuous feature (destination inter-packet arrival time). |\n",
       "| sjit | 117101 | Continuous feature (source jitter). |\n",
       "| djit | 114861 | Continuous feature (destination jitter). |\n",
       "| stcpb | 114473 | Continuous feature (source TCP base sequence number). |\n",
       "| dtcpb | 114187 | Continuous feature (destination TCP base sequence number). |\n",
       "| tcprtt | 63878 | Continuous feature (TCP round trip time). |\n",
       "| synack | 57366 | Continuous feature (SYN-ACK time). |\n",
       "| ackdat | 53248 | Continuous feature (ACK-DAT time). |\n",
       "\n",
       "### Data Quality Analysis\n",
       "\n",
       "| Dimension | Assessment |\n",
       "| :--- | :--- |\n",
       "| **Accuracy** | The data appears plausible for network traffic metrics, which can span very large ranges (e.g., `sload`). However, without an external source of truth or data dictionary, the absolute accuracy of these measurements cannot be verified. |\n",
       "| **Completeness** | The dataset is **100% complete**, with no missing values reported in any column. This is excellent for modeling. |\n",
       "| **Consistency** | The `label` and `attack_cat` columns appear consistent (`attack_cat`='Normal' corresponds to `label`=0, and any other category to `label`=1). However, there are potential inconsistencies to investigate, such as records where packet counts (`spkts`, `dpkts`) are zero but byte counts (`sbytes`, `dbytes`) are non-zero. |\n",
       "| **Uniqueness** | There is a significant issue with uniqueness. The `id` column contains **82,332 duplicate values**. This suggests either data duplication or that `id` represents a session or host that can have multiple records. This must be investigated and resolved. |\n",
       "| **Validity** | Data types are appropriate for all columns. Most values fall within expected logical ranges. The column `is_ftp_login` has 4 unique values, which contradicts a simple binary (0/1) interpretation and requires clarification. The `attack_cat` column contains 10 distinct and valid-looking string categories. |\n",
       "| **Timeliness** | The dataset lacks any timestamp or date-related columns, making it **impossible to assess the timeliness** or recency of the data. |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chain Step 2: Cleaning Plan ---\n",
      "[plan] Attempting with model: gemini-2.5-pro\n",
      "[plan] Success model=gemini-2.5-pro attempt=1 time=21.61s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Cleaning Plan Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Data Cleaning Steps\n",
       "\n",
       "This plan outlines the ordered steps for cleaning and preprocessing the network traffic dataset to prepare it for machine learning modeling.\n",
       "\n",
       "*   **1. Handle Duplicate Records**\n",
       "    *   **Objective**: Investigate and resolve the issue of non-unique `id` values to ensure each record is a distinct observation.\n",
       "    *   **Columns Impacted**: All columns, with a focus on the `id` column.\n",
       "    *   **Rationale**: The EDA revealed 82,332 duplicate `id` values. The first step is to determine if these correspond to fully duplicated rows. If entire rows are identical, they represent redundant information and should be removed to prevent data leakage and model bias. If only the `id` is duplicated across different records, it confirms `id` is not a unique record identifier and should be treated as a session or host identifier.\n",
       "\n",
       "*   **2. Select and Prepare the Target Variable**\n",
       "    *   **Objective**: Define the prediction target for the machine learning model and remove the redundant label column.\n",
       "    *   **Columns Impacted**: `label`, `attack_cat`.\n",
       "    *   **Rationale**: The `label` column is a binary representation of the multi-class `attack_cat` column. Keeping both is redundant. For a more granular prediction task (identifying the *type* of attack), `attack_cat` should be the target. We will drop the `label` column to avoid redundancy and data leakage. The `attack_cat` column will then be prepared for modeling (e.g., via label encoding).\n",
       "\n",
       "*   **3. Remove Identifier Column**\n",
       "    *   **Objective**: Remove the `id` column from the feature set.\n",
       "    *   **Columns Impacted**: `id`.\n",
       "    *   **Rationale**: The `id` column serves as an identifier and provides no predictive value for the model. Including it would add noise and could lead the model to learn spurious correlations. After investigating its role in Step 1, it should be dropped.\n",
       "\n",
       "*   **4. Re-evaluate and Correct Data Types**\n",
       "    *   **Objective**: Identify integer-based columns that are truly categorical and ensure all features are correctly typed for subsequent preprocessing.\n",
       "    *   **Columns Impacted**: `sttl`, `dttl`, `ct_state_ttl`, `is_sm_ips_ports`, `is_ftp_login`, `ct_ftp_cmd`, `ct_flw_http_mthd`, and other `ct_*` columns with low cardinality.\n",
       "    *   **Rationale**: Several columns with integer types represent discrete categories rather than continuous or ordinal values (e.g., `ct_state_ttl` represents different states). Correctly identifying these as categorical is crucial for applying the appropriate encoding strategy (like one-hot encoding) instead of incorrectly treating them as numerical quantities.\n",
       "\n",
       "*   **5. Encode Categorical Features**\n",
       "    *   **Objective**: Convert all identified categorical features into a numerical format suitable for machine learning algorithms.\n",
       "    *   **Columns Impacted**: All columns identified as categorical in the previous step.\n",
       "    *   **Rationale**: Most machine learning models cannot process string or categorical data directly. One-hot encoding is the proposed method, as it creates new binary columns for each category level, preventing the model from assuming a false ordinal relationship between categories. This is suitable for the low-cardinality categorical features in this dataset.\n",
       "\n",
       "*   **6. Scale Numerical Features**\n",
       "    *   **Objective**: Normalize the range of all continuous numerical features.\n",
       "    *   **Columns Impacted**: All non-categorical, non-identifier, and non-target columns (e.g., `dur`, `rate`, `sload`, `dload`, `sbytes`, `dbytes`).\n",
       "    *   **Rationale**: Numerical features in the dataset have vastly different scales and distributions (e.g., `dur` vs. `sload`). Scaling them to a common range (e.g., using `StandardScaler` or `RobustScaler`) is essential for distance-based algorithms (like SVMs, k-NN) and gradient-based algorithms (like logistic regression, neural networks) to converge effectively and prevent features with larger scales from dominating the model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chain Step 3: Code Generation ---\n",
      "[code] Attempting with model: gemini-2.5-pro\n",
      "[code] Success model=gemini-2.5-pro attempt=1 time=34.79s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Data Cleaning Code Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Data Cleaning Code\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
       "from sklearn.compose import ColumnTransformer\n",
       "\n",
       "# --- Configuration ---\n",
       "# Set the file path for the dataset.\n",
       "# IMPORTANT: Ensure this path is correct for your local environment.\n",
       "FILE_PATH = \"E:\\\\Datasets\\\\UNSW-NB15\\\\Training and Testing Sets\\\\UNSW_NB15_concatenated_dropped.csv\"\n",
       "\n",
       "# --- Data Loading ---\n",
       "# Load the dataset from the specified CSV file into a pandas DataFrame.\n",
       "# We use a try-except block to handle potential FileNotFoundError.\n",
       "try:\n",
       "    df = pd.read_csv(FILE_PATH)\n",
       "    print(\"--- Initial Data Load ---\")\n",
       "    print(f\"Successfully loaded the dataset from: {FILE_PATH}\")\n",
       "    print(f\"Initial dataset shape: {df.shape}\")\n",
       "    print(\"\\n\")\n",
       "except FileNotFoundError:\n",
       "    print(f\"Error: The file was not found at the specified path: {FILE_PATH}\")\n",
       "    print(\"Please update the FILE_PATH variable with the correct location of your dataset.\")\n",
       "    # Exit the script if the file cannot be loaded.\n",
       "    exit()\n",
       "\n",
       "# --- Data Cleaning and Preprocessing Plan Implementation ---\n",
       "\n",
       "# **Step 1: Handle Duplicate Records**\n",
       "# Objective: Remove fully duplicated rows to ensure data integrity and prevent model bias.\n",
       "# Rationale: The EDA identified duplicate 'id' values. We first check for and remove\n",
       "# any rows that are identical across all columns, as they represent redundant information.\n",
       "print(\"--- Step 1: Handling Duplicate Records ---\")\n",
       "initial_rows = df.shape[0]\n",
       "# The `id` column was identified as having duplicates. We check for full row duplicates.\n",
       "# The `id` column itself will be dropped later as it's just an identifier.\n",
       "df.drop_duplicates(inplace=True)\n",
       "remaining_rows = df.shape[0]\n",
       "print(f\"Identified and removed {initial_rows - remaining_rows} fully duplicate rows.\")\n",
       "print(f\"Dataset shape after removing duplicates: {df.shape}\")\n",
       "print(\"\\n\")\n",
       "\n",
       "\n",
       "# **Step 2: Select and Prepare the Target Variable**\n",
       "# Objective: Define 'attack_cat' as the target and remove the redundant 'label' column.\n",
       "# Rationale: The 'label' column is a binary indicator (attack or not), while 'attack_cat'\n",
       "# provides the specific type of attack. For a more detailed prediction task, 'attack_cat'\n",
       "# is the preferred target. Keeping 'label' would be redundant and could lead to data leakage.\n",
       "print(\"--- Step 2: Selecting Target Variable ---\")\n",
       "if 'label' in df.columns:\n",
       "    df.drop(columns=['label'], inplace=True)\n",
       "    print(\"Dropped the redundant 'label' column.\")\n",
       "    print(f\"Dataset shape after dropping 'label': {df.shape}\")\n",
       "else:\n",
       "    print(\"'label' column not found, skipping.\")\n",
       "print(\"\\n\")\n",
       "\n",
       "\n",
       "# **Step 3: Remove Identifier Column**\n",
       "# Objective: Remove the 'id' column from the feature set.\n",
       "# Rationale: The 'id' column is a record identifier and holds no predictive power.\n",
       "# Including it in the model would add noise and is against machine learning best practices.\n",
       "print(\"--- Step 3: Removing Identifier Column ---\")\n",
       "if 'id' in df.columns:\n",
       "    df.drop(columns=['id'], inplace=True)\n",
       "    print(\"Dropped the 'id' column.\")\n",
       "    print(f\"Dataset shape after dropping 'id': {df.shape}\")\n",
       "else:\n",
       "    print(\"'id' column not found, skipping.\")\n",
       "print(\"\\n\")\n",
       "\n",
       "\n",
       "# **Step 4: Re-evaluate and Correct Data Types (Feature Identification)**\n",
       "# Objective: Identify numerical and categorical features for subsequent preprocessing.\n",
       "# Rationale: Correctly classifying features is crucial for applying the right\n",
       "# transformation (scaling for numerical, encoding for categorical). Several integer-based\n",
       "# columns represent categories, not continuous values.\n",
       "print(\"--- Step 4: Identifying Feature Types ---\")\n",
       "\n",
       "# Separate features (X) from the target variable (y)\n",
       "X = df.drop(columns=['attack_cat'])\n",
       "y = df['attack_cat']\n",
       "\n",
       "# Identify categorical features based on their data type ('object') and domain knowledge.\n",
       "# The plan specifically calls out several integer-based columns that are categorical in nature.\n",
       "categorical_features = [\n",
       "    'proto', 'service', 'state',  # Object types, clearly categorical\n",
       "    'sttl', 'dttl', 'ct_state_ttl', 'is_sm_ips_ports', 'is_ftp_login',\n",
       "    'ct_ftp_cmd', 'ct_flw_http_mthd' # Integer types that represent categories\n",
       "]\n",
       "# Ensure all listed categorical features are actually present in the dataframe\n",
       "categorical_features = [col for col in categorical_features if col in X.columns]\n",
       "\n",
       "# Identify numerical features by selecting all columns that are not categorical.\n",
       "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
       "\n",
       "print(f\"Identified {len(numerical_features)} numerical features.\")\n",
       "print(f\"Identified {len(categorical_features)} categorical features.\")\n",
       "print(\"\\n\")\n",
       "\n",
       "\n",
       "# **Steps 5 & 6: Encode Categorical Features and Scale Numerical Features**\n",
       "# Objective: Convert all features into a machine-learning-ready numerical format.\n",
       "# Rationale:\n",
       "#   - One-Hot Encoding (Step 5): Converts categorical features into a numerical format\n",
       "#     without implying an ordinal relationship between categories.\n",
       "#   - Scaling (Step 6): Normalizes the range of numerical features, ensuring that\n",
       "#     features with larger scales do not disproportionately influence the model.\n",
       "# We use scikit-learn's ColumnTransformer to apply these steps efficiently and robustly.\n",
       "print(\"--- Steps 5 & 6: Encoding Categorical and Scaling Numerical Features ---\")\n",
       "\n",
       "# Create a preprocessing pipeline for numerical features (Standard Scaling)\n",
       "numeric_transformer = StandardScaler()\n",
       "\n",
       "# Create a preprocessing pipeline for categorical features (One-Hot Encoding)\n",
       "# handle_unknown='ignore' prevents errors if new categories appear in future data.\n",
       "# sparse_output=False ensures the output is a dense numpy array.\n",
       "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
       "\n",
       "# Create a ColumnTransformer to apply different transformations to different columns\n",
       "# This is the standard best practice for handling mixed-type data.\n",
       "preprocessor = ColumnTransformer(\n",
       "    transformers=[\n",
       "        ('num', numeric_transformer, numerical_features),\n",
       "        ('cat', categorical_transformer, categorical_features)\n",
       "    ],\n",
       "    remainder='passthrough' # Keep other columns (if any) untouched\n",
       ")\n",
       "\n",
       "# Apply the transformations to the feature set X\n",
       "# The fit_transform method learns the scaling parameters and encoding schema and then applies them.\n",
       "X_processed = preprocessor.fit_transform(X)\n",
       "\n",
       "# Reconstruct the DataFrame with new column names\n",
       "# Get the new column names from the one-hot encoder\n",
       "encoded_cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
       "# Combine numerical and new categorical feature names\n",
       "all_feature_names = numerical_features + list(encoded_cat_feature_names)\n",
       "\n",
       "# Create the processed features DataFrame\n",
       "X_processed_df = pd.DataFrame(X_processed, columns=all_feature_names, index=X.index)\n",
       "\n",
       "print(f\"Original number of features: {len(X.columns)}\")\n",
       "print(f\"Number of features after one-hot encoding and scaling: {len(X_processed_df.columns)}\")\n",
       "print(\"\\n\")\n",
       "\n",
       "\n",
       "# --- Final Preprocessed DataFrame ---\n",
       "# Combine the processed features (X_processed_df) and the original target (y)\n",
       "# into a single, fully preprocessed DataFrame.\n",
       "print(\"--- Finalizing the Preprocessed DataFrame ---\")\n",
       "df_processed = pd.concat([X_processed_df, y], axis=1)\n",
       "\n",
       "print(\"Successfully created the final preprocessed DataFrame.\")\n",
       "print(f\"Final DataFrame shape: {df_processed.shape}\")\n",
       "print(\"\\n--- Sample of the Final Preprocessed Data ---\")\n",
       "print(df_processed.head())\n",
       "print(\"\\n--- Info of the Final Preprocessed Data ---\")\n",
       "df_processed.info()\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chain Step 4: Code Validation ---\n",
      "[validation] Attempting with model: gemini-2.5-pro\n",
      "[validation] Success model=gemini-2.5-pro attempt=1 time=36.99s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Data Validation Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Excellent, the provided code is well-structured and follows best practices for data preprocessing. It correctly implements the logical steps required to clean the data and prepare it for a machine learning model.\n",
       "\n",
       "No logical errors were found in the original script. The validation process confirmed its correctness. The following version is returned with enhanced, more detailed comments as requested by the directives, providing deeper insight into each step and the rationale behind the choices made.\n",
       "\n",
       "## Data Cleaning Code - Validated\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
       "from sklearn.compose import ColumnTransformer\n",
       "\n",
       "# --- Configuration ---\n",
       "# Set the file path for the dataset.\n",
       "# IMPORTANT: This path must be updated to match the location of the dataset on your local machine.\n",
       "FILE_PATH = \"E:\\\\Datasets\\\\UNSW-NB15\\\\Training and Testing Sets\\\\UNSW_NB15_concatenated_dropped.csv\"\n",
       "\n",
       "# --- Data Loading ---\n",
       "# Load the dataset from the specified CSV file into a pandas DataFrame.\n",
       "# A try-except block is used to gracefully handle the case where the file is not found,\n",
       "# preventing the script from crashing and providing a clear error message to the user.\n",
       "try:\n",
       "    df = pd.read_csv(FILE_PATH)\n",
       "    print(\"--- Initial Data Load ---\")\n",
       "    print(f\"Successfully loaded the dataset from: {FILE_PATH}\")\n",
       "    print(f\"Initial dataset shape: {df.shape} (rows, columns)\")\n",
       "    print(\"\\n\")\n",
       "except FileNotFoundError:\n",
       "    print(f\"Error: The file was not found at the specified path: {FILE_PATH}\")\n",
       "    print(\"Please update the FILE_PATH variable with the correct location of your dataset.\")\n",
       "    # Exit the script if the file cannot be loaded, as subsequent steps cannot proceed.\n",
       "    exit()\n",
       "\n",
       "# --- Data Cleaning and Preprocessing Plan Implementation ---\n",
       "\n",
       "# **Step 1: Handle Duplicate Records**\n",
       "# Objective: Remove rows that are exact copies of each other.\n",
       "# Rationale: Duplicate records can introduce bias into a machine learning model, causing it\n",
       "# to overweight the patterns present in those duplicated samples. Removing them ensures that\n",
       "# each data point is unique and contributes independently to the model's training.\n",
       "print(\"--- Step 1: Handling Duplicate Records ---\")\n",
       "initial_rows = df.shape[0]\n",
       "# The pandas `drop_duplicates()` method identifies and removes rows that are identical across all columns.\n",
       "# `inplace=True` modifies the DataFrame directly, saving memory by not creating a new object.\n",
       "df.drop_duplicates(inplace=True)\n",
       "remaining_rows = df.shape[0]\n",
       "print(f\"Identified and removed {initial_rows - remaining_rows} fully duplicate rows.\")\n",
       "print(f\"Dataset shape after removing duplicates: {df.shape}\")\n",
       "print(\"\\n\")\n",
       "\n",
       "\n",
       "# **Step 2: Select and Prepare the Target Variable**\n",
       "# Objective: Isolate the desired target variable ('attack_cat') and remove redundant or leaky columns.\n",
       "# Rationale: The 'label' column (binary: 0 or 1) is a direct derivative of 'attack_cat' (multi-class).\n",
       "# For a multi-class classification task, 'attack_cat' is the correct target. Keeping 'label' in the\n",
       "# feature set would represent a \"data leak,\" giving the model a perfect predictor and leading to\n",
       "# unrealistically high performance that would not generalize to new data.\n",
       "print(\"--- Step 2: Selecting Target Variable ---\")\n",
       "if 'label' in df.columns:\n",
       "    # Drop the 'label' column from the DataFrame.\n",
       "    df.drop(columns=['label'], inplace=True)\n",
       "    print(\"Dropped the redundant 'label' column to prevent data leakage.\")\n",
       "    print(f\"Dataset shape after dropping 'label': {df.shape}\")\n",
       "else:\n",
       "    print(\"'label' column not found, skipping.\")\n",
       "print(\"\\n\")\n",
       "\n",
       "\n",
       "# **Step 3: Remove Identifier Column**\n",
       "# Objective: Remove columns that serve as identifiers but have no predictive value.\n",
       "# Rationale: The 'id' column is a unique identifier for each row (like a primary key). It contains\n",
       "# no information about the nature of the network traffic itself and would only add noise if included\n",
       "# as a feature in the model. It is standard practice to remove such identifiers.\n",
       "print(\"--- Step 3: Removing Identifier Column ---\")\n",
       "if 'id' in df.columns:\n",
       "    # Drop the 'id' column from the DataFrame.\n",
       "    df.drop(columns=['id'], inplace=True)\n",
       "    print(\"Dropped the 'id' column as it has no predictive value.\")\n",
       "    print(f\"Dataset shape after dropping 'id': {df.shape}\")\n",
       "else:\n",
       "    print(\"'id' column not found, skipping.\")\n",
       "print(\"\\n\")\n",
       "\n",
       "\n",
       "# **Step 4: Re-evaluate and Correct Data Types (Feature Identification)**\n",
       "# Objective: Systematically identify and separate features into numerical and categorical types.\n",
       "# Rationale: Machine learning algorithms require numerical input. Therefore, we must apply different\n",
       "# preprocessing techniques based on a feature's data type. Numerical features need scaling, while\n",
       "# categorical features need encoding. This step is crucial for setting up the transformation pipeline.\n",
       "print(\"--- Step 4: Identifying Feature Types ---\")\n",
       "\n",
       "# Separate the features (independent variables, X) from the target (dependent variable, y).\n",
       "# This is a standard and necessary step before any feature transformation.\n",
       "X = df.drop(columns=['attack_cat'])\n",
       "y = df['attack_cat']\n",
       "\n",
       "# Define which columns are categorical based on their data type ('object') and domain knowledge.\n",
       "# Some features are encoded as integers but represent distinct categories rather than a continuous\n",
       "# scale (e.g., 'sttl', 'dttl'). These must be explicitly identified and treated as categorical.\n",
       "categorical_features = [\n",
       "    'proto', 'service', 'state',  # Object types, clearly categorical\n",
       "    'sttl', 'dttl', 'ct_state_ttl', 'is_sm_ips_ports', 'is_ftp_login',\n",
       "    'ct_ftp_cmd', 'ct_flw_http_mthd' # Integer types that represent categories\n",
       "]\n",
       "# A robust check to ensure all features listed above are actually present in the DataFrame's columns.\n",
       "# This prevents errors if the input data changes or a column was already removed.\n",
       "categorical_features = [col for col in categorical_features if col in X.columns]\n",
       "\n",
       "# Identify numerical features by taking all columns from X that were NOT identified as categorical.\n",
       "# This is an efficient way to partition the feature set.\n",
       "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
       "\n",
       "print(f\"Identified {len(numerical_features)} numerical features.\")\n",
       "print(f\"Identified {len(categorical_features)} categorical features.\")\n",
       "print(\"\\n\")\n",
       "\n",
       "\n",
       "# **Steps 5 & 6: Encode Categorical Features and Scale Numerical Features**\n",
       "# Objective: Apply transformations to convert all features into a suitable numerical format for ML models.\n",
       "# Rationale: We use scikit-learn's ColumnTransformer to create a single, unified preprocessing\n",
       "# pipeline. This is highly efficient and less error-prone than transforming feature subsets manually.\n",
       "#   - One-Hot Encoding (Step 5): Converts categorical features into a binary vector format. This\n",
       "#     prevents the model from assuming any ordinal relationship between categories.\n",
       "#   - Standard Scaling (Step 6): Transforms numerical features to have a mean of 0 and a standard\n",
       "#     deviation of 1. This is crucial for algorithms sensitive to feature scales, such as SVMs,\n",
       "#     Logistic Regression, and Neural Networks, ensuring all features contribute fairly to the result.\n",
       "print(\"--- Steps 5 & 6: Encoding Categorical and Scaling Numerical Features ---\")\n",
       "\n",
       "# Define the transformer for numerical features. StandardScaler is a robust choice.\n",
       "numeric_transformer = StandardScaler()\n",
       "\n",
       "# Define the transformer for categorical features.\n",
       "# `handle_unknown='ignore'` is a critical parameter that prevents errors if the model encounters\n",
       "# a category in new data that it didn't see during training. It will encode that new category as all zeros.\n",
       "# `sparse_output=False` ensures the output is a standard NumPy array, which is easier to work with.\n",
       "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
       "\n",
       "# Create the ColumnTransformer. This object applies specified transformers to specified columns.\n",
       "# The 'remainder' parameter is set to 'passthrough', which ensures that any columns not explicitly\n",
       "# handled by the transformers are kept in the dataset. In our case, all columns are handled.\n",
       "preprocessor = ColumnTransformer(\n",
       "    transformers=[\n",
       "        ('num', numeric_transformer, numerical_features),\n",
       "        ('cat', categorical_transformer, categorical_features)\n",
       "    ],\n",
       "    remainder='passthrough'\n",
       ")\n",
       "\n",
       "# Apply the defined transformations to the feature set X.\n",
       "# The `fit_transform` method first learns the parameters from the data (e.g., mean/std for scaling,\n",
       "# unique categories for encoding) and then applies the transformation.\n",
       "X_processed = preprocessor.fit_transform(X)\n",
       "\n",
       "# --- Reconstruct the Processed DataFrame ---\n",
       "# The output of the ColumnTransformer is a NumPy array, which lacks column names.\n",
       "# We reconstruct a pandas DataFrame to maintain readability and for easier analysis.\n",
       "\n",
       "# Get the new column names generated by the OneHotEncoder.\n",
       "# `get_feature_names_out` creates meaningful names like 'proto_tcp', 'proto_udp', etc.\n",
       "encoded_cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
       "\n",
       "# Combine the original numerical feature names with the new one-hot encoded feature names.\n",
       "all_feature_names = numerical_features + list(encoded_cat_feature_names)\n",
       "\n",
       "# Create the new DataFrame containing the processed features.\n",
       "# We use the original index from X to ensure correct alignment when we later combine it with the target series y.\n",
       "X_processed_df = pd.DataFrame(X_processed, columns=all_feature_names, index=X.index)\n",
       "\n",
       "print(f\"Original number of features: {len(X.columns)}\")\n",
       "print(f\"Number of features after one-hot encoding and scaling: {len(X_processed_df.columns)}\")\n",
       "print(\"\\n\")\n",
       "\n",
       "\n",
       "# --- Final Preprocessed DataFrame ---\n",
       "# Combine the processed features (X_processed_df) and the original target (y)\n",
       "# into a single, fully preprocessed DataFrame ready for machine learning.\n",
       "print(\"--- Finalizing the Preprocessed DataFrame ---\")\n",
       "# `pd.concat` is used to join the two DataFrames column-wise (`axis=1`).\n",
       "# Because we preserved the index throughout the process, we can be confident that\n",
       "# each row of features correctly aligns with its corresponding target value.\n",
       "df_processed = pd.concat([X_processed_df, y], axis=1)\n",
       "\n",
       "print(\"Successfully created the final preprocessed DataFrame.\")\n",
       "print(f\"Final DataFrame shape: {df_processed.shape}\")\n",
       "print(\"\\n--- Sample of the Final Preprocessed Data ---\")\n",
       "print(df_processed.head())\n",
       "print(\"\\n--- Info of the Final Preprocessed Data ---\")\n",
       "# .info() provides a concise summary, confirming data types are all numeric and there are no missing values.\n",
       "df_processed.info()\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prompt Chain Complete ---\n",
      "Exported chain artifacts to: C:\\Users\\macdo\\Github\\VNFCDR-1\\Chris Code\\Generative AI Data Cleaning\\Machine Learning\\Step 2_Gemini Prompting\\UNSW_NB15_Train_Test_Concatenated\\Gemini_API_Exploratory_Analysis_Export.md\n",
      "\n",
      "=== Diagnostics Summary ===\n",
      "Models attempted (in order): ['gemini-2.5-pro', 'gemini-1.5-pro-latest', 'gemini-1.5-flash-latest']\n",
      "Artifacts generated: ['analysis_md', 'plan_md', 'code_md']\n"
     ]
    }
   ],
   "source": [
    "# --- Gemini Prompt Chain for Cybersecurity EDA ---\n",
    "\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "import google.api_core.exceptions\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import io, os, time, math, traceback\n",
    "from IPython.display import display, Markdown\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "# 1. Environment / API Setup --------------------------------------------------\n",
    "#ENV_PATH = r\"/Users/sarahsetiawan/Desktop/VNFCDR-1/SarahCode/Generative_AI/GEMINI_API_KEY.env\"\n",
    "ENV_PATH = r\"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 2_Gemini Prompting\\\\GEMINI_API_KEY.env\"\n",
    "\n",
    "load_dotenv(ENV_PATH)\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY not found. Ensure the .env file exists and key is set.\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "print(\"Gemini API configured successfully.\")\n",
    "\n",
    "# 2. Data Load ----------------------------------------------------------------\n",
    "#DATA_PATH = r\"/Users/sarahsetiawan/Desktop/VNFCDR-1/SarahCode/Sample_df/Code/MachineLearning/SampleData_API/CSVs/Representative_APISample_20000_2.csv\"\n",
    "DATA_PATH = r\"E:\\\\Datasets\\\\UNSW-NB15\\\\Training and Testing Sets\\\\UNSW_NB15_concatenated_dropped.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}\")\n",
    "\n",
    "print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# 3. Context Preparation ------------------------------------\n",
    "\n",
    "def build_schema_summary(dataframe: pd.DataFrame) -> str:\n",
    "    info_buf = io.StringIO()\n",
    "    dataframe.info(buf=info_buf)\n",
    "    raw_info = info_buf.getvalue()\n",
    "\n",
    "    # Build a concise table: column | dtype | non-null | null_pct | unique\n",
    "    rows = []\n",
    "    total = len(dataframe)\n",
    "    for col in dataframe.columns:\n",
    "        non_null = dataframe[col].notna().sum()\n",
    "        null_pct = 100 * (1 - non_null / total)\n",
    "        uniq = dataframe[col].nunique(dropna=True)\n",
    "        rows.append(f\"{col} | {dataframe[col].dtype} | {non_null} | {null_pct:.2f}% | {uniq}\")\n",
    "    header = \"Column | DType | Non-Null | Null% | Unique\\n------ | ----- | -------- | ----- | ------\"\n",
    "    concise = header + \"\\n\" + \"\\n\".join(rows)\n",
    "    return concise\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_ROWS = 10\n",
    "schema_concise = build_schema_summary(df)\n",
    "sample_md = df.sample(n=SAMPLE_ROWS, random_state=RANDOM_SEED).to_markdown(index=False)\n",
    "\n",
    "# 4. System Instruction -------------------------------------------------------\n",
    "system_instruction = \"\"\"\n",
    "You are an expert data scientist specializing in data cleaning and preparation for machine learning. \n",
    "Your task is to perform exploratory data analysis (EDA), data cleaning and preprocessing for machine learning application. \n",
    "You will do this by generating structured insights, plans, and code.\n",
    "\n",
    "Goals:\n",
    "1. Perform a sharp initial EDA on a given dataset.\n",
    "2. Propose an ordered data cleaning and preprocessing plan based on the EDA and best practices.\n",
    "3. Following the data cleaning plan, produce fully executable, well-commented Python code.\n",
    "\n",
    "Constraints:\n",
    "- The created code must be executable without errors using the full dataset loaded from the provided CSV file path.\n",
    "- Use Markdown headings exactly as requested.\n",
    "\"\"\"\n",
    "\n",
    "# 5. Model Strategy & Fallback ------------------------------------------------\n",
    "PRIMARY_MODEL = \"gemini-2.5-pro\"          # Change as needed\n",
    "FALLBACK_MODELS = [\"gemini-1.5-pro-latest\", \"gemini-1.5-flash-latest\"]\n",
    "\n",
    "def list_available_models() -> List[str]:\n",
    "    names = []\n",
    "    try:\n",
    "        for m in genai.list_models():\n",
    "            # Keep only models that support generateContent\n",
    "            if getattr(m, \"supported_generation_methods\", None) and \"generateContent\" in m.supported_generation_methods:\n",
    "                names.append(m.name)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return names\n",
    "\n",
    "available_models = list_available_models()\n",
    "print(f\"Models with generateContent capability (truncated list): {available_models[:8]}{' ...' if len(available_models) > 8 else ''}\")\n",
    "\n",
    "MODEL_SEQUENCE = [PRIMARY_MODEL] + [m for m in FALLBACK_MODELS if m != PRIMARY_MODEL]\n",
    "\n",
    "def pick_first_accessible_model(model_names: List[str]) -> Tuple[str, Optional[str]]:\n",
    "    for mn in model_names:\n",
    "        if any(mn.endswith(x) or mn in x for x in available_models):\n",
    "            try:\n",
    "                _ = genai.GenerativeModel(mn)  # lightweight instantiation\n",
    "                return mn, None\n",
    "            except Exception as e:\n",
    "                last_err = f\"{mn}: {e}\"\n",
    "        else:\n",
    "            last_err = f\"{mn}: not in available model list (or list inaccessible).\"\n",
    "    return \"\", last_err\n",
    "\n",
    "\n",
    "\n",
    "# 6. Safety & Generation Config ----------------------------------------------\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "BASE_GENERATION_CONFIG = dict(\n",
    "    temperature=0.15,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    max_output_tokens=8192,  \n",
    ")\n",
    "\n",
    "# 7. Utility: Safe Text Extraction -------------------------------------------\n",
    "def safe_extract_text(response) -> str:\n",
    "    if not response:\n",
    "        return \"\"\n",
    "    try:\n",
    "        # response.candidates[0].content.parts may be empty\n",
    "        parts = getattr(response.candidates[0].content, \"parts\", [])\n",
    "        texts = []\n",
    "        for p in parts:\n",
    "            if hasattr(p, \"text\") and p.text:\n",
    "                texts.append(p.text)\n",
    "        return \"\\n\".join(texts).strip()\n",
    "    except Exception:\n",
    "        # Fallback to response.text if available\n",
    "        try:\n",
    "            return (response.text or \"\").strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "# 8. Retry + Fallback Engine -------------------------------------------------\n",
    "def generate_with_retry(prompt_text: str,\n",
    "                        model_sequence: List[str],\n",
    "                        max_retries_per_model: int = 2,\n",
    "                        sleep_base: float = 1.0,\n",
    "                        label: str = \"request\"):\n",
    "\n",
    "    errors: List[str] = []\n",
    "    for model_name in model_sequence:\n",
    "        print(f\"[{label}] Attempting with model: {model_name}\")\n",
    "        model = genai.GenerativeModel(model_name=model_name,\n",
    "                                      system_instruction=system_instruction)\n",
    "        for attempt in range(1, max_retries_per_model + 1):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                response = model.generate_content(\n",
    "                    contents=prompt_text,\n",
    "                    generation_config=genai.types.GenerationConfig(**BASE_GENERATION_CONFIG),\n",
    "                    safety_settings=safety_settings,\n",
    "                    request_options={\"timeout\": 60},  # seconds\n",
    "                )\n",
    "                elapsed = time.time() - start\n",
    "                text = safe_extract_text(response)\n",
    "                if not text:\n",
    "                    raise ValueError(\"Empty or blocked response content.\")\n",
    "                print(f\"[{label}] Success model={model_name} attempt={attempt} time={elapsed:.2f}s\")\n",
    "                return dict(response=response, text=text, model_used=model_name, attempts=attempt, errors=errors)\n",
    "            except google.api_core.exceptions.InternalServerError as e:\n",
    "                err_msg = f\"500 InternalServerError model={model_name} attempt={attempt}: {e.message if hasattr(e,'message') else e}\"\n",
    "                print(err_msg)\n",
    "                errors.append(err_msg)\n",
    "            except google.api_core.exceptions.GoogleAPIError as e:\n",
    "                err_msg = f\"GoogleAPIError model={model_name} attempt={attempt}: {e}\"\n",
    "                print(err_msg)\n",
    "                errors.append(err_msg)\n",
    "            except ValueError as e:\n",
    "                err_msg = f\"ValueError model={model_name} attempt={attempt}: {e}\"\n",
    "                print(err_msg)\n",
    "                errors.append(err_msg)\n",
    "            except Exception as e:\n",
    "                tb = traceback.format_exc(limit=1)\n",
    "                err_msg = f\"Unexpected model={model_name} attempt={attempt}: {e} | {tb}\"\n",
    "                print(err_msg)\n",
    "                errors.append(err_msg)\n",
    "            # Backoff\n",
    "            time.sleep(sleep_base * (2 ** (attempt - 1)))\n",
    "        print(f\"[{label}] Moving to next model after failures on {model_name}.\")\n",
    "    print(f\"[{label}] All model attempts failed.\")\n",
    "    return dict(response=None, text=\"\", model_used=None, attempts=None, errors=errors)\n",
    "\n",
    "# 9. Prompts -----------------------------------------------------------------\n",
    "prompt_1_analysis = f\"\"\"\n",
    "## Task 1: Initial Data Analysis\n",
    "\n",
    "You are given:\n",
    "### Concise Schema Summary\n",
    "```\n",
    "{schema_concise}\n",
    "```\n",
    "\n",
    "### Random Sample ({SAMPLE_ROWS} rows)\n",
    "```\n",
    "{sample_md}\n",
    "```\n",
    "\n",
    "### Directives\n",
    "1. Use the schema and sample to perform a sharp exploratory data analysis (EDA).\n",
    "2. Create a table of unique value counts for all columns in the dataset.\n",
    "3. Create a table of high cardinality columns. Ensure to explain why the columns were selected.\n",
    "4. Generate a data quality analysis containing accuracy, completeness, consistency, uniqueness, validity, and timeliness before data cleaning.\n",
    "\n",
    "Output ONLY under heading:\n",
    "## Analytical Insights\n",
    "Use concise Markdown sections & tables. Avoid code.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# 10. Execution Chain --------------------------------------------------------\n",
    "chain_artifacts: Dict[str, str] = {}\n",
    "\n",
    "print(\"--- Chain Step 1: Initial Analysis ---\")\n",
    "analysis_result = generate_with_retry(prompt_1_analysis, MODEL_SEQUENCE, label=\"analysis\")\n",
    "\n",
    "if not analysis_result[\"text\"]:\n",
    "    print(\"Aborting chain: analysis stage failed.\")\n",
    "else:\n",
    "    chain_artifacts[\"analysis_md\"] = analysis_result[\"text\"]\n",
    "    display(Markdown(\"### Analysis Received\"))\n",
    "    display(Markdown(chain_artifacts[\"analysis_md\"]))\n",
    "\n",
    "    # Step 2: Plan\n",
    "    prompt_2_plan = f\"\"\"\n",
    "## Task 2: Data Cleaning Plan\n",
    "\n",
    "Using the prior analysis:\n",
    "\n",
    "### Analytical Insights\n",
    "{chain_artifacts['analysis_md']}\n",
    "\n",
    "Produce a prioritized, ordered bullet list of data cleaning and machine learning preprocessing steps.\n",
    "Generate a data quality analysis containing accuracy, completeness, consistency, uniqueness, validity, and timeliness before data cleaning.\n",
    "Do not perform train-test splits or modeling.\n",
    "No code. Include: objective, columns impacted, and rationale.\n",
    "Heading required: ## Data Cleaning Steps\n",
    "\"\"\"\n",
    "    print(\"\\n--- Chain Step 2: Cleaning Plan ---\")\n",
    "    plan_result = generate_with_retry(prompt_2_plan, MODEL_SEQUENCE, label=\"plan\")\n",
    "\n",
    "    if not plan_result[\"text\"]:\n",
    "        print(\"Aborting chain: plan stage failed.\")\n",
    "    else:\n",
    "        chain_artifacts[\"plan_md\"] = plan_result[\"text\"]\n",
    "        display(Markdown(\"### Cleaning Plan Received\"))\n",
    "        display(Markdown(chain_artifacts[\"plan_md\"]))\n",
    "\n",
    "        # Step 3: Code\n",
    "        prompt_3_code = f\"\"\"\n",
    "## Task 3: Data Cleaning Code\n",
    "\n",
    "### Data Cleaning Steps\n",
    "{chain_artifacts['plan_md']}\n",
    "\n",
    "## Directives\n",
    "1. Create fully executable Python code that implements the data cleaning plan developed in Task 2.\n",
    "2. Your code should prepare one fully preprocessed dataframe. Do NOT perform any splitting or modeling.\n",
    "3. Do not use sample data, only the full dataset loaded from the CSV file at path: {DATA_PATH}\n",
    "4. Provide extremely detailed comments explaining each step of the code.\n",
    "\n",
    "\n",
    "Return as ONE fenced Python code block under heading:\n",
    "## Data Cleaning Code\n",
    "\"\"\"\n",
    "        print(\"\\n--- Chain Step 3: Code Generation ---\")\n",
    "        code_result = generate_with_retry(prompt_3_code, MODEL_SEQUENCE, label=\"code\")\n",
    "\n",
    "        if not code_result[\"text\"]:\n",
    "            print(\"Code generation failed.\")\n",
    "        else:\n",
    "            chain_artifacts[\"code_md\"] = code_result[\"text\"]\n",
    "            display(Markdown(\"### Data Cleaning Code Received\"))\n",
    "            display(Markdown(chain_artifacts[\"code_md\"]))\n",
    "\n",
    "        # Step 4: Validation\n",
    "        prompt_4_validation = f\"\"\"\n",
    "## Task 4: Data Cleaning Validation\n",
    "\n",
    "### Data Cleaning Steps\n",
    "{chain_artifacts['code_md']}\n",
    "\n",
    "## Directives\n",
    "1. Provide reasoning for any changes made from the original code.\n",
    "2. Review the provided data cleaning code for correctness and completeness.\n",
    "3. Ensure all steps from the cleaning plan are implemented in order.\n",
    "4. Ensure the code is executable without errors using the full dataset loaded from the provided CSV file.\n",
    "5. Provide extremely detailed comments explaining each step of the code.\n",
    "6. Your code should prepare one fully preprocessed dataframe. Do NOT perform any splitting or modeling.\n",
    "7. If the original code is correct, return it unchanged and note that no changes were needed.\n",
    "\n",
    "Return as ONE fenced Python code block under heading:\n",
    "## Data Cleaning Code - Validated\n",
    "\"\"\"\n",
    "        print(\"\\n--- Chain Step 4: Code Validation ---\")\n",
    "        code_result = generate_with_retry(prompt_4_validation, MODEL_SEQUENCE, label=\"validation\")\n",
    "\n",
    "        if not code_result[\"text\"]:\n",
    "            print(\"Validation check failed.\")\n",
    "        else:\n",
    "            chain_artifacts[\"code_md\"] = code_result[\"text\"]\n",
    "            display(Markdown(\"### Data Validation Received\"))\n",
    "            display(Markdown(chain_artifacts[\"code_md\"]))\n",
    "\n",
    "\n",
    "print(\"\\n--- Prompt Chain Complete ---\")\n",
    "\n",
    "# 11. Export Artifacts --------------------------------------------------------\n",
    "EXPORT_DIR = os.path.dirname(DATA_PATH)\n",
    "export_path = \"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 2_Gemini Prompting\\\\UNSW_NB15_Train_Test_Concatenated\\\\Gemini_API_Exploratory_Analysis_Export.md\"\n",
    "try:\n",
    "    with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        if chain_artifacts:\n",
    "            f.write(\"# Gemini EDA Prompt Chain Output\\n\\n\")\n",
    "            for k, v in chain_artifacts.items():\n",
    "                pretty = k.replace(\"_md\", \"\").capitalize()\n",
    "                f.write(f\"\\n\\n## {pretty}\\n\\n\")\n",
    "                f.write(v.strip() + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"No artifacts generated (chain failed).\")\n",
    "    print(f\"Exported chain artifacts to: {export_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Export failed: {e}\")\n",
    "\n",
    "# 12. Diagnostic Recap --------------------------------------------------------\n",
    "def print_diagnostics():\n",
    "    print(\"\\n=== Diagnostics Summary ===\")\n",
    "    print(f\"Models attempted (in order): {MODEL_SEQUENCE}\")\n",
    "    if analysis_result.get('errors'):\n",
    "        print(f\"Analysis errors count: {len(analysis_result['errors'])}\")\n",
    "        for err in analysis_result['errors'][:3]:\n",
    "            print(f\"  - {err[:160]}{'...' if len(err)>160 else ''}\")\n",
    "    if 'plan_result' in locals() and plan_result.get('errors'):\n",
    "        print(f\"Plan errors count: {len(plan_result['errors'])}\")\n",
    "    if 'code_result' in locals() and code_result.get('errors'):\n",
    "        print(f\"Code errors count: {len(code_result['errors'])}\")\n",
    "    print(\"Artifacts generated:\", list(chain_artifacts.keys()))\n",
    "\n",
    "print_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af89f9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini API configured successfully.\n",
      "Dataset loaded successfully. Shape: (257673, 83)\n",
      "Models with generateContent capability (truncated list): ['models/gemini-1.5-pro-latest', 'models/gemini-1.5-pro-002', 'models/gemini-1.5-pro', 'models/gemini-1.5-flash-latest', 'models/gemini-1.5-flash', 'models/gemini-1.5-flash-002', 'models/gemini-1.5-flash-8b', 'models/gemini-1.5-flash-8b-001'] ...\n",
      "--- Chain Step 1: Initial Analysis ---\n",
      "[analysis] Attempting with model: gemini-2.5-pro\n",
      "[analysis] Success model=gemini-2.5-pro attempt=1 time=28.23s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Analysis Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Analytical Insights\n",
       "\n",
       "This report provides a comprehensive data quality analysis of the provided dataset, focusing on six key dimensions. The dataset appears to be pre-processed and scaled, likely in preparation for machine learning modeling.\n",
       "\n",
       "### Data Quality Summary\n",
       "\n",
       "| Dimension | Status | Key Observation |\n",
       "| :--- | :--- | :--- |\n",
       "| **Completeness** | Excellent | 100% complete; no missing values in any column. |\n",
       "| **Validity** | Good | Data types are appropriate, but integer/binary features are cast as floats, likely due to scaling. |\n",
       "| **Uniqueness** | Good | No single primary key exists. High cardinality in continuous features and low cardinality in binary/categorical features are as expected. |\n",
       "| **Consistency** | Excellent | The dataset is structurally consistent with uniform data types within columns and a consistent number of records across the board. |\n",
       "| **Accuracy** | Unverifiable | Values appear systematically processed (scaled), but their accuracy cannot be confirmed without external validation or domain knowledge. |\n",
       "| **Timeliness** | Not Applicable | The dataset contains no timestamps, making it impossible to assess its currency or recency. |\n",
       "\n",
       "---\n",
       "\n",
       "### 1. Completeness\n",
       "\n",
       "The dataset is **100% complete**.\n",
       "\n",
       "*   **Observation**: The schema summary indicates that all 89 columns have 257,673 non-null values, resulting in a null percentage of 0.00% for the entire dataset.\n",
       "*   **Implication**: No imputation or handling of missing values is required, which simplifies the data preparation process for machine learning.\n",
       "\n",
       "### 2. Validity\n",
       "\n",
       "The dataset demonstrates good validity, with data types conforming to their expected formats.\n",
       "\n",
       "*   **Data Types**: All feature columns are of type `float64`, and the target column `attack_cat` is `object` (string). This is valid for modeling purposes.\n",
       "*   **Format & Range**:\n",
       "    *   Many columns that are inherently integer-based (e.g., `spkts`, `dpkts`) or binary (e.g., `is_sm_ips_ports_0`, `sttl_64`) are represented as floats.\n",
       "    *   The sample data shows negative values and values between -1 and 1, strongly suggesting that the numerical features have been standardized or normalized. This is a valid and common pre-processing step.\n",
       "    *   The `attack_cat` column contains 10 unique string values, which is valid for a multi-class classification target.\n",
       "\n",
       "### 3. Uniqueness\n",
       "\n",
       "The dataset has a reasonable uniqueness profile but lacks a primary key.\n",
       "\n",
       "*   **Row Uniqueness**: There is no single column that uniquely identifies each record, as no column has 257,673 unique values. A check for fully duplicated rows would be necessary to confirm complete record uniqueness.\n",
       "*   **Feature Uniqueness**:\n",
       "    *   Continuous features like `dur`, `rate`, and `sload` show a very high number of unique values, which is expected.\n",
       "    *   A large number of features (e.g., the `sttl_*`, `dttl_*`, and `is_*` columns) have only 2 unique values, correctly identifying them as binary flags.\n",
       "    *   Categorical features like `ct_srv_src` have a moderate number of unique values (57), which is manageable.\n",
       "\n",
       "### 4. Consistency\n",
       "\n",
       "The dataset is highly consistent both structurally and logically.\n",
       "\n",
       "*   **Structural Consistency**: Every column contains the same number of records (257,673), ensuring proper alignment.\n",
       "*   **Formatting Consistency**: The representation of binary features as floats (0.0 and 1.0) is applied consistently across all relevant columns. Naming conventions (e.g., prefixes like `ct_`, `sttl_`, `dttl_`) are used consistently, which aids in feature understanding.\n",
       "\n",
       "### 5. Accuracy\n",
       "\n",
       "The accuracy of the data values cannot be fully verified without an external source of truth or domain expertise.\n",
       "\n",
       "*   **Plausibility**: The values, although scaled, appear plausible within the context of a pre-processed dataset. The categories in `attack_cat` (e.g., 'Normal', 'Generic', 'Exploits') are standard for network intrusion detection datasets.\n",
       "*   **Verifiability**: It is impossible to determine if the recorded measurements (e.g., `sbytes`, `dbytes`, `dur`) or the assigned `attack_cat` labels are correct without comparing them to ground truth logs or expert analysis. The quality of any subsequent machine learning model will be highly dependent on the accuracy of these labels.\n",
       "\n",
       "### 6. Timeliness\n",
       "\n",
       "An assessment of timeliness is **not possible**.\n",
       "\n",
       "*   **Observation**: The dataset does not contain any date or timestamp columns that would indicate when the network traffic data was captured.\n",
       "*   **Implication**: While the data can be used to model the patterns present at the time of collection, it's impossible to know how current those patterns are. This is a critical consideration if the model is intended for real-time application, as network attack patterns evolve over time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported chain artifacts to: C:\\Users\\macdo\\Github\\VNFCDR-1\\Chris Code\\Generative AI Data Cleaning\\Machine Learning\\Step 2_Gemini Prompting\\UNSW_NB15_Train_Test_Concatenated\\Gemini_API_After_Cleaning_Analysis_Export.md\n",
      "\n",
      "=== Diagnostics Summary ===\n",
      "Models attempted (in order): ['gemini-2.5-pro', 'gemini-1.5-pro-latest', 'gemini-1.5-flash-latest']\n",
      "Artifacts generated: ['analysis_md']\n"
     ]
    }
   ],
   "source": [
    "# Post Cleaning Data Quality Analysis\n",
    "\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "import google.api_core.exceptions\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import io, os, time, math, traceback\n",
    "from IPython.display import display, Markdown\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "# 1. Environment / API Setup --------------------------------------------------\n",
    "#ENV_PATH = r\"/Users/sarahsetiawan/Desktop/VNFCDR-1/SarahCode/Generative_AI/GEMINI_API_KEY.env\"\n",
    "ENV_PATH = r\"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 2_Gemini Prompting\\\\GEMINI_API_KEY.env\"\n",
    "\n",
    "load_dotenv(ENV_PATH)\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY not found. Ensure the .env file exists and key is set.\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "print(\"Gemini API configured successfully.\")\n",
    "\n",
    "# 2. Data Load ----------------------------------------------------------------\n",
    "#DATA_PATH = r\"/Users/sarahsetiawan/Desktop/VNFCDR-1/SarahCode/Sample_df/Code/MachineLearning/SampleData_API/CSVs/Representative_APISample_20000_2.csv\"\n",
    "DATA_PATH = r\"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 2_Gemini Prompting\\\\UNSW_NB15_Train_Test_Concatenated\\\\cleaned_data.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}\")\n",
    "\n",
    "print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# 3. Context Preparation (Token-Optimized) ------------------------------------\n",
    "\n",
    "def build_schema_summary(dataframe: pd.DataFrame) -> str:\n",
    "    info_buf = io.StringIO()\n",
    "    dataframe.info(buf=info_buf)\n",
    "    raw_info = info_buf.getvalue()\n",
    "\n",
    "    # Build a concise table: column | dtype | non-null | null_pct | unique\n",
    "    rows = []\n",
    "    total = len(dataframe)\n",
    "    for col in dataframe.columns:\n",
    "        non_null = dataframe[col].notna().sum()\n",
    "        null_pct = 100 * (1 - non_null / total)\n",
    "        uniq = dataframe[col].nunique(dropna=True)\n",
    "        rows.append(f\"{col} | {dataframe[col].dtype} | {non_null} | {null_pct:.2f}% | {uniq}\")\n",
    "    header = \"Column | DType | Non-Null | Null% | Unique\\n------ | ----- | -------- | ----- | ------\"\n",
    "    concise = header + \"\\n\" + \"\\n\".join(rows)\n",
    "    return concise\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_ROWS = 10\n",
    "schema_concise = build_schema_summary(df)\n",
    "sample_md = df.sample(n=SAMPLE_ROWS, random_state=RANDOM_SEED).to_markdown(index=False)\n",
    "\n",
    "# 4. System Instruction -------------------------------------------------------\n",
    "system_instruction = \"\"\"\n",
    "You are an expert data scientist specializing in data cleaning and preparation for machine learning. \n",
    "Your task is to Generate a data quality analysis containing accuracy, completeness, consistency, uniqueness, validity, and timeliness on an imported file. \n",
    "\n",
    "Goals:\n",
    "1. Perform a data quality analysis on a given dataset.\n",
    "\n",
    "Constraints:\n",
    "- Use Markdown headings exactly as requested.\n",
    "\"\"\"\n",
    "\n",
    "# 5. Model Strategy & Fallback ------------------------------------------------\n",
    "PRIMARY_MODEL = \"gemini-2.5-pro\"          # Change as needed\n",
    "FALLBACK_MODELS = [\"gemini-1.5-pro-latest\", \"gemini-1.5-flash-latest\"]\n",
    "\n",
    "def list_available_models() -> List[str]:\n",
    "    names = []\n",
    "    try:\n",
    "        for m in genai.list_models():\n",
    "            # Keep only models that support generateContent\n",
    "            if getattr(m, \"supported_generation_methods\", None) and \"generateContent\" in m.supported_generation_methods:\n",
    "                names.append(m.name)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return names\n",
    "\n",
    "available_models = list_available_models()\n",
    "print(f\"Models with generateContent capability (truncated list): {available_models[:8]}{' ...' if len(available_models) > 8 else ''}\")\n",
    "\n",
    "MODEL_SEQUENCE = [PRIMARY_MODEL] + [m for m in FALLBACK_MODELS if m != PRIMARY_MODEL]\n",
    "\n",
    "def pick_first_accessible_model(model_names: List[str]) -> Tuple[str, Optional[str]]:\n",
    "    for mn in model_names:\n",
    "        if any(mn.endswith(x) or mn in x for x in available_models):\n",
    "            try:\n",
    "                _ = genai.GenerativeModel(mn)  # lightweight instantiation\n",
    "                return mn, None\n",
    "            except Exception as e:\n",
    "                last_err = f\"{mn}: {e}\"\n",
    "        else:\n",
    "            last_err = f\"{mn}: not in available model list (or list inaccessible).\"\n",
    "    return \"\", last_err\n",
    "\n",
    "\n",
    "\n",
    "# 6. Safety & Generation Config ----------------------------------------------\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "BASE_GENERATION_CONFIG = dict(\n",
    "    temperature=0.15,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    max_output_tokens=8192,  \n",
    ")\n",
    "\n",
    "# 7. Utility: Safe Text Extraction -------------------------------------------\n",
    "def safe_extract_text(response) -> str:\n",
    "    if not response:\n",
    "        return \"\"\n",
    "    try:\n",
    "        # response.candidates[0].content.parts may be empty\n",
    "        parts = getattr(response.candidates[0].content, \"parts\", [])\n",
    "        texts = []\n",
    "        for p in parts:\n",
    "            if hasattr(p, \"text\") and p.text:\n",
    "                texts.append(p.text)\n",
    "        return \"\\n\".join(texts).strip()\n",
    "    except Exception:\n",
    "        # Fallback to response.text if available\n",
    "        try:\n",
    "            return (response.text or \"\").strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "# 8. Retry + Fallback Engine -------------------------------------------------\n",
    "def generate_with_retry(prompt_text: str,\n",
    "                        model_sequence: List[str],\n",
    "                        max_retries_per_model: int = 2,\n",
    "                        sleep_base: float = 1.0,\n",
    "                        label: str = \"request\"):\n",
    "\n",
    "    errors: List[str] = []\n",
    "    for model_name in model_sequence:\n",
    "        print(f\"[{label}] Attempting with model: {model_name}\")\n",
    "        model = genai.GenerativeModel(model_name=model_name,\n",
    "                                      system_instruction=system_instruction)\n",
    "        for attempt in range(1, max_retries_per_model + 1):\n",
    "            try:\n",
    "                start = time.time()\n",
    "                response = model.generate_content(\n",
    "                    contents=prompt_text,\n",
    "                    generation_config=genai.types.GenerationConfig(**BASE_GENERATION_CONFIG),\n",
    "                    safety_settings=safety_settings,\n",
    "                    request_options={\"timeout\": 60},  # seconds\n",
    "                )\n",
    "                elapsed = time.time() - start\n",
    "                text = safe_extract_text(response)\n",
    "                if not text:\n",
    "                    raise ValueError(\"Empty or blocked response content.\")\n",
    "                print(f\"[{label}] Success model={model_name} attempt={attempt} time={elapsed:.2f}s\")\n",
    "                return dict(response=response, text=text, model_used=model_name, attempts=attempt, errors=errors)\n",
    "            except google.api_core.exceptions.InternalServerError as e:\n",
    "                err_msg = f\"500 InternalServerError model={model_name} attempt={attempt}: {e.message if hasattr(e,'message') else e}\"\n",
    "                print(err_msg)\n",
    "                errors.append(err_msg)\n",
    "            except google.api_core.exceptions.GoogleAPIError as e:\n",
    "                err_msg = f\"GoogleAPIError model={model_name} attempt={attempt}: {e}\"\n",
    "                print(err_msg)\n",
    "                errors.append(err_msg)\n",
    "            except ValueError as e:\n",
    "                err_msg = f\"ValueError model={model_name} attempt={attempt}: {e}\"\n",
    "                print(err_msg)\n",
    "                errors.append(err_msg)\n",
    "            except Exception as e:\n",
    "                tb = traceback.format_exc(limit=1)\n",
    "                err_msg = f\"Unexpected model={model_name} attempt={attempt}: {e} | {tb}\"\n",
    "                print(err_msg)\n",
    "                errors.append(err_msg)\n",
    "            # Backoff\n",
    "            time.sleep(sleep_base * (2 ** (attempt - 1)))\n",
    "        print(f\"[{label}] Moving to next model after failures on {model_name}.\")\n",
    "    print(f\"[{label}] All model attempts failed.\")\n",
    "    return dict(response=None, text=\"\", model_used=None, attempts=None, errors=errors)\n",
    "\n",
    "# 9. Prompts -----------------------------------------------------------------\n",
    "prompt_1_analysis = f\"\"\"\n",
    "## Task 1: Data Quality Analysis\n",
    "\n",
    "You are given:\n",
    "### Concise Schema Summary\n",
    "```\n",
    "{schema_concise}\n",
    "```\n",
    "\n",
    "### Random Sample ({SAMPLE_ROWS} rows)\n",
    "```\n",
    "{sample_md}\n",
    "```\n",
    "\n",
    "### Directives\n",
    "1. Generate a data quality analysis containing accuracy, completeness, consistency, uniqueness, validity, and timeliness on the impored file at {DATA_PATH}.\n",
    "\n",
    "Output ONLY under heading:\n",
    "## Analytical Insights\n",
    "Use concise Markdown sections & tables. Avoid code.\n",
    "\"\"\"\n",
    "\n",
    "# 10. Execution Chain --------------------------------------------------------\n",
    "chain_artifacts: Dict[str, str] = {}\n",
    "\n",
    "print(\"--- Chain Step 1: Initial Analysis ---\")\n",
    "analysis_result = generate_with_retry(prompt_1_analysis, MODEL_SEQUENCE, label=\"analysis\")\n",
    "\n",
    "if not analysis_result[\"text\"]:\n",
    "    print(\"Aborting chain: analysis stage failed.\")\n",
    "else:\n",
    "    chain_artifacts[\"analysis_md\"] = analysis_result[\"text\"]\n",
    "    display(Markdown(\"### Analysis Received\"))\n",
    "    display(Markdown(chain_artifacts[\"analysis_md\"]))\n",
    "\n",
    "# 11. Export Artifacts --------------------------------------------------------\n",
    "EXPORT_DIR = os.path.dirname(DATA_PATH)\n",
    "export_path = \"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 2_Gemini Prompting\\\\UNSW_NB15_Train_Test_Concatenated\\\\Gemini_API_After_Cleaning_Analysis_Export.md\"\n",
    "try:\n",
    "    with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        if chain_artifacts:\n",
    "            f.write(\"# Gemini EDA Prompt Chain Output\\n\\n\")\n",
    "            for k, v in chain_artifacts.items():\n",
    "                pretty = k.replace(\"_md\", \"\").capitalize()\n",
    "                f.write(f\"\\n\\n## {pretty}\\n\\n\")\n",
    "                f.write(v.strip() + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"No artifacts generated (chain failed).\")\n",
    "    print(f\"Exported chain artifacts to: {export_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Export failed: {e}\")\n",
    "\n",
    "# 12. Diagnostic Recap --------------------------------------------------------\n",
    "def print_diagnostics():\n",
    "    print(\"\\n=== Diagnostics Summary ===\")\n",
    "    print(f\"Models attempted (in order): {MODEL_SEQUENCE}\")\n",
    "    if analysis_result.get('errors'):\n",
    "        print(f\"Analysis errors count: {len(analysis_result['errors'])}\")\n",
    "        for err in analysis_result['errors'][:3]:\n",
    "            print(f\"  - {err[:160]}{'...' if len(err)>160 else ''}\")\n",
    "    if 'plan_result' in locals() and plan_result.get('errors'):\n",
    "        print(f\"Plan errors count: {len(plan_result['errors'])}\")\n",
    "    if 'code_result' in locals() and code_result.get('errors'):\n",
    "        print(f\"Code errors count: {len(code_result['errors'])}\")\n",
    "    print(\"Artifacts generated:\", list(chain_artifacts.keys()))\n",
    "\n",
    "print_diagnostics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_supervised_20250810",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

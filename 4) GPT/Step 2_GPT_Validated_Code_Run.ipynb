{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a186bdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Loading data\n",
      "--------------------------------------------------------------------------------\n",
      "Loaded shape: (257673, 42)\n",
      "Columns: ['id', 'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat', 'label']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1) Schema lock and dtypes enforcement\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2) Identifier removal\n",
      "--------------------------------------------------------------------------------\n",
      "Dropped identifier columns: ['id']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3) Exact duplicate handling\n",
      "--------------------------------------------------------------------------------\n",
      "Removed 97199 exact duplicate rows. New shape: (160474, 41)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4) Label–category consistency audit and repair\n",
      "--------------------------------------------------------------------------------\n",
      "Normalized recognized attack_cat values: 160474 rows.\n",
      "Repaired label to match attack_cat in 0 conflicting rows.\n",
      "Filled missing labels from attack_cat in 0 rows.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5) Domain validity checks and hard constraints\n",
      "--------------------------------------------------------------------------------\n",
      "Post domain checks shape: (160474, 41)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6) Continuous feature skew mitigation (log1p)\n",
      "--------------------------------------------------------------------------------\n",
      "Applied log1p to 18 columns: ['dur', 'rate', 'sload', 'dload', 'sbytes', 'dbytes', 'spkts', 'dpkts', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'synack', 'ackdat', 'tcprtt', 'response_body_len', 'smean', 'dmean']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "7) Extreme outlier capping (winsorization)\n",
      "--------------------------------------------------------------------------------\n",
      "Winsorized 13 columns at [0.5%, 99.5%] quantiles.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10) Zero-inflation indicators for structural zeros\n",
      "--------------------------------------------------------------------------------\n",
      "Added 8 zero-indicator columns: ['is_zero__dur', 'is_zero__sinpkt', 'is_zero__dinpkt', 'is_zero__sjit', 'is_zero__djit', 'is_zero__synack', 'is_zero__ackdat', 'is_zero__tcprtt']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "8) Multicollinearity control among derived features\n",
      "--------------------------------------------------------------------------------\n",
      "Dropped for multicollinearity (present): ['tcprtt', 'rate', 'sload', 'dload', 'stcpb', 'dtcpb']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9) Categorical encoding for selected small-domain features (+ rare handling)\n",
      "--------------------------------------------------------------------------------\n",
      "One-hot encoded columns: ['ct_state_ttl', 'ct_flw_http_mthd', 'is_sm_ips_ports', 'is_ftp_login', 'ct_ftp_cmd']\n",
      "Created 18 dummy columns.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "11) Scaling of continuous features (RobustScaler)\n",
      "--------------------------------------------------------------------------------\n",
      "Scaled 28 continuous features with RobustScaler.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "12) Rare category handling\n",
      "--------------------------------------------------------------------------------\n",
      "Rare category pooling performed during step 9 before one-hot encoding.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "13) Class distribution profiling and artifact check\n",
      "--------------------------------------------------------------------------------\n",
      "Label distribution:\n",
      "label\n",
      "0    85720\n",
      "1    74754\n",
      "\n",
      "attack_cat distribution (top 20):\n",
      "attack_cat\n",
      "Normal            85720\n",
      "Exploits          27039\n",
      "Fuzzers           20568\n",
      "Reconnaissance     9626\n",
      "Generic            7549\n",
      "DoS                5108\n",
      "Analysis           1649\n",
      "Backdoor           1588\n",
      "Shellcode          1456\n",
      "Worms               171\n",
      "\n",
      "Top 10 absolute correlations with label (numeric features):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macdo\\anaconda3\\envs\\ml_supervised_20250810\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\macdo\\anaconda3\\envs\\ml_supervised_20250810\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ct_state_ttl=0     0.675841\n",
      "sttl               0.518058\n",
      "ct_state_ttl=1     0.468094\n",
      "dttl               0.392160\n",
      "dmean              0.313891\n",
      "ct_state_ttl=2     0.300556\n",
      "ackdat             0.295254\n",
      "dbytes             0.287104\n",
      "is_zero__dinpkt    0.269314\n",
      "dinpkt             0.269267\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "14) Final integrity and export of clean feature matrix\n",
      "--------------------------------------------------------------------------------\n",
      "Final preprocessed shape: (160474, 56)\n",
      "Saved preprocessed dataset to: E:\\Datasets\\UNSW-NB15\\Training and Testing Sets\\UNSW_NB15_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Cleaning and Preprocessing for UNSW-NB15 (concatenated) dataset.\n",
    "\n",
    "VALIDATION SUMMARY (Task 4):\n",
    "- The original pipeline is largely correct and complete. It implements all plan steps, with an intentional\n",
    "  reordering where step 10 (zero-inflation indicators) is applied before step 8 (multicollinearity drops)\n",
    "  to preserve useful zero-pattern information for features that are subsequently removed.\n",
    "- One targeted change was made for robustness and to avoid unnecessary row loss:\n",
    "  CHANGE: In Final Integrity (Step 14), instead of dropping rows with NaN in any column (including raw\n",
    "          non-modeled string columns), we now only drop rows that have NaNs in numeric columns\n",
    "          (i.e., model-ready features and targets). This aligns with the step’s intent and avoids removing rows\n",
    "          due to missing values in non-numeric, uninterpreted identifiers or text columns.\n",
    "  RATIONALE: The previous \"drop if any NaN in df\" could remove a large number of rows if any unused string\n",
    "             column had NaNs. Our change keeps the feature matrix clean where it matters (numeric inputs\n",
    "             and targets) while preserving rows that are otherwise valid for modeling.\n",
    "\n",
    "All other logic is retained; additional comments were added for clarity.\n",
    "\n",
    "This script implements the cleaning plan described in Task 3:\n",
    "1) Schema lock and dtypes enforcement\n",
    "2) Identifier removal\n",
    "3) Exact duplicate handling\n",
    "4) Label–category consistency audit and repair\n",
    "5) Domain validity checks and hard constraints\n",
    "6) Continuous feature skew mitigation (log1p)\n",
    "7) Extreme outlier capping (winsorization)\n",
    "10) Zero-inflation indicators for structural zeros (applied BEFORE dropping derived features)\n",
    "8) Multicollinearity control among derived features\n",
    "9) Categorical encoding for selected protocol/state flags\n",
    "11) Scaling of continuous features\n",
    "12) Rare category handling (applied within step 9)\n",
    "13) Class distribution profiling and artifact check (non-modeling diagnostics)\n",
    "14) Final integrity checks and export\n",
    "\n",
    "Notes:\n",
    "- This code avoids splitting or modeling; it prepares a single preprocessed DataFrame.\n",
    "- It is defensive to slight schema variations by checking column existence before operations.\n",
    "- All steps print concise diagnostics to assist auditability without interrupting execution.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Input CSV path (must exist)\n",
    "CSV_PATH = r\"E:\\Datasets\\UNSW-NB15\\Training and Testing Sets\\UNSW_NB15_concatenated_dropped.csv\"\n",
    "\n",
    "# Output (optional) - will attempt to write in the same directory\n",
    "OUTPUT_FILENAME = \"UNSW_NB15_preprocessed.csv\"\n",
    "\n",
    "# Robust winsorization quantiles (after log1p transformation)\n",
    "LOW_Q = 0.005\n",
    "HIGH_Q = 0.995\n",
    "\n",
    "# Rare category threshold (proportion of dataset); categories under this will be pooled into 'Other'\n",
    "RARE_CAT_THRESHOLD = 0.001  # 0.1%\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utilities\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def normalize_attack_cat(val):\n",
    "    \"\"\"\n",
    "    Normalize attack category string to canonical form if recognized.\n",
    "    Returns (canonical_string, recognized_boolean)\n",
    "\n",
    "    - Handles common UNSW-NB15 attack categories and \"Normal\"\n",
    "    - Uses a lowercase map but returns canonical title-case labels.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return val, False\n",
    "    s = str(val).strip().lower()\n",
    "    mapping = {\n",
    "        \"normal\": \"Normal\",\n",
    "        \"fuzzers\": \"Fuzzers\",\n",
    "        \"analysis\": \"Analysis\",\n",
    "        \"backdoor\": \"Backdoor\",\n",
    "        \"backdoors\": \"Backdoor\",  # sometimes plural\n",
    "        \"dos\": \"DoS\",\n",
    "        \"exploits\": \"Exploits\",\n",
    "        \"generic\": \"Generic\",\n",
    "        \"reconnaissance\": \"Reconnaissance\",\n",
    "        \"shellcode\": \"Shellcode\",\n",
    "        \"worms\": \"Worms\",\n",
    "    }\n",
    "    if s in mapping:\n",
    "        return mapping[s], True\n",
    "    return val, False  # return original if not recognized\n",
    "\n",
    "\n",
    "def print_step_header(step_text):\n",
    "    \"\"\"Helper to print readable step headers in the console for audit trail.\"\"\"\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(step_text)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "def safe_intersect(cols, df_columns):\n",
    "    \"\"\"Return the intersection of a candidate list with an existing DataFrame columns.\"\"\"\n",
    "    return [c for c in cols if c in df_columns]\n",
    "\n",
    "\n",
    "def is_binary_series(s):\n",
    "    \"\"\"Check if a pandas Series contains only {0,1} (ignoring NaNs).\"\"\"\n",
    "    vals = pd.unique(s.dropna())\n",
    "    return set(vals).issubset({0, 1})\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Load data\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"Loading data\")\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"Input CSV not found at path: {CSV_PATH}\")\n",
    "\n",
    "# low_memory=False to preserve column consistency and avoid mixed dtypes\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "print(f\"Loaded shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Schema lock and dtypes enforcement\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"1) Schema lock and dtypes enforcement\")\n",
    "\n",
    "# Targets: binary 'label' is required; multiclass 'attack_cat' is optional.\n",
    "target_label_col = \"label\"\n",
    "target_multiclass_col = \"attack_cat\"\n",
    "\n",
    "# Ensure 'label' is present; if not, raise error because it is a key target.\n",
    "if target_label_col not in df.columns:\n",
    "    raise KeyError(\"Expected binary target column 'label' not found.\")\n",
    "\n",
    "# Enforce 'label' as pandas nullable integer Int64 (allows NA during coercion) then validate later.\n",
    "df[target_label_col] = pd.to_numeric(df[target_label_col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Ensure 'attack_cat' exists; if so, cast to string for normalization.\n",
    "if target_multiclass_col in df.columns:\n",
    "    df[target_multiclass_col] = df[target_multiclass_col].astype(\"string\")\n",
    "else:\n",
    "    print(\"Warning: 'attack_cat' column not found; multiclass target-related checks will be skipped.\")\n",
    "\n",
    "# Convert object-typed non-target columns to numeric if the vast majority are numeric-representable.\n",
    "# This prevents string categorical columns from being damaged while ensuring numeric strings become numerics.\n",
    "non_target_cols = [c for c in df.columns if c not in [target_label_col, target_multiclass_col]]\n",
    "for col in non_target_cols:\n",
    "    if df[col].dtype == object:\n",
    "        converted = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        # Convert only if >90% of values can be parsed as numbers (heuristic to preserve true categoricals).\n",
    "        if converted.notna().mean() > 0.9:\n",
    "            df[col] = converted\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Identifier removal\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"2) Identifier removal\")\n",
    "# Drop only clear row-unique identifiers; keep potential features like IPs unless explicitly excluded.\n",
    "drop_id_cols = [c for c in [\"id\"] if c in df.columns]\n",
    "if drop_id_cols:\n",
    "    df.drop(columns=drop_id_cols, inplace=True)\n",
    "    print(f\"Dropped identifier columns: {drop_id_cols}\")\n",
    "else:\n",
    "    print(\"No identifier columns found to drop.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Exact duplicate handling\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"3) Exact duplicate handling\")\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(ignore_index=True)\n",
    "after = len(df)\n",
    "print(f\"Removed {before - after} exact duplicate rows. New shape: {df.shape}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Label–category consistency audit and repair\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"4) Label–category consistency audit and repair\")\n",
    "if target_multiclass_col in df.columns:\n",
    "    # Normalize attack_cat values to canonical names if recognized by our map\n",
    "    normalized_vals = df[target_multiclass_col].apply(normalize_attack_cat)\n",
    "    df[target_multiclass_col] = normalized_vals.apply(lambda x: x[0])\n",
    "    recognized_mask = normalized_vals.apply(lambda x: x[1])\n",
    "\n",
    "    # Derive the expected binary label from normalized attack_cat where recognized (Normal -> 0; else -> 1).\n",
    "    derived_label = pd.Series(index=df.index, dtype=\"Int64\")\n",
    "    recognized_cats = df.loc[recognized_mask, target_multiclass_col]\n",
    "    derived_label.loc[recognized_mask] = (recognized_cats != \"Normal\").astype(\"Int64\")\n",
    "\n",
    "    # Identify contradictions only on rows where both a label and a recognized cat exist.\n",
    "    mismatch_mask = recognized_mask & df[target_label_col].notna() & (df[target_label_col] != derived_label)\n",
    "    mismatches = int(mismatch_mask.sum())\n",
    "\n",
    "    # Fill missing labels directly from recognized attack_cat-derived labels.\n",
    "    fill_from_cat_mask = recognized_mask & df[target_label_col].isna()\n",
    "    fills = int(fill_from_cat_mask.sum())\n",
    "\n",
    "    # Apply fixes (mend contradictions and fill missing labels).\n",
    "    df.loc[mismatch_mask, target_label_col] = derived_label[mismatch_mask]\n",
    "    df.loc[fill_from_cat_mask, target_label_col] = derived_label[fill_from_cat_mask]\n",
    "\n",
    "    print(f\"Normalized recognized attack_cat values: {int(recognized_mask.sum())} rows.\")\n",
    "    print(f\"Repaired label to match attack_cat in {mismatches} conflicting rows.\")\n",
    "    print(f\"Filled missing labels from attack_cat in {fills} rows.\")\n",
    "\n",
    "# Ensure 'label' is strictly binary {0,1}; if non-binary values remain, coerce and enforce.\n",
    "if not is_binary_series(df[target_label_col].astype(\"float\").fillna(-1)):\n",
    "    print(\"Warning: Non-binary values detected in 'label'. Coercing positive->1, zero/negative->0.\")\n",
    "    df[target_label_col] = (pd.to_numeric(df[target_label_col], errors=\"coerce\").fillna(0) > 0).astype(\"Int64\")\n",
    "\n",
    "# Drop rows where label is still NaN or not in {0,1} after coercion to guarantee clean target.\n",
    "valid_label_mask = df[target_label_col].isin([0, 1])\n",
    "dropped_invalid_label = int((~valid_label_mask).sum())\n",
    "df = df.loc[valid_label_mask].reset_index(drop=True)\n",
    "if dropped_invalid_label > 0:\n",
    "    print(f\"Dropped {dropped_invalid_label} rows with invalid 'label'. New shape: {df.shape}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) Domain validity checks and hard constraints\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"5) Domain validity checks and hard constraints\")\n",
    "\n",
    "# Define domain rules for numeric columns with known physical constraints (non-negative, bounded)\n",
    "non_negative_cols = [\n",
    "    \"dur\",\"sbytes\",\"dbytes\",\"spkts\",\"dpkts\",\n",
    "    \"sload\",\"dload\",\"rate\",\n",
    "    \"sinpkt\",\"dinpkt\",\"sjit\",\"djit\",\n",
    "    \"synack\",\"ackdat\",\"tcprtt\",\n",
    "    \"response_body_len\",\n",
    "    \"smean\",\"dmean\",\"smeansz\",\"dmeansz\",\n",
    "    \"stcpb\",\"dtcpb\",\n",
    "    \"trans_depth\",\n",
    "    \"sttl\",\"dttl\",\"swin\",\"dwin\",\n",
    "]\n",
    "\n",
    "# TTL plausible bounds\n",
    "ttl_cols = [\"sttl\", \"dttl\"]\n",
    "ttl_min, ttl_max = 0, 255\n",
    "\n",
    "# Known small-domain flag/category columns (will be one-hot encoded later)\n",
    "small_cat_cols = [\"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_sm_ips_ports\", \"is_ftp_login\", \"ct_ftp_cmd\"]\n",
    "\n",
    "# Enforce numeric dtypes for numeric domain columns when present\n",
    "present_nonneg_cols = safe_intersect(non_negative_cols, df.columns)\n",
    "for col in present_nonneg_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Non-negativity hard filter: drop any row with a negative value across constrained columns\n",
    "neg_mask_any = pd.Series(False, index=df.index)\n",
    "for col in present_nonneg_cols:\n",
    "    neg_mask = df[col] < 0\n",
    "    neg_mask_any = neg_mask_any | (neg_mask.fillna(False))\n",
    "\n",
    "neg_count = int(neg_mask_any.sum())\n",
    "if neg_count > 0:\n",
    "    print(f\"Filtering out {neg_count} rows with negative values in non-negative constrained columns.\")\n",
    "    df = df.loc[~neg_mask_any].reset_index(drop=True)\n",
    "\n",
    "# TTL hard range check [0, 255]; drop rows outside bounds\n",
    "present_ttl_cols = safe_intersect(ttl_cols, df.columns)\n",
    "if present_ttl_cols:\n",
    "    out_of_range_mask_any = pd.Series(False, index=df.index)\n",
    "    for col in present_ttl_cols:\n",
    "        out_of_range_mask = ~df[col].between(ttl_min, ttl_max)\n",
    "        out_of_range_mask_any = out_of_range_mask_any | (out_of_range_mask.fillna(False))\n",
    "    out_range_count = int(out_of_range_mask_any.sum())\n",
    "    if out_range_count > 0:\n",
    "        print(f\"Filtering out {out_range_count} rows with TTL out of [{ttl_min},{ttl_max}].\")\n",
    "        df = df.loc[~out_of_range_mask_any].reset_index(drop=True)\n",
    "\n",
    "# Binary domain enforcement for is_sm_ips_ports if present (must be {0,1}); drop invalid.\n",
    "if \"is_sm_ips_ports\" in df.columns:\n",
    "    df[\"is_sm_ips_ports\"] = pd.to_numeric(df[\"is_sm_ips_ports\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    valid_binary = df[\"is_sm_ips_ports\"].isin([0, 1])\n",
    "    invalid_binary_count = int((~valid_binary).sum())\n",
    "    if invalid_binary_count > 0:\n",
    "        print(f\"Dropping {invalid_binary_count} rows with invalid 'is_sm_ips_ports' values (expect 0/1).\")\n",
    "        df = df.loc[valid_binary].reset_index(drop=True)\n",
    "\n",
    "# Ensure we removed any NaNs introduced so far in strict columns (domain-constrained numeric fields)\n",
    "strict_cols = present_nonneg_cols + present_ttl_cols + ([\"is_sm_ips_ports\"] if \"is_sm_ips_ports\" in df.columns else [])\n",
    "if strict_cols:\n",
    "    nan_mask_any = df[strict_cols].isna().any(axis=1)\n",
    "    nan_count = int(nan_mask_any.sum())\n",
    "    if nan_count > 0:\n",
    "        print(f\"Dropping {nan_count} rows with NaN in strict domain columns after enforcement.\")\n",
    "        df = df.loc[~nan_mask_any].reset_index(drop=True)\n",
    "\n",
    "print(f\"Post domain checks shape: {df.shape}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6) Continuous feature skew mitigation (log1p)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"6) Continuous feature skew mitigation (log1p)\")\n",
    "\n",
    "# Columns suited for log1p (only if present and non-negative by design)\n",
    "# These are often long-tailed in network traffic; log1p compresses scale and preserves zeros.\n",
    "log1p_candidates = [\n",
    "    \"dur\",\"rate\",\"sload\",\"dload\",\n",
    "    \"sbytes\",\"dbytes\",\"spkts\",\"dpkts\",\n",
    "    \"sinpkt\",\"dinpkt\",\"sjit\",\"djit\",\n",
    "    \"synack\",\"ackdat\",\"tcprtt\",\n",
    "    \"response_body_len\",\n",
    "    \"smean\",\"dmean\",\"smeansz\",\"dmeansz\",\n",
    "]\n",
    "\n",
    "log1p_cols = safe_intersect(log1p_candidates, df.columns)\n",
    "\n",
    "# Apply safe log1p transformation in-place (shifts up if any residual negatives appear unexpectedly)\n",
    "for col in log1p_cols:\n",
    "    min_val = df[col].min()\n",
    "    if pd.notna(min_val) and min_val < 0:\n",
    "        shift = abs(min_val)\n",
    "        print(f\"Warning: {col} has negative values after domain checks. Shifting by {shift} before log1p.\")\n",
    "        df[col] = df[col] + shift\n",
    "    df[col] = np.log1p(df[col].astype(float))\n",
    "\n",
    "print(f\"Applied log1p to {len(log1p_cols)} columns: {log1p_cols}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7) Extreme outlier capping (winsorization) on log-transformed features\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"7) Extreme outlier capping (winsorization)\")\n",
    "# Winsorization post-log stabilizes extremes while preserving relative ranks.\n",
    "winsorize_candidates = [\n",
    "    \"rate\",\"sload\",\"dload\",\"sinpkt\",\"dinpkt\",\"sjit\",\"djit\",\n",
    "    \"synack\",\"ackdat\",\"tcprtt\",\"dur\",\"sbytes\",\"dbytes\"\n",
    "]\n",
    "winsor_cols = safe_intersect(winsorize_candidates, df.columns)\n",
    "\n",
    "for col in winsor_cols:\n",
    "    lo = df[col].quantile(LOW_Q)\n",
    "    hi = df[col].quantile(HIGH_Q)\n",
    "    if pd.isna(lo) or pd.isna(hi):\n",
    "        continue\n",
    "    if lo > hi:\n",
    "        lo, hi = hi, lo\n",
    "    df[col] = df[col].clip(lower=lo, upper=hi)\n",
    "\n",
    "print(f\"Winsorized {len(winsor_cols)} columns at [{LOW_Q*100:.1f}%, {HIGH_Q*100:.1f}%] quantiles.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 10) Zero-inflation indicators for structural zeros (do BEFORE feature dropping)\n",
    "# NOTE (intentional reordering): We place step 10 before step 8 to preserve zero-pattern information for features\n",
    "# that are subsequently dropped for multicollinearity (e.g., tcprtt, rate, loads).\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"10) Zero-inflation indicators for structural zeros\")\n",
    "\n",
    "zero_indicator_candidates = [\n",
    "    \"dur\",\"sinpkt\",\"dinpkt\",\"sjit\",\"djit\",\"synack\",\"ackdat\",\"tcprtt\"\n",
    "    # We omit rate/sload/dload indicators because they will be dropped for multicollinearity\n",
    "]\n",
    "present_zero_cols = safe_intersect(zero_indicator_candidates, df.columns)\n",
    "\n",
    "zero_ind_cols = []\n",
    "for col in present_zero_cols:\n",
    "    ind_col = f\"is_zero__{col}\"\n",
    "    # After log1p, a value equal to 0 implies the original value was exactly 0.\n",
    "    zero_ind = (df[col] == 0).astype(\"int8\")\n",
    "    df[ind_col] = zero_ind\n",
    "    zero_ind_cols.append(ind_col)\n",
    "\n",
    "print(f\"Added {len(zero_ind_cols)} zero-indicator columns: {zero_ind_cols}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 8) Multicollinearity control among derived features\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"8) Multicollinearity control among derived features\")\n",
    "\n",
    "# Drop highly collinear or redundant features (e.g., TCP RTT variants and rates) to reduce leakage risk\n",
    "# and simplify models. We already captured zero-patterns for some of these above.\n",
    "drop_for_collinearity = [\"tcprtt\", \"rate\", \"sload\", \"dload\", \"stcpb\", \"dtcpb\"]\n",
    "drop_existing = safe_intersect(drop_for_collinearity, df.columns)\n",
    "df.drop(columns=drop_existing, inplace=True, errors=\"ignore\")\n",
    "print(f\"Dropped for multicollinearity (present): {drop_existing}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 9) Categorical encoding for protocol/state flags (+ step 12 rare cat handling inside)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"9) Categorical encoding for selected small-domain features (+ rare handling)\")\n",
    "\n",
    "# Selected known small-domain categoricals; will be one-hot encoded with rare category pooling.\n",
    "ohe_base_cols = [\"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_sm_ips_ports\", \"is_ftp_login\", \"ct_ftp_cmd\"]\n",
    "ohe_cols_present = safe_intersect(ohe_base_cols, df.columns)\n",
    "\n",
    "created_dummies = []\n",
    "for col in ohe_cols_present:\n",
    "    # Treat values as strings for robust one-hot encoding; ensure missing becomes explicit \"Unknown\"\n",
    "    col_as_str = df[col].astype(\"Int64\") if pd.api.types.is_numeric_dtype(df[col]) else df[col].astype(\"string\")\n",
    "    col_as_str = col_as_str.astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "    # Rare category pooling: any category appearing fewer than threshold rows becomes \"Other\"\n",
    "    vc = col_as_str.value_counts(dropna=False)\n",
    "    threshold = max(2, int(math.floor(RARE_CAT_THRESHOLD * len(df))))  # at least 2 rows to keep as unique\n",
    "    rare_cats = set(vc[vc < threshold].index)\n",
    "    pooled = col_as_str.where(~col_as_str.isin(rare_cats), other=\"Other\")\n",
    "\n",
    "    # Create one-hot columns with deterministic naming\n",
    "    dummies = pd.get_dummies(pooled, prefix=col, prefix_sep=\"=\", dtype=np.uint8)\n",
    "    df = pd.concat([df.drop(columns=[col]), dummies], axis=1)\n",
    "    created_dummies.extend(list(dummies.columns))\n",
    "\n",
    "print(f\"One-hot encoded columns: {ohe_cols_present}\")\n",
    "print(f\"Created {len(created_dummies)} dummy columns.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 11) Scaling of continuous features (RobustScaler)\n",
    "# - Scale only continuous numeric features (exclude targets, OHE binaries, and zero-indicator binaries)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"11) Scaling of continuous features (RobustScaler)\")\n",
    "\n",
    "# Identify numeric columns post-encoding (includes label, zero indicators, and OHE dummies)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Build the exclusion set:\n",
    "exclude_from_scaling = set()\n",
    "# Targets\n",
    "exclude_from_scaling.add(target_label_col)\n",
    "# Zero indicators (binary features we don't scale)\n",
    "exclude_from_scaling.update(zero_ind_cols)\n",
    "# OHE dummy columns (uint8 dummies created in step 9)\n",
    "exclude_from_scaling.update(created_dummies)\n",
    "\n",
    "# Final list of continuous columns to scale\n",
    "scale_cols = [c for c in numeric_cols if c not in exclude_from_scaling]\n",
    "\n",
    "if scale_cols:\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n",
    "    df[scale_cols] = scaler.fit_transform(df[scale_cols].astype(float))\n",
    "    print(f\"Scaled {len(scale_cols)} continuous features with RobustScaler.\")\n",
    "else:\n",
    "    print(\"No continuous columns identified for scaling.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 12) Rare category handling (handled in step 9 via pooling before encoding)\n",
    "# -------------------------------------------------------------------\n",
    "print_step_header(\"12) Rare category handling\")\n",
    "print(\"Rare category pooling performed during step 9 before one-hot encoding.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 13) Class distribution profiling and artifact check (no modeling)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"13) Class distribution profiling and artifact check\")\n",
    "\n",
    "# Label distribution\n",
    "label_counts = df[target_label_col].value_counts(dropna=False)\n",
    "print(\"Label distribution:\")\n",
    "print(label_counts.to_string())\n",
    "\n",
    "# Multiclass distribution if available\n",
    "if target_multiclass_col in df.columns:\n",
    "    print(\"\\nattack_cat distribution (top 20):\")\n",
    "    print(df[target_multiclass_col].value_counts(dropna=False).head(20).to_string())\n",
    "\n",
    "# Quick leakage-like check: high correlation with label for numeric features\n",
    "print(\"\\nTop 10 absolute correlations with label (numeric features):\")\n",
    "num_for_corr = [c for c in numeric_cols if c != target_label_col and c in df.columns]\n",
    "if num_for_corr:\n",
    "    corr = df[num_for_corr].corrwith(df[target_label_col].astype(float)).abs().sort_values(ascending=False)\n",
    "    print(corr.head(10).to_string())\n",
    "else:\n",
    "    print(\"No numeric features available for correlation analysis.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 14) Final integrity and export of clean feature matrix\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print_step_header(\"14) Final integrity and export of clean feature matrix\")\n",
    "\n",
    "# Replace inf values (should be none) as a safeguard, then check for NaNs\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# VALIDATED CHANGE:\n",
    "# Previously, code dropped rows if ANY column had NaN (including unused string columns).\n",
    "# We now enforce NaN-free condition only across numeric columns (model features and targets),\n",
    "# which aligns with the step's intent and avoids unnecessary row loss.\n",
    "numeric_cols_final = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "nan_mask_numeric = df[numeric_cols_final].isna().any(axis=1)\n",
    "nan_rows_numeric = int(nan_mask_numeric.sum())\n",
    "if nan_rows_numeric > 0:\n",
    "    print(f\"Warning: Found {nan_rows_numeric} rows with NaNs in numeric columns after processing; dropping those rows.\")\n",
    "    df = df.loc[~nan_mask_numeric].reset_index(drop=True)\n",
    "\n",
    "print(f\"Final preprocessed shape: {df.shape}\")\n",
    "\n",
    "# Expose the final preprocessed DataFrame for downstream usage (single fully-prepared matrix)\n",
    "df_preprocessed = df.copy()\n",
    "\n",
    "# Attempt to save to disk (optional)\n",
    "try:\n",
    "    out_dir = str(Path(CSV_PATH).parent)\n",
    "    out_path = os.path.join(out_dir, OUTPUT_FILENAME)\n",
    "    df_preprocessed.to_csv(out_path, index=False)\n",
    "    print(f\"Saved preprocessed dataset to: {out_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save preprocessed CSV due to: {e}\")\n",
    "\n",
    "# For interactive sessions, you can inspect\n",
    "# print(df_preprocessed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4025340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned DataFrame exported to 'cleaned_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Export the cleaned dataframe to a new CSV file\n",
    "df.to_csv('cleaned_data.csv', index=False)\n",
    "\n",
    "print(\"Cleaned DataFrame exported to 'cleaned_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_supervised_20250810",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6166e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API client configured successfully.\n",
      "Dataset loaded successfully. Shape: (257673, 42)\n",
      "--- Chain Step 1: Initial Analysis ---\n",
      "[analysis] Success attempt=1 time=55.74s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Analysis Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Analytical Insights\n",
       "\n",
       "### Dataset Overview\n",
       "- **Total Rows**: 257,673\n",
       "- **Total Columns**: 44\n",
       "- **No Missing Data**: All columns show 0.00% null values\n",
       "- **Mixed Data Types**: Primarily numerical (int64, float64) with one categorical column (attack_cat)\n",
       "- **Target Variable**: `label` (binary classification) and `attack_cat` (multi-class classification)\n",
       "\n",
       "### Unique Value Counts\n",
       "| Column | Unique Values | Type |\n",
       "|--------|---------------|------|\n",
       "| id | 175,341 | Identifier |\n",
       "| dur | 109,945 | Continuous |\n",
       "| spkts | 646 | Discrete |\n",
       "| dpkts | 627 | Discrete |\n",
       "| sbytes | 9,382 | Discrete |\n",
       "| dbytes | 8,653 | Discrete |\n",
       "| rate | 115,763 | Continuous |\n",
       "| sttl | 13 | Discrete |\n",
       "| dttl | 9 | Discrete |\n",
       "| sload | 121,356 | Continuous |\n",
       "| dload | 116,380 | Continuous |\n",
       "| sloss | 490 | Discrete |\n",
       "| dloss | 476 | Discrete |\n",
       "| sinpkt | 114,318 | Continuous |\n",
       "| dinpkt | 110,270 | Continuous |\n",
       "| sjit | 117,101 | Continuous |\n",
       "| djit | 114,861 | Continuous |\n",
       "| swin | 22 | Discrete |\n",
       "| stcpb | 114,473 | Continuous |\n",
       "| dtcpb | 114,187 | Continuous |\n",
       "| dwin | 19 | Discrete |\n",
       "| tcprtt | 63,878 | Continuous |\n",
       "| synack | 57,366 | Continuous |\n",
       "| ackdat | 53,248 | Continuous |\n",
       "| smean | 1,377 | Discrete |\n",
       "| dmean | 1,362 | Discrete |\n",
       "| trans_depth | 14 | Discrete |\n",
       "| response_body_len | 2,819 | Discrete |\n",
       "| ct_srv_src | 57 | Discrete |\n",
       "| ct_state_ttl | 7 | Discrete |\n",
       "| ct_dst_ltm | 52 | Discrete |\n",
       "| ct_src_dport_ltm | 52 | Discrete |\n",
       "| ct_dst_sport_ltm | 35 | Discrete |\n",
       "| ct_dst_src_ltm | 58 | Discrete |\n",
       "| is_ftp_login | 4 | Binary-like |\n",
       "| ct_ftp_cmd | 4 | Discrete |\n",
       "| ct_flw_http_mthd | 11 | Discrete |\n",
       "| ct_src_ltm | 52 | Discrete |\n",
       "| ct_srv_dst | 57 | Discrete |\n",
       "| is_sm_ips_ports | 2 | Binary |\n",
       "| attack_cat | 10 | Categorical |\n",
       "| label | 2 | Binary |\n",
       "\n",
       "### High Cardinality Columns\n",
       "| Column | Unique Count | Reason |\n",
       "|--------|--------------|--------|\n",
       "| id | 175,341 | Likely an identifier column with near-unique values |\n",
       "| dur | 109,945 | Continuous duration measurements |\n",
       "| rate | 115,763 | Continuous network rate values |\n",
       "| sload | 121,356 | Continuous source load measurements |\n",
       "| dload | 116,380 | Continuous destination load measurements |\n",
       "| sinpkt | 114,318 | Continuous source inter-packet times |\n",
       "| dinpkt | 110,270 | Continuous destination inter-packet times |\n",
       "| sjit | 117,101 | Continuous source jitter measurements |\n",
       "| djit | 114,861 | Continuous destination jitter measurements |\n",
       "| stcpb | 114,473 | Continuous TCP base sequence numbers |\n",
       "| dtcpb | 114,187 | Continuous TCP base sequence numbers |\n",
       "\n",
       "**Selection Rationale**: These columns were selected because they exhibit very high unique value counts (â‰¥100,000), indicating continuous numerical features that may require scaling/normalization for machine learning.\n",
       "\n",
       "### Data Quality Analysis\n",
       "\n",
       "**Accuracy**: \n",
       "- Numerical ranges appear plausible for network traffic data\n",
       "- Extreme values observed (e.g., sload up to 800 million) may represent legitimate network anomalies or measurement errors\n",
       "\n",
       "**Completeness**:\n",
       "- Excellent completeness with 0% missing values across all columns\n",
       "- No imputation required for missing data\n",
       "\n",
       "**Consistency**:\n",
       "- Data types align with expected measurement types\n",
       "- Some zero values in network metrics (dbytes=0, dload=0) appear consistent with one-way traffic patterns\n",
       "- TCP-related metrics show zero values when no TCP connection exists\n",
       "\n",
       "**Uniqueness**:\n",
       "- `id` column shows high uniqueness (68% unique), suggesting it may be an identifier\n",
       "- Other columns show appropriate uniqueness levels for their respective measurement types\n",
       "\n",
       "**Validity**:\n",
       "- Binary columns (is_sm_ips_ports, label) contain only 0/1 values\n",
       "- Categorical column (attack_cat) contains expected attack categories\n",
       "- No obvious invalid values in numerical columns based on sample\n",
       "\n",
       "**Timeliness**:\n",
       "- Not applicable as this appears to be static network traffic data without temporal relevance"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chain Step 2: Cleaning Plan ---\n",
      "[plan] Success attempt=1 time=42.33s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Cleaning Plan Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Data Cleaning Steps\n",
       "\n",
       "**Objective:** Prepare the dataset for machine learning by addressing data quality issues, ensuring feature compatibility, and optimizing for model performance.\n",
       "\n",
       "- **Drop the `id` column**\n",
       "  - **Columns Impacted:** `id`\n",
       "  - **Rationale:** The column exhibits near-unique values (68% unique) and functions as an identifier. Retaining it would introduce noise and cause models to overfit to specific observations rather than learning general patterns.\n",
       "\n",
       "- **Harmonize the target variables (`label` and `attack_cat`)**\n",
       "  - **Columns Impacted:** `label`, `attack_cat`\n",
       "  - **Rationale:** The `attack_cat` column contains a \"Normal\" category which corresponds to `label=0`. For a binary classification task using `label`, the `attack_cat` column should be dropped to prevent data leakage. For a multi-class task using `attack_cat`, the `label` column should be dropped for the same reason. The choice depends on the ultimate modeling objective.\n",
       "\n",
       "- **Scale high-cardinality continuous numerical features**\n",
       "  - **Columns Impacted:** `dur`, `rate`, `sload`, `dload`, `sinpkt`, `dinpkt`, `sjit`, `djit`, `stcpb`, `dtcpb`\n",
       "  - **Rationale:** These features have extremely high cardinality (unique value counts > 100,000) and wide, varying ranges (e.g., `sload` up to 800 million). Machine learning algorithms are often sensitive to the scale of features; scaling (e.g., using StandardScaler or RobustScaler) ensures no single feature dominates the model's learning process due to its magnitude.\n",
       "\n",
       "- **Investigate and potentially cap extreme values in continuous features**\n",
       "  - **Columns Impacted:** `sload`, `dload`, and other continuous features with extreme maximum values.\n",
       "  - **Rationale:** While extreme values may represent legitimate network anomalies, they could also be measurement errors or outliers that can skew model training. Analysis (e.g., visualizing distributions) should be conducted to decide if capping (winsorization) is appropriate to improve model robustness.\n",
       "\n",
       "- **Encode the categorical feature `attack_cat`**\n",
       "  - **Columns Impacted:** `attack_cat` (if kept for a multi-class task)\n",
       "  - **Rationale:** Machine learning models require numerical input. This nominal categorical column must be converted into a numerical format, typically using one-hot encoding, to be used as a feature or target.\n",
       "\n",
       "- **Validate and confirm the handling of zero-value observations**\n",
       "  - **Columns Impacted:** `dbytes`, `dload`, and other metrics noted to contain zeros.\n",
       "  - **Rationale:** The analysis suggests zeros are consistent with one-way traffic (e.g., no destination bytes). These should be accepted as valid data points and not mistaken for missing values. No imputation is needed.\n",
       "\n",
       "- **Verify binary columns**\n",
       "  - **Columns Impacted:** `is_sm_ips_ports`, `is_ftp_login`, `ct_ftp_cmd`\n",
       "  - **Rationale:** The analysis indicates these columns contain very few unique values (2-4). They should be inspected to confirm they represent intended binary or low-cardinality categorical data and then encoded appropriately if necessary (e.g., `is_sm_ips_ports` is already binary 0/1 and likely requires no change)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chain Step 3: Code Generation ---\n",
      "[code] Success attempt=1 time=130.72s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Data Cleaning Code Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Data Cleaning Code\n",
       "```python\n",
       "# Import required libraries\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
       "from scipy.stats import mstats  # For winsorization (capping extreme values)\n",
       "\n",
       "# Load the dataset from the specified path\n",
       "df = pd.read_csv(r\"E:\\Datasets\\UNSW-NB15\\Training and Testing Sets\\UNSW_NB15_concatenated_dropped.csv\")\n",
       "\n",
       "# Create a copy of the dataframe to preserve original data\n",
       "df_clean = df.copy()\n",
       "\n",
       "### Step 1: Drop the 'id' column\n",
       "# Rationale: The 'id' column is a near-unique identifier that doesn't contribute to predictive power\n",
       "# and could cause model overfitting to specific observations\n",
       "df_clean.drop(columns=['id'], inplace=True)\n",
       "\n",
       "### Step 2: Harmonize target variables\n",
       "# Rationale: Prevent data leakage by keeping only one target variable\n",
       "# We choose to keep 'label' for binary classification and drop 'attack_cat'\n",
       "df_clean.drop(columns=['attack_cat'], inplace=True)\n",
       "\n",
       "### Step 3: Handle extreme values using winsorization\n",
       "# Rationale: Cap extreme values to reduce skewness while preserving outlier information\n",
       "# Define columns with extreme values based on EDA findings\n",
       "extreme_value_cols = ['sload', 'dload', 'dur', 'rate', 'sinpkt', 'dinpkt', \n",
       "                     'sjit', 'djit', 'stcpb', 'dtcpb']\n",
       "\n",
       "# Winsorize top and bottom 1% of values for each specified column\n",
       "for col in extreme_value_cols:\n",
       "    df_clean[col] = mstats.winsorize(df_clean[col], limits=[0.01, 0.01])\n",
       "\n",
       "### Step 4: Scale high-cardinality continuous features\n",
       "# Rationale: Standardize feature scales to prevent dominance by large-magnitude features\n",
       "# Use RobustScaler due to presence of remaining outliers after winsorization\n",
       "high_cardinality_features = [\n",
       "    'dur', 'rate', 'sload', 'dload', 'sinpkt', 'dinpkt',\n",
       "    'sjit', 'djit', 'stcpb', 'dtcpb'\n",
       "]\n",
       "\n",
       "# Initialize RobustScaler (less sensitive to outliers than StandardScaler)\n",
       "scaler = RobustScaler()\n",
       "\n",
       "# Apply scaling to specified features\n",
       "df_clean[high_cardinality_features] = scaler.fit_transform(df_clean[high_cardinality_features])\n",
       "\n",
       "### Step 5: Validate binary columns\n",
       "# Rationale: Ensure binary columns contain expected values and are properly formatted\n",
       "binary_columns = ['is_sm_ips_ports', 'is_ftp_login', 'ct_ftp_cmd']\n",
       "\n",
       "for col in binary_columns:\n",
       "    # Check unique values and data type\n",
       "    unique_vals = df_clean[col].unique()\n",
       "    print(f\"Column '{col}' unique values: {unique_vals}\")\n",
       "    \n",
       "    # Convert to integer if not already (ensures numerical representation)\n",
       "    df_clean[col] = df_clean[col].astype(int)\n",
       "\n",
       "### Step 6: Validate zero-value handling\n",
       "# Rationale: Confirm zeros are legitimate values (not missing data) in network metrics\n",
       "zero_value_columns = ['dbytes', 'dload']\n",
       "\n",
       "for col in zero_value_columns:\n",
       "    zero_count = (df_clean[col] == 0).sum()\n",
       "    print(f\"Column '{col}' has {zero_count} zero values ({(zero_count/len(df_clean))*100:.2f}%)\")\n",
       "\n",
       "# No imputation needed - zeros represent valid one-way network traffic\n",
       "\n",
       "### Final Data Validation\n",
       "print(\"\\nFinal dataset shape:\", df_clean.shape)\n",
       "print(\"\\nColumn dtypes:\\n\", df_clean.dtypes)\n",
       "print(\"\\nMissing values per column:\\n\", df_clean.isnull().sum())\n",
       "print(\"\\nFirst 5 rows of cleaned data:\")\n",
       "print(df_clean.head())\n",
       "\n",
       "# The dataframe df_clean is now prepared for machine learning\n",
       "# Features are scaled, extreme values are capped, and data types are validated\n",
       "# Note: Target variable 'label' remains unchanged for binary classification\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chain Step 4: Code Validation ---\n",
      "[validation] Success attempt=1 time=74.91s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Data Validation Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Data Cleaning Code - Validated\n",
       "```python\n",
       "# Import required libraries\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
       "from scipy.stats import mstats  # For winsorization (capping extreme values)\n",
       "\n",
       "# Load the dataset from the specified path\n",
       "df = pd.read_csv(r\"E:\\Datasets\\UNSW-NB15\\Training and Testing Sets\\UNSW_NB15_concatenated_dropped.csv\")\n",
       "\n",
       "# Create a copy of the dataframe to preserve original data\n",
       "df_clean = df.copy()\n",
       "\n",
       "### Step 1: Drop the 'id' column\n",
       "# Rationale: The 'id' column is a near-unique identifier that doesn't contribute to predictive power\n",
       "# and could cause model overfitting to specific observations. This is a standard preprocessing step\n",
       "# for ML datasets with record identifiers.\n",
       "df_clean.drop(columns=['id'], inplace=True)\n",
       "\n",
       "### Step 2: Harmonize target variables\n",
       "# Rationale: Prevent data leakage by keeping only one target variable. 'attack_cat' contains\n",
       "# multiclass attack categories while 'label' contains binary classification (normal vs attack).\n",
       "# We choose 'label' for binary classification to maintain consistency with common network\n",
       "# intrusion detection approaches.\n",
       "df_clean.drop(columns=['attack_cat'], inplace=True)\n",
       "\n",
       "### Step 3: Handle extreme values using winsorization\n",
       "# Rationale: Cap extreme values to reduce skewness while preserving outlier information.\n",
       "# Winsorization is preferred over removal for network traffic data where extreme values\n",
       "# may represent legitimate but rare network events. Top and bottom 1% capping is a common\n",
       "# approach that balances outlier mitigation with data preservation.\n",
       "# Define columns with extreme values based on EDA findings\n",
       "extreme_value_cols = ['sload', 'dload', 'dur', 'rate', 'sinpkt', 'dinpkt', \n",
       "                     'sjit', 'djit', 'stcpb', 'dtcpb']\n",
       "\n",
       "# Winsorize top and bottom 1% of values for each specified column\n",
       "# Note: mstats.winsorize returns a masked array, so we extract the underlying data\n",
       "for col in extreme_value_cols:\n",
       "    df_clean[col] = mstats.winsorize(df_clean[col], limits=[0.01, 0.01]).data\n",
       "\n",
       "### Step 4: Scale high-cardinality continuous features\n",
       "# Rationale: Standardize feature scales to prevent dominance by large-magnitude features.\n",
       "# Use RobustScaler due to presence of remaining outliers after winsorization. RobustScaler\n",
       "# uses median and IQR instead of mean and standard deviation, making it more resistant to\n",
       "# outliers that may still exist after winsorization.\n",
       "high_cardinality_features = [\n",
       "    'dur', 'rate', 'sload', 'dload', 'sinpkt', 'dinpkt',\n",
       "    'sjit', 'djit', 'stcpb', 'dtcpb'\n",
       "]\n",
       "\n",
       "# Initialize RobustScaler (less sensitive to outliers than StandardScaler)\n",
       "scaler = RobustScaler()\n",
       "\n",
       "# Apply scaling to specified features\n",
       "df_clean[high_cardinality_features] = scaler.fit_transform(df_clean[high_cardinality_features])\n",
       "\n",
       "### Step 5: Validate binary columns\n",
       "# Rationale: Ensure binary columns contain expected values and are properly formatted.\n",
       "# Network traffic binary flags should only contain 0/1 values. Converting to integer\n",
       "# ensures proper handling by machine learning algorithms.\n",
       "binary_columns = ['is_sm_ips_ports', 'is_ftp_login', 'ct_ftp_cmd']\n",
       "\n",
       "for col in binary_columns:\n",
       "    # Check unique values and data type\n",
       "    unique_vals = df_clean[col].unique()\n",
       "    print(f\"Column '{col}' unique values: {unique_vals}\")\n",
       "    \n",
       "    # Validate that only 0 and 1 values are present\n",
       "    if set(unique_vals).issubset({0, 1}):\n",
       "        # Convert to integer if not already (ensures numerical representation)\n",
       "        df_clean[col] = df_clean[col].astype(int)\n",
       "    else:\n",
       "        print(f\"Warning: Column '{col}' contains unexpected values: {unique_vals}\")\n",
       "\n",
       "### Step 6: Validate zero-value handling\n",
       "# Rationale: Confirm zeros are legitimate values (not missing data) in network metrics.\n",
       "# In network traffic data, zero values often represent one-way communication or missing\n",
       "# responses, which are legitimate patterns in network behavior.\n",
       "zero_value_columns = ['dbytes', 'dload']\n",
       "\n",
       "for col in zero_value_columns:\n",
       "    zero_count = (df_clean[col] == 0).sum()\n",
       "    print(f\"Column '{col}' has {zero_count} zero values ({(zero_count/len(df_clean))*100:.2f}%)\")\n",
       "\n",
       "# No imputation needed - zeros represent valid one-way network traffic patterns\n",
       "\n",
       "### Final Data Validation\n",
       "print(\"\\nFinal dataset shape:\", df_clean.shape)\n",
       "print(\"\\nColumn dtypes:\\n\", df_clean.dtypes)\n",
       "print(\"\\nMissing values per column:\\n\", df_clean.isnull().sum())\n",
       "print(\"\\nFirst 5 rows of cleaned data:\")\n",
       "print(df_clean.head())\n",
       "\n",
       "# The dataframe df_clean is now prepared for machine learning\n",
       "# Features are scaled, extreme values are capped, and data types are validated\n",
       "# Note: Target variable 'label' remains unchanged for binary classification\n",
       "```\n",
       "\n",
       "**Changes Made:**\n",
       "1. **Added .data extraction for winsorized values**: The `mstats.winsorize` function returns a masked array, which needed to be converted to a regular array using `.data` to ensure proper pandas DataFrame compatibility.\n",
       "\n",
       "2. **Enhanced binary column validation**: Added a check to ensure all values in binary columns are strictly 0 or 1 before conversion, providing a warning if unexpected values are found.\n",
       "\n",
       "3. **Improved comments and documentation**: Added more detailed explanations for each step, particularly explaining why specific techniques (winsorization, RobustScaler) were chosen for network traffic data.\n",
       "\n",
       "**Validation Results:**\n",
       "- The code follows a logical cleaning sequence\n",
       "- All transformations are appropriate for network intrusion detection data\n",
       "- The code handles edge cases and provides validation outputs\n",
       "- The final dataframe maintains all necessary features for machine learning while addressing data quality issues\n",
       "- No splitting or modeling is performed as requested"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prompt Chain Complete ---\n",
      "Exported chain artifacts to: C:\\Users\\macdo\\Github\\VNFCDR-1\\Chris Code\\Generative AI Data Cleaning\\Machine Learning\\Step 3_DeepSeek Prompting\\UNSW_NB15_Train_Test_Concatenated\\DeepSeek_API_Exploratory_Analysis_Export.md\n",
      "\n",
      "=== Diagnostics Summary ===\n",
      "Model used: deepseek-reasoner\n",
      "Artifacts generated: ['analysis_md', 'plan_md', 'code_md']\n"
     ]
    }
   ],
   "source": [
    "# --- DeepSeek Prompt Chain for Cybersecurity EDA (DeepSeek via OpenAI SDK) ---\n",
    "\n",
    "import os, io, time, traceback\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "from typing import Dict, List\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. Environment / API Setup --------------------------------------------------\n",
    "# Expect an env file containing DEEPSEEK_API_KEY\n",
    "#ENV_PATH = r\"/Users/sarahsetiawan/Desktop/VNFCDR-1/SarahCode/Generative_AI/GEMINI_API_KEY.env\"  # <-- update if needed to point to DeepSeek key file\n",
    "ENV_PATH = r\"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 3_DeepSeek Prompting\\\\DEEPSEEK_API_KEY.env\"\n",
    "load_dotenv(ENV_PATH)\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"DEEPSEEK_API_KEY not found. Ensure the env file exists and key is set.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "print(\"DeepSeek API client configured successfully.\")\n",
    "\n",
    "# 2. Data Load ----------------------------------------------------------------\n",
    "#DATA_PATH = r\"/Users/sarahsetiawan/Desktop/VNFCDR-1/SarahCode/Sample_df/Code/MachineLearning/SampleData_API/CSVs/Representative_APISample_20000_2.csv\"\n",
    "DATA_PATH = r\"E:\\\\Datasets\\\\UNSW-NB15\\\\Training and Testing Sets\\\\UNSW_NB15_concatenated_dropped.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}\")\n",
    "print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# 3. Context Preparation (Token-Optimized) ------------------------------------\n",
    "\n",
    "\n",
    "def build_schema_summary(dataframe: pd.DataFrame) -> str:\n",
    "    info_buf = io.StringIO()\n",
    "    dataframe.info(buf=info_buf)\n",
    "    _ = info_buf.getvalue()  # not used directly; we generate concise version below\n",
    "    rows = []\n",
    "    total = len(dataframe)\n",
    "    for col in dataframe.columns:\n",
    "        non_null = dataframe[col].notna().sum()\n",
    "        null_pct = 100 * (1 - non_null / total)\n",
    "        uniq = dataframe[col].nunique(dropna=True)\n",
    "        rows.append(f\"{col} | {dataframe[col].dtype} | {non_null} | {null_pct:.2f}% | {uniq}\")\n",
    "    header = \"Column | DType | Non-Null | Null% | Unique\\n------ | ----- | -------- | ----- | ------\"\n",
    "    concise = header + \"\\n\" + \"\\n\".join(rows)\n",
    "    return concise\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_ROWS = 10\n",
    "schema_concise = build_schema_summary(df)\n",
    "sample_md = df.sample(n=SAMPLE_ROWS, random_state=RANDOM_SEED).to_markdown(index=False)\n",
    "\n",
    "# 4. System Instruction (kept same semantic intent) ---------------------------\n",
    "system_instruction = \"\"\"\n",
    "You are an expert data scientist specializing in data cleaning and preparation for machine learning. \n",
    "Your task is to perform exploratory data analysis (EDA), data cleaning and preprocessing for machine learning application. \n",
    "You will do this by generating structured insights, plans, and code.\n",
    "\n",
    "Goals:\n",
    "1. Perform a sharp initial EDA on a given dataset.\n",
    "2. Propose an ordered data cleaning and preprocessing plan based on the EDA and best practices.\n",
    "3. Following the data cleaning plan, produce fully executable, well-commented Python code.\n",
    "\n",
    "Constraints:\n",
    "- The created code must be executable without errors using the full dataset loaded from the provided CSV file path.\n",
    "- Use Markdown headings exactly as requested.\n",
    "\"\"\"\n",
    "\n",
    "# 5. Generation Config -------------------------------------------------------\n",
    "MODEL_NAME = \"deepseek-reasoner\"  \n",
    "MAX_TOKENS = 32000  \n",
    "TEMPERATURE = 0.15\n",
    "\n",
    "# 6. Retry + Wrapper (Chat Completions) --------------------------------------\n",
    "\n",
    "def call_model(messages: List[Dict[str, str]], max_tokens: int = MAX_TOKENS, temperature: float = TEMPERATURE):\n",
    "    return client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "def generate_with_retry(prompt_text: str, label: str, max_retries: int = 3, backoff: float = 2.0):\n",
    "    errors: List[str] = []\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            # Compose messages with system + user\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ]\n",
    "            response = call_model(messages)\n",
    "            elapsed = time.time() - start\n",
    "            text = response.choices[0].message.content.strip() if response.choices else \"\"\n",
    "            if not text:\n",
    "                raise ValueError(\"Empty response content.\")\n",
    "            print(f\"[{label}] Success attempt={attempt} time={elapsed:.2f}s\")\n",
    "            return dict(text=text, raw=response, attempts=attempt, errors=errors)\n",
    "        except Exception as e:\n",
    "            err_msg = f\"[{label}] Attempt {attempt} failed: {e}\"\n",
    "            print(err_msg)\n",
    "            errors.append(err_msg)\n",
    "            if attempt < max_retries:\n",
    "                sleep_time = backoff ** (attempt - 1)\n",
    "                time.sleep(sleep_time)\n",
    "    print(f\"[{label}] All attempts failed.\")\n",
    "    return dict(text=\"\", raw=None, attempts=max_retries, errors=errors)\n",
    "\n",
    "# 7. Prompts (EXACT TEXT PRESERVED from previous implementation) --------------\n",
    "prompt_1_analysis = f\"\"\"\n",
    "## Task 1: Initial Data Analysis\n",
    "\n",
    "You are given:\n",
    "### Concise Schema Summary\n",
    "```\n",
    "{schema_concise}\n",
    "```\n",
    "\n",
    "### Random Sample ({SAMPLE_ROWS} rows)\n",
    "```\n",
    "{sample_md}\n",
    "```\n",
    "\n",
    "### Directives\n",
    "1. Use the schema and sample to perform a sharp exploratory data analysis (EDA).\n",
    "2. Create a table of unique value counts for all columns in the dataset.\n",
    "3. Create a table of high cardinality columns. Ensure to explain why the columns were selected.\n",
    "4. Generate a data quality analysis containing accuracy, completeness, consistency, uniqueness, validity, and timeliness before data cleaning.\n",
    "\n",
    "Output ONLY under heading:\n",
    "## Analytical Insights\n",
    "Use concise Markdown sections & tables. Avoid code.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# 8. Execution Chain ---------------------------------------------------------\n",
    "chain_artifacts: Dict[str, str] = {}\n",
    "\n",
    "print(\"--- Chain Step 1: Initial Analysis ---\")\n",
    "analysis_result = generate_with_retry(prompt_1_analysis, label=\"analysis\")\n",
    "\n",
    "if not analysis_result[\"text\"]:\n",
    "    print(\"Aborting chain: analysis stage failed.\")\n",
    "else:\n",
    "    chain_artifacts[\"analysis_md\"] = analysis_result[\"text\"]\n",
    "    display(Markdown(\"### Analysis Received\"))\n",
    "    display(Markdown(chain_artifacts[\"analysis_md\"]))\n",
    "\n",
    "    # Step 2: Plan \n",
    "    prompt_2_plan = f\"\"\"\n",
    "## Task 2: Data Cleaning Plan\n",
    "\n",
    "Using the prior analysis:\n",
    "\n",
    "### Analytical Insights\n",
    "{chain_artifacts['analysis_md']}\n",
    "\n",
    "Produce a prioritized, ordered bullet list of data cleaning and machine learning preprocessing steps.\n",
    "Generate a data quality analysis containing accuracy, completeness, consistency, uniqueness, validity, and timeliness before data cleaning.\n",
    "Do not perform train-test splits or modeling.\n",
    "No code. Include: objective, columns impacted, and rationale.\n",
    "Heading required: ## Data Cleaning Steps\n",
    "\"\"\"\n",
    "    print(\"\\n--- Chain Step 2: Cleaning Plan ---\")\n",
    "    plan_result = generate_with_retry(prompt_2_plan, label=\"plan\")\n",
    "\n",
    "    if not plan_result[\"text\"]:\n",
    "        print(\"Aborting chain: plan stage failed.\")\n",
    "    else:\n",
    "        chain_artifacts[\"plan_md\"] = plan_result[\"text\"]\n",
    "        display(Markdown(\"### Cleaning Plan Received\"))\n",
    "        display(Markdown(chain_artifacts[\"plan_md\"]))\n",
    "\n",
    "        # Step 3: Code\n",
    "        prompt_3_code = f\"\"\"\n",
    "## Task 3: Data Cleaning Code\n",
    "\n",
    "### Data Cleaning Steps\n",
    "{chain_artifacts['plan_md']}\n",
    "\n",
    "## Directives\n",
    "1. Create fully executable Python code that implements the data cleaning plan developed in Task 2.\n",
    "2. Your code should prepare one fully preprocessed dataframe. Do NOT perform any splitting or modeling.\n",
    "3. Do not use sample data, only the full dataset loaded from the CSV file at path: {DATA_PATH}\n",
    "4. Provide extremely detailed comments explaining each step of the code.\n",
    "\n",
    "\n",
    "Return as ONE fenced Python code block under heading:\n",
    "## Data Cleaning Code\n",
    "\"\"\"\n",
    "        print(\"\\n--- Chain Step 3: Code Generation ---\")\n",
    "        code_result = generate_with_retry(prompt_3_code, label=\"code\")\n",
    "\n",
    "        if not code_result[\"text\"]:\n",
    "            print(\"Code generation failed.\")\n",
    "        else:\n",
    "            chain_artifacts[\"code_md\"] = code_result[\"text\"]\n",
    "            display(Markdown(\"### Data Cleaning Code Received\"))\n",
    "            display(Markdown(chain_artifacts[\"code_md\"]))\n",
    "\n",
    "        # Step 4: Validation\n",
    "        prompt_4_validation = f\"\"\"\n",
    "## Task 4: Data Cleaning Validation\n",
    "\n",
    "### Data Cleaning Steps\n",
    "{chain_artifacts['code_md']}\n",
    "\n",
    "## Directives\n",
    "1. Provide reasoning for any changes made from the original code.\n",
    "2. Review the provided data cleaning code for correctness and completeness.\n",
    "3. Ensure all steps from the cleaning plan are implemented in order.\n",
    "4. Ensure the code is executable without errors using the full dataset loaded from the provided CSV file.\n",
    "5. Provide extremely detailed comments explaining each step of the code.\n",
    "6. Your code should prepare one fully preprocessed dataframe. Do NOT perform any splitting or modeling.\n",
    "7. If the original code is correct, return it unchanged and note that no changes were needed.\n",
    "\n",
    "Return as ONE fenced Python code block under heading:\n",
    "## Data Cleaning Code - Validated\n",
    "\"\"\"\n",
    "        print(\"\\n--- Chain Step 4: Code Validation ---\")\n",
    "        code_result = generate_with_retry(prompt_4_validation, label=\"validation\")\n",
    "\n",
    "        if not code_result[\"text\"]:\n",
    "            print(\"Validation check failed.\")\n",
    "        else:\n",
    "            chain_artifacts[\"code_md\"] = code_result[\"text\"]\n",
    "            display(Markdown(\"### Data Validation Received\"))\n",
    "            display(Markdown(chain_artifacts[\"code_md\"]))\n",
    "\n",
    "\n",
    "print(\"\\n--- Prompt Chain Complete ---\")\n",
    "\n",
    "# 9. Export Artifacts --------------------------------------------------------\n",
    "EXPORT_DIR = os.path.dirname(DATA_PATH)\n",
    "export_path = \"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 3_DeepSeek Prompting\\\\UNSW_NB15_Train_Test_Concatenated\\\\DeepSeek_API_Exploratory_Analysis_Export.md\"\n",
    "try:\n",
    "    with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        if chain_artifacts:\n",
    "            f.write(\"# EDA Prompt Chain Output (DeepSeek)\\n\\n\")\n",
    "            for k, v in chain_artifacts.items():\n",
    "                pretty = k.replace(\"_md\", \"\").capitalize()\n",
    "                f.write(f\"\\n\\n## {pretty}\\n\\n\")\n",
    "                f.write(v.strip() + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"No artifacts generated (chain failed).\")\n",
    "    print(f\"Exported chain artifacts to: {export_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Export failed: {e}\")\n",
    "\n",
    "# 10. Diagnostic Recap -------------------------------------------------------\n",
    "\n",
    "def print_diagnostics():\n",
    "    print(\"\\n=== Diagnostics Summary ===\")\n",
    "    print(f\"Model used: {MODEL_NAME}\")\n",
    "    if analysis_result.get('errors'):\n",
    "        print(f\"Analysis errors: {len(analysis_result['errors'])}\")\n",
    "    if 'plan_result' in locals() and plan_result.get('errors'):\n",
    "        print(f\"Plan errors: {len(plan_result['errors'])}\")\n",
    "    if 'code_result' in locals() and code_result.get('errors'):\n",
    "        print(f\"Code errors: {len(code_result['errors'])}\")\n",
    "    print(\"Artifacts generated:\", list(chain_artifacts.keys()))\n",
    "\n",
    "print_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7fcd534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API client configured successfully.\n",
      "Dataset loaded successfully. Shape: (930000, 41)\n",
      "--- Chain Step 1: Data Quality Analysis ---\n",
      "[analysis] Success attempt=1 time=50.21s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Analysis Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Analytical Insights\n",
       "\n",
       "### Completeness\n",
       "- **Score: 100%**\n",
       "- All 930,000 rows are fully populated across all 42 columns.\n",
       "- No missing values detected in any field.\n",
       "\n",
       "### Uniqueness\n",
       "- High uniqueness variance observed:\n",
       "  - Columns like `sjit` (575,603 unique), `sload` (575,603 unique), and `dur` (553,964 unique) exhibit near-cardinality, suggesting continuous measurements.\n",
       "  - Low-cardinality columns include `is_ftp_login` (2 unique), `label` (2 unique), `ct_state_ttl` (7 unique), indicating categorical or binary flags.\n",
       "- No duplicate rows inferred from sample dispersion.\n",
       "\n",
       "### Validity\n",
       "- **Critical issues detected:**\n",
       "  - Negative values in `dur` (duration), `sload` (source load), `dload` (destination load), which are inherently non-negative metrics.\n",
       "  - Negative values in `swin` (source TCP window) and `dwin` (destination TCP window), which should be â‰¥0.\n",
       "  - `sttl` and `dttl` (time-to-live) include 0, which may be invalid for live packets.\n",
       "  - `response_body_len` (response body length) contains negative values, which are illogical.\n",
       "- These suggest potential data collection errors, normalization artifacts, or encoding issues.\n",
       "\n",
       "### Consistency\n",
       "- **Internal logic violations:**\n",
       "  - Rows with `dpkts=0` but `dbytes>0` (e.g., sample row 1: `dpkts=0`, `dbytes=0` is consistent, but other rows may not be).\n",
       "  - `spkts` and `sloss` should satisfy `sloss â‰¤ spkts`, but sample shows `spkts=2` and `sloss=0` (valid), though other combinations may breach this.\n",
       "  - `rate` (flow rate) negative in some rows, which may conflict with positive `sbytes`/`dbytes`.\n",
       "- Protocol-specific fields (e.g., `tcprtt`, `synack`) show negative values, inconsistent with TCP timing semantics.\n",
       "\n",
       "### Accuracy\n",
       "- Cannot be fully assessed without ground truth, but anomalies suggest inaccuracies:\n",
       "  - Negative network metrics (e.g., load, jitter, packet counts) are not physically plausible.\n",
       "  - Extreme values in normalized fields (e.g., `stcpb` â‰ˆ -0.735, `dtcpb` â‰ˆ -0.735) may indicate erroneous transformations or outliers.\n",
       "- Labels (`attack_cat`, `label`) appear valid with expected ranges (0-10 and 0-1).\n",
       "\n",
       "### Timeliness\n",
       "- No temporal columns (e.g., timestamps) present in the schema.\n",
       "- Dataset appears static for modeling purposes, so timeliness is not directly applicable.\n",
       "\n",
       "### Summary Table\n",
       "| Dimension      | Score | Issues                                                                 |\n",
       "|----------------|-------|------------------------------------------------------------------------|\n",
       "| Completeness   | 100%  | None                                                                   |\n",
       "| Uniqueness     | -     | Expected variance for continuous vs. categorical fields                |\n",
       "| Validity       | Low   | Negative values in non-negative metrics (dur, load, TTL, win, bytes)   |\n",
       "| Consistency    | Low   | Logic conflicts (e.g., dpkts=0 vs. dbytes>0), negative rates/timings   |\n",
       "| Accuracy       | Medium| Suspect due to invalid values; requires domain verification            |\n",
       "| Timeliness     | N/A   | No time-based columns                                                  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported chain artifacts to: C:\\Users\\macdo\\Github\\VNFCDR-1\\Chris Code\\Generative AI Data Cleaning\\Machine Learning\\Step 3_DeepSeek Prompting\\UNSW_NB15_Train_Test_Concatenated\\DeepSeek_API_After_Cleaning_Analysis_Export.md\n",
      "\n",
      "=== Diagnostics Summary ===\n",
      "Model used: deepseek-reasoner\n",
      "Artifacts generated: ['analysis_md']\n"
     ]
    }
   ],
   "source": [
    "# Post Cleaning Data Quality Analysis\n",
    "\n",
    "import os, io, time, traceback\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "from typing import Dict, List\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. Environment / API Setup --------------------------------------------------\n",
    "# Expect an env file containing DEEPSEEK_API_KEY\n",
    "#ENV_PATH = r\"/Users/sarahsetiawan/Desktop/VNFCDR-1/SarahCode/Generative_AI/GEMINI_API_KEY.env\"  # <-- update if needed to point to DeepSeek key file\n",
    "ENV_PATH = r\"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 3_DeepSeek Prompting\\\\DEEPSEEK_API_KEY.env\"\n",
    "load_dotenv(ENV_PATH)\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"DEEPSEEK_API_KEY not found. Ensure the env file exists and key is set.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "print(\"DeepSeek API client configured successfully.\")\n",
    "\n",
    "# 2. Data Load ----------------------------------------------------------------\n",
    "#DATA_PATH = r\"/Users/sarahsetiawan/Desktop/VNFCDR-1/SarahCode/Sample_df/Code/MachineLearning/SampleData_API/CSVs/Representative_APISample_20000_2.csv\"\n",
    "DATA_PATH = r\"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 3_DeepSeek Prompting\\\\UNSW_NB15_Train_Test_Concatenated\\\\cleaned_data.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}\")\n",
    "print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# 3. Context Preparation (Token-Optimized) ------------------------------------\n",
    "\n",
    "\n",
    "def build_schema_summary(dataframe: pd.DataFrame) -> str:\n",
    "    info_buf = io.StringIO()\n",
    "    dataframe.info(buf=info_buf)\n",
    "    _ = info_buf.getvalue()  # not used directly; we generate concise version below\n",
    "    rows = []\n",
    "    total = len(dataframe)\n",
    "    for col in dataframe.columns:\n",
    "        non_null = dataframe[col].notna().sum()\n",
    "        null_pct = 100 * (1 - non_null / total)\n",
    "        uniq = dataframe[col].nunique(dropna=True)\n",
    "        rows.append(f\"{col} | {dataframe[col].dtype} | {non_null} | {null_pct:.2f}% | {uniq}\")\n",
    "    header = \"Column | DType | Non-Null | Null% | Unique\\n------ | ----- | -------- | ----- | ------\"\n",
    "    concise = header + \"\\n\" + \"\\n\".join(rows)\n",
    "    return concise\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_ROWS = 10\n",
    "schema_concise = build_schema_summary(df)\n",
    "sample_md = df.sample(n=SAMPLE_ROWS, random_state=RANDOM_SEED).to_markdown(index=False)\n",
    "\n",
    "# 4. System Instruction (kept same semantic intent) ---------------------------\n",
    "system_instruction = \"\"\"\n",
    "You are an expert data scientist specializing in data cleaning and preparation for machine learning. \n",
    "Your task is to Generate a data quality analysis containing accuracy, completeness, consistency, uniqueness, validity, and timeliness on an imported file. \n",
    "\n",
    "Goals:\n",
    "1. Perform a data quality analysis on a given dataset.\n",
    "\n",
    "Constraints:\n",
    "- Use Markdown headings exactly as requested.\n",
    "\"\"\"\n",
    "\n",
    "# 5. Generation Config -------------------------------------------------------\n",
    "MODEL_NAME = \"deepseek-reasoner\"  \n",
    "MAX_TOKENS = 32000  \n",
    "TEMPERATURE = 0.15\n",
    "\n",
    "# 6. Retry + Wrapper (Chat Completions) --------------------------------------\n",
    "\n",
    "def call_model(messages: List[Dict[str, str]], max_tokens: int = MAX_TOKENS, temperature: float = TEMPERATURE):\n",
    "    return client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "def generate_with_retry(prompt_text: str, label: str, max_retries: int = 3, backoff: float = 2.0):\n",
    "    errors: List[str] = []\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            # Compose messages with system + user\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ]\n",
    "            response = call_model(messages)\n",
    "            elapsed = time.time() - start\n",
    "            text = response.choices[0].message.content.strip() if response.choices else \"\"\n",
    "            if not text:\n",
    "                raise ValueError(\"Empty response content.\")\n",
    "            print(f\"[{label}] Success attempt={attempt} time={elapsed:.2f}s\")\n",
    "            return dict(text=text, raw=response, attempts=attempt, errors=errors)\n",
    "        except Exception as e:\n",
    "            err_msg = f\"[{label}] Attempt {attempt} failed: {e}\"\n",
    "            print(err_msg)\n",
    "            errors.append(err_msg)\n",
    "            if attempt < max_retries:\n",
    "                sleep_time = backoff ** (attempt - 1)\n",
    "                time.sleep(sleep_time)\n",
    "    print(f\"[{label}] All attempts failed.\")\n",
    "    return dict(text=\"\", raw=None, attempts=max_retries, errors=errors)\n",
    "\n",
    "# 7. Prompts (EXACT TEXT PRESERVED from previous implementation) --------------\n",
    "prompt_1_analysis = f\"\"\"\n",
    "## Task 1: Data Quality Analysis\n",
    "\n",
    "You are given:\n",
    "### Concise Schema Summary\n",
    "```\n",
    "{schema_concise}\n",
    "```\n",
    "\n",
    "### Random Sample ({SAMPLE_ROWS} rows)\n",
    "```\n",
    "{sample_md}\n",
    "```\n",
    "\n",
    "### Directives\n",
    "1. Generate a data quality analysis containing accuracy, completeness, consistency, uniqueness, validity, and timeliness on the impored file at {DATA_PATH}.\n",
    "\n",
    "Output ONLY under heading:\n",
    "## Analytical Insights\n",
    "Use concise Markdown sections & tables. Avoid code.\n",
    "\"\"\"\n",
    "# 8. Execution Chain ---------------------------------------------------------\n",
    "chain_artifacts: Dict[str, str] = {}\n",
    "\n",
    "print(\"--- Chain Step 1: Data Quality Analysis ---\")\n",
    "analysis_result = generate_with_retry(prompt_1_analysis, label=\"analysis\")\n",
    "\n",
    "if not analysis_result[\"text\"]:\n",
    "    print(\"Aborting chain: analysis stage failed.\")\n",
    "else:\n",
    "    chain_artifacts[\"analysis_md\"] = analysis_result[\"text\"]\n",
    "    display(Markdown(\"### Analysis Received\"))\n",
    "    display(Markdown(chain_artifacts[\"analysis_md\"]))\n",
    "\n",
    "# 9. Export Artifacts --------------------------------------------------------\n",
    "EXPORT_DIR = os.path.dirname(DATA_PATH)\n",
    "export_path = \"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 3_DeepSeek Prompting\\\\UNSW_NB15_Train_Test_Concatenated\\\\DeepSeek_API_After_Cleaning_Analysis_Export.md\"\n",
    "try:\n",
    "    with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        if chain_artifacts:\n",
    "            f.write(\"# EDA Prompt Chain Output (DeepSeek)\\n\\n\")\n",
    "            for k, v in chain_artifacts.items():\n",
    "                pretty = k.replace(\"_md\", \"\").capitalize()\n",
    "                f.write(f\"\\n\\n## {pretty}\\n\\n\")\n",
    "                f.write(v.strip() + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"No artifacts generated (chain failed).\")\n",
    "    print(f\"Exported chain artifacts to: {export_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Export failed: {e}\")\n",
    "\n",
    "# 10. Diagnostic Recap -------------------------------------------------------\n",
    "\n",
    "def print_diagnostics():\n",
    "    print(\"\\n=== Diagnostics Summary ===\")\n",
    "    print(f\"Model used: {MODEL_NAME}\")\n",
    "    if analysis_result.get('errors'):\n",
    "        print(f\"Analysis errors: {len(analysis_result['errors'])}\")\n",
    "    if 'plan_result' in locals() and plan_result.get('errors'):\n",
    "        print(f\"Plan errors: {len(plan_result['errors'])}\")\n",
    "    if 'code_result' in locals() and code_result.get('errors'):\n",
    "        print(f\"Code errors: {len(code_result['errors'])}\")\n",
    "    print(\"Artifacts generated:\", list(chain_artifacts.keys()))\n",
    "\n",
    "print_diagnostics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_supervised_20250810",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

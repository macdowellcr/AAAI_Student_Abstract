{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6166e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API client configured successfully.\n",
      "Dataset loaded successfully. Shape: (257673, 42)\n",
      "--- Chain Step 1: Initial Analysis ---\n",
      "[analysis] Success attempt=1 time=100.13s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Analysis Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Analytical Insights\n",
       "\n",
       "### Dataset overview\n",
       "- Rows: 257,673; Columns: 42\n",
       "- Types: 41 numeric (int/float), 1 categorical (attack_cat), 1 binary label\n",
       "- Nulls: 0% across all columns\n",
       "- Target: label (binary) and attack_cat (10 classes). Sample shows mapping: Normal → label 0; other categories → label 1.\n",
       "\n",
       "### Initial EDA highlights (from schema and sample)\n",
       "- Feature scale/dispersion:\n",
       "  - Many “rate/load/time” features (dur, rate, sload, dload, sinpkt, dinpkt, sjit, djit, tcprtt, synack, ackdat) exhibit very high cardinality, implying continuous/heavy-tailed distributions. Extremely small dur values in sample create very large sload/dload/rate outliers.\n",
       "  - Byte/packet features (sbytes, dbytes, spkts, dpkts) show moderate-to-high cardinality and likely strong positive skew.\n",
       "- Low-cardinality protocol/state features:\n",
       "  - sttl (13), dttl (9), swin (22), dwin (19), ct_state_ttl (7), ct_flw_http_mthd (11), is_sm_ips_ports (2), is_ftp_login (4), ct_ftp_cmd (4) — amenable to integer/bin encoding or small one-hot.\n",
       "- Potentially derived/collinear groups:\n",
       "  - sload/dload likely functions of bytes and time; tcprtt ~ synack + ackdat; sbytes/dbytes with spkts/dpkts; suggests multicollinearity to manage during modeling.\n",
       "- Identifiers:\n",
       "  - id has 175,341 unique values (≈68% of rows), indicating duplicates of id exist; id should not be used as a predictive feature.\n",
       "- Class distribution:\n",
       "  - Not provided; given 10 attack_cat classes, class imbalance is plausible and should be quantified later.\n",
       "\n",
       "### Unique value counts (all columns)\n",
       "| Column | Unique |\n",
       "|---|---:|\n",
       "| id | 175341 |\n",
       "| dur | 109945 |\n",
       "| spkts | 646 |\n",
       "| dpkts | 627 |\n",
       "| sbytes | 9382 |\n",
       "| dbytes | 8653 |\n",
       "| rate | 115763 |\n",
       "| sttl | 13 |\n",
       "| dttl | 9 |\n",
       "| sload | 121356 |\n",
       "| dload | 116380 |\n",
       "| sloss | 490 |\n",
       "| dloss | 476 |\n",
       "| sinpkt | 114318 |\n",
       "| dinpkt | 110270 |\n",
       "| sjit | 117101 |\n",
       "| djit | 114861 |\n",
       "| swin | 22 |\n",
       "| stcpb | 114473 |\n",
       "| dtcpb | 114187 |\n",
       "| dwin | 19 |\n",
       "| tcprtt | 63878 |\n",
       "| synack | 57366 |\n",
       "| ackdat | 53248 |\n",
       "| smean | 1377 |\n",
       "| dmean | 1362 |\n",
       "| trans_depth | 14 |\n",
       "| response_body_len | 2819 |\n",
       "| ct_srv_src | 57 |\n",
       "| ct_state_ttl | 7 |\n",
       "| ct_dst_ltm | 52 |\n",
       "| ct_src_dport_ltm | 52 |\n",
       "| ct_dst_sport_ltm | 35 |\n",
       "| ct_dst_src_ltm | 58 |\n",
       "| is_ftp_login | 4 |\n",
       "| ct_ftp_cmd | 4 |\n",
       "| ct_flw_http_mthd | 11 |\n",
       "| ct_src_ltm | 52 |\n",
       "| ct_srv_dst | 57 |\n",
       "| is_sm_ips_ports | 2 |\n",
       "| attack_cat | 10 |\n",
       "| label | 2 |\n",
       "\n",
       "### High-cardinality columns\n",
       "Criterion: unique values ≥ 1% of rows (≥ 2,577 distinct), which signals near-continuous features that should not be one-hot encoded and may need scaling/outlier treatment.\n",
       "\n",
       "| Column | Unique | % of rows | Rationale |\n",
       "|---|---:|---:|---|\n",
       "| id | 175341 | 68.1% | Identifier; high uniqueness; exclude from modeling to avoid leakage/noise. |\n",
       "| dur | 109945 | 42.7% | Continuous time; heavy tail due to tiny durations. |\n",
       "| rate | 115763 | 44.9% | Derived rate; large spread; sensitive to small dur. |\n",
       "| sload | 121356 | 47.1% | Load metric; heavy-tailed; closely tied to sbytes/dur. |\n",
       "| dload | 116380 | 45.1% | As above for destination. |\n",
       "| sinpkt | 114318 | 44.4% | Inter-packet timing; continuous, skewed. |\n",
       "| dinpkt | 110270 | 42.8% | As above for destination. |\n",
       "| sjit | 117101 | 45.4% | Jitter; continuous, heavy-tailed. |\n",
       "| djit | 114861 | 44.6% | As above for destination. |\n",
       "| stcpb | 114473 | 44.4% | TCP base/seq-like; near-continuous; may encode order rather than magnitude. |\n",
       "| dtcpb | 114187 | 44.3% | As above for destination. |\n",
       "| tcprtt | 63878 | 24.8% | RTT; continuous; likely correlated with synack/ackdat. |\n",
       "| synack | 57366 | 22.3% | TCP phase timing; continuous. |\n",
       "| ackdat | 53248 | 20.7% | TCP phase timing; continuous. |\n",
       "| sbytes | 9382 | 3.6% | Byte counts; skewed; many unique values. |\n",
       "| dbytes | 8653 | 3.4% | As above for destination. |\n",
       "| response_body_len | 2819 | 1.1% | HTTP payload length; moderate-high variety. |\n",
       "\n",
       "Why selected: These columns are near-continuous with many unique values relative to dataset size. One-hot encoding would explode dimensionality and add noise; appropriate treatment includes numeric scaling, winsorization/clipping, or log transforms. id is included as high-cardinality but should be dropped as it is an identifier.\n",
       "\n",
       "### Data quality analysis (pre-cleaning)\n",
       "\n",
       "- Completeness\n",
       "  - Strength: 0% missing across all columns (per schema).\n",
       "  - Caveat: Structural zeros may represent “not applicable” rather than true zeros (e.g., many zero values in synack/ackdat/tcprtt for some flows). Flag for semantic-missing assessment.\n",
       "\n",
       "- Accuracy\n",
       "  - Unknown without external ground truth. Plausibility checks advised:\n",
       "    - Non-negativity for counts/times (all samples consistent).\n",
       "    - TCP timing consistency: tcprtt ≈ synack + ackdat should roughly hold.\n",
       "    - sload/dload/rate consistency with bytes/duration; extreme ratios likely when dur ~ 0.\n",
       "\n",
       "- Consistency\n",
       "  - Categorical alignment: Sample shows attack_cat = Normal implies label = 0; non-Normal implies label = 1. Verify dataset-wide.\n",
       "  - Domain coherence:\n",
       "    - sttl/dttl small value sets are consistent with TTL/state buckets.\n",
       "    - Window sizes (swin/dwin) and means (smean/dmean) within expected ranges in sample; verify upper bounds and MTU-related constraints.\n",
       "  - Derived relationships likely produce multicollinearity (e.g., sload with sbytes/dur; tcprtt with synack/ackdat). Not an error, but relevant for modeling.\n",
       "\n",
       "- Uniqueness\n",
       "  - id is not unique (175,341 unique vs. 257,673 rows), indicating repeated ids or session reuse. Assess whether duplicates are legitimate multi-record flows or require deduplication.\n",
       "  - No other explicit primary key present.\n",
       "\n",
       "- Validity\n",
       "  - Types: All numeric columns correctly typed; attack_cat is object (categorical).\n",
       "  - Value ranges: Counts/times non-negative in sample; TTL-like features within small sets; binary flags limited to 0/1 (is_sm_ips_ports) and small sets for is_ftp_login/ct_ftp_cmd. Verify full-range adherence and out-of-domain values.\n",
       "  - Class labels: Binary with 2 unique values; ensure only {0,1} present.\n",
       "\n",
       "- Timeliness\n",
       "  - No event timestamp present; dur is per-flow duration, not dataset recency. Timeliness/recency cannot be assessed from given schema. If modeling for detection in current environments, confirm data collection period and drift risk separately.\n",
       "\n",
       "- Distributional risks for modeling\n",
       "  - Heavy tails and extreme outliers in rate/load/time features due to very small durations.\n",
       "  - Potential class-conditional sparsity patterns (e.g., many zeros in timing features for certain attack types) may yield strong separability; validate that no data leakage from post-hoc features exists.\n",
       "  - Class imbalance likely across 10 attack categories; quantify before model training."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chain Step 2: Cleaning Plan ---\n",
      "[plan] Success attempt=1 time=101.32s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Cleaning Plan Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Data Quality Analysis (pre-cleaning)\n",
       "- Completeness\n",
       "  - 0% missing across all columns. Watch for structural zeros that mean “not applicable” (e.g., tcprtt/synack/ackdat/sinpkt/dinpkt can be 0 by design).\n",
       "- Accuracy\n",
       "  - Plausibility checks recommended: non-negativity for counts/times; tcprtt ≈ synack + ackdat; loads/rates consistent with bytes and duration. Extremely small dur can inflate rate/sload/dload; cap or transform.\n",
       "- Consistency\n",
       "  - Verify mapping: attack_cat == \"Normal\" implies label == 0; all others imply label == 1. Ensure no contradictions. Check domain ranges of TTL/window sizes. Confirm binary fields only take allowed values.\n",
       "- Uniqueness\n",
       "  - id is not unique (≈68% unique). Remove as a predictive feature and consider deduplicating exact duplicate rows to reduce bias.\n",
       "- Validity\n",
       "  - Types appear correct: 41 numeric, 1 categorical (attack_cat), 1 binary label. Ensure labels ∈ {0,1}, attack_cat in 10 known classes, and no out-of-domain values (e.g., negative durations/bytes).\n",
       "- Timeliness\n",
       "  - No event timestamps; cannot assess recency. If used for current detection, validate collection period and drift separately.\n",
       "\n",
       "## Data Cleaning Steps\n",
       "1) Schema lock and dtypes enforcement\n",
       "- Objective: Establish stable, correct data types and expected value domains to prevent downstream errors.\n",
       "- Columns impacted: All; special attention to attack_cat (category), label (integer/bool), low-cardinality flags, and numeric metrics.\n",
       "- Rationale: Explicit dtypes avoid implicit conversions and ensure consistent handling (e.g., categorical encoding vs numeric scaling).\n",
       "\n",
       "2) Identifier removal\n",
       "- Objective: Eliminate non-predictive identifiers to prevent leakage/noise.\n",
       "- Columns impacted: id (drop).\n",
       "- Rationale: High-cardinality identifier with no causal relationship; retaining can harm generalization.\n",
       "\n",
       "3) Exact duplicate handling\n",
       "- Objective: Remove duplicated records that can bias learning and metrics.\n",
       "- Columns impacted: All feature columns plus targets; define duplicates on all columns except id (already dropped).\n",
       "- Rationale: Exact duplicates overweight certain patterns and risk leakage if later split by row.\n",
       "\n",
       "4) Label–category consistency audit and repair\n",
       "- Objective: Ensure binary label aligns with attack_cat semantics.\n",
       "- Columns impacted: label, attack_cat.\n",
       "- Rationale: Inconsistent supervision corrupts training. If rows violate: either correct label based on attack_cat or exclude conflicting rows (document count).\n",
       "\n",
       "5) Domain validity checks and hard constraints\n",
       "- Objective: Enforce physical/semantic constraints; quarantine impossible rows.\n",
       "- Columns impacted: dur, bytes/packets (sbytes, dbytes, spkts, dpkts), timing fields (synack, ackdat, tcprtt, sinpkt, dinpkt, sjit, djit), loads/rates (sload, dload, rate), window/TTL (swin, dwin, sttl, dttl), binary flags.\n",
       "- Rationale: Negative values are invalid; window/TTL ranges should be non-negative and within plausible bounds. Either filter invalid rows or set offending values to NaN and impute (prefer filtering if rare).\n",
       "\n",
       "6) Continuous feature skew mitigation (log1p)\n",
       "- Objective: Stabilize variance and reduce heavy right tails.\n",
       "- Columns impacted: Positive/zero-valued skewed metrics: dur, rate, sload, dload, sbytes, dbytes, spkts, dpkts, sinpkt, dinpkt, sjit, djit, synack, ackdat, tcprtt, response_body_len, smean, dmean. Exclude zero/negative-including true centered measures; use log1p to safely handle zeros.\n",
       "- Rationale: Heavy tails degrade many models; log1p is robust for non-negative data and reduces sensitivity to tiny dur producing huge ratios.\n",
       "\n",
       "7) Extreme outlier capping (winsorization)\n",
       "- Objective: Limit undue influence of extreme values while preserving order.\n",
       "- Columns impacted: After log1p, apply per-feature caps on highly volatile features: rate, sload, dload, sinpkt, dinpkt, sjit, djit, synack, ackdat, tcprtt, dur, sbytes, dbytes.\n",
       "- Rationale: Even after transformation, tails may remain. Cap at robust quantiles (e.g., 0.1th–99.9th or 0.5th–99.5th) determined on training data to control leverage.\n",
       "\n",
       "8) Multicollinearity control among derived features\n",
       "- Objective: Reduce redundant information and improve model stability/interpretability.\n",
       "- Columns impacted: Derived groups: {rate, sload, dload} vs {sbytes, dbytes, dur}; {tcprtt} vs {synack, ackdat}; {smean, dmean} vs packet/byte counts; stcpb, dtcpb (sequence-like).\n",
       "- Rationale: Strong linear dependencies inflate variance in linear models and can confuse feature importance. Default plan:\n",
       "  - Drop tcprtt (keep synack, ackdat).\n",
       "  - Prefer core primitives: keep dur, sbytes, dbytes, spkts, dpkts; drop rate, sload, dload. Document alternative if model family benefits from rates.\n",
       "  - Drop stcpb, dtcpb (near-random magnitudes, not semantically meaningful).\n",
       "  - Optionally assess VIF/feature correlation to finalize drops; target VIF < 10.\n",
       "\n",
       "9) Categorical encoding for protocol/state flags\n",
       "- Objective: Properly represent nominal features without imposing false ordinality.\n",
       "- Columns impacted: One-hot encode ct_state_ttl (7), ct_flw_http_mthd (11), is_sm_ips_ports (2), is_ftp_login (4), ct_ftp_cmd (4). Keep sttl, dttl, swin, dwin as numeric (ordered, small-range).\n",
       "- Rationale: These are codes/flags or small sets; one-hot preserves semantics and is compact. Numeric TTL/window values contain magnitude information; treat as continuous.\n",
       "\n",
       "10) Zero-inflation indicators for structural zeros\n",
       "- Objective: Preserve informative absence patterns without distorting distributions.\n",
       "- Columns impacted: Add binary indicators for zero values in key timing/load fields: I(dur==0), I(sinpkt==0), I(dinpkt==0), I(sjit==0), I(djit==0), I(synack==0), I(ackdat==0), I(rate==0), I(sload==0), I(dload==0) before any dropping of derived features; if rate/sload/dload dropped, omit those indicators.\n",
       "- Rationale: Structural zeros can be class-informative; separate indicators let models benefit while main features are transformed/scaled.\n",
       "\n",
       "11) Scaling of continuous features\n",
       "- Objective: Put features on comparable scales robust to outliers.\n",
       "- Columns impacted: All continuous numeric features retained after steps 6–8 (including sttl, dttl, swin, dwin, counts, and transformed timings/bytes).\n",
       "- Rationale: RobustScaler (IQR-based) or StandardScaler (if tails are well-tamed) improves optimization and model performance, especially for distance/gradient-based models.\n",
       "\n",
       "12) Rare category handling (if any)\n",
       "- Objective: Prevent sparse dummies from overfitting.\n",
       "- Columns impacted: One-hot encoded categories with very low frequency (e.g., <0.1% of rows).\n",
       "- Rationale: Combine into an “Other” bucket during encoding to stabilize estimates.\n",
       "\n",
       "13) Class distribution profiling and artifact check\n",
       "- Objective: Quantify imbalance and ensure no leakage-like near-deterministic features.\n",
       "- Columns impacted: Targets (label, attack_cat) and all features for quick mutual information or separability scans.\n",
       "- Rationale: Guides downstream choices (e.g., class weights) and validates that post-hoc features do not trivially reveal the label. No modeling performed here; just profiling and documentation.\n",
       "\n",
       "14) Final integrity and export of clean feature matrix\n",
       "- Objective: Validate no NaNs/Infs after transformations; persist clean dataset for modeling.\n",
       "- Columns impacted: All; ensure finite values; verify dummy matrices align with categories seen in training data (to be fixed later during pipeline fit).\n",
       "- Rationale: Guarantees downstream code stability and reproducibility.\n",
       "\n",
       "Notes\n",
       "- Targets: Support both binary (label) and multiclass (attack_cat). Keep both; modelers can choose task.\n",
       "- Train/test considerations (deferred): When splitting later, stratify by target and consider grouping by id or duplicate keys to avoid leakage across folds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chain Step 3: Code Generation ---\n",
      "[code] Success attempt=1 time=128.11s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Data Cleaning Code Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Data Cleaning Code\n",
       "```python\n",
       "\"\"\"\n",
       "Data Cleaning and Preprocessing for UNSW-NB15 (concatenated) dataset.\n",
       "\n",
       "This script implements the cleaning plan described in Task 3:\n",
       "1) Schema lock and dtypes enforcement\n",
       "2) Identifier removal\n",
       "3) Exact duplicate handling\n",
       "4) Label–category consistency audit and repair\n",
       "5) Domain validity checks and hard constraints\n",
       "6) Continuous feature skew mitigation (log1p)\n",
       "7) Extreme outlier capping (winsorization)\n",
       "10) Zero-inflation indicators for structural zeros (applied BEFORE dropping derived features)\n",
       "8) Multicollinearity control among derived features\n",
       "9) Categorical encoding for selected protocol/state flags\n",
       "11) Scaling of continuous features\n",
       "12) Rare category handling (applied within step 9)\n",
       "13) Class distribution profiling and artifact check (non-modeling diagnostics)\n",
       "14) Final integrity checks and export\n",
       "\n",
       "Notes:\n",
       "- This code avoids splitting or modeling; it prepares a single preprocessed DataFrame.\n",
       "- It is defensive to slight schema variations by checking column existence before operations.\n",
       "- All steps print concise diagnostics to assist auditability without interrupting execution.\n",
       "\"\"\"\n",
       "\n",
       "import os\n",
       "import sys\n",
       "import math\n",
       "import warnings\n",
       "from pathlib import Path\n",
       "\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "from sklearn.preprocessing import RobustScaler\n",
       "\n",
       "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# Configuration\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "# Input CSV path (must exist)\n",
       "CSV_PATH = r\"E:\\Datasets\\UNSW-NB15\\Training and Testing Sets\\UNSW_NB15_concatenated_dropped.csv\"\n",
       "\n",
       "# Output (optional) - will attempt to write in the same directory\n",
       "OUTPUT_FILENAME = \"UNSW_NB15_preprocessed.csv\"\n",
       "\n",
       "# Robust winsorization quantiles (after log1p transformation)\n",
       "LOW_Q = 0.005\n",
       "HIGH_Q = 0.995\n",
       "\n",
       "# Rare category threshold (proportion of dataset); categories under this will be pooled into 'Other'\n",
       "RARE_CAT_THRESHOLD = 0.001  # 0.1%\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# Utilities\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "def normalize_attack_cat(val):\n",
       "    \"\"\"\n",
       "    Normalize attack category string to canonical form if recognized.\n",
       "    Returns (canonical_string, recognized_boolean)\n",
       "    \"\"\"\n",
       "    if pd.isna(val):\n",
       "        return val, False\n",
       "    s = str(val).strip().lower()\n",
       "    mapping = {\n",
       "        \"normal\": \"Normal\",\n",
       "        \"fuzzers\": \"Fuzzers\",\n",
       "        \"analysis\": \"Analysis\",\n",
       "        \"backdoor\": \"Backdoor\",\n",
       "        \"backdoors\": \"Backdoor\",  # sometimes plural\n",
       "        \"dos\": \"DoS\",\n",
       "        \"exploits\": \"Exploits\",\n",
       "        \"generic\": \"Generic\",\n",
       "        \"reconnaissance\": \"Reconnaissance\",\n",
       "        \"shellcode\": \"Shellcode\",\n",
       "        \"worms\": \"Worms\",\n",
       "    }\n",
       "    if s in mapping:\n",
       "        return mapping[s], True\n",
       "    return val, False  # return original if not recognized\n",
       "\n",
       "\n",
       "def print_step_header(step_text):\n",
       "    print(\"\\n\" + \"-\" * 80)\n",
       "    print(step_text)\n",
       "    print(\"-\" * 80)\n",
       "\n",
       "\n",
       "def safe_intersect(cols, df_columns):\n",
       "    return [c for c in cols if c in df_columns]\n",
       "\n",
       "\n",
       "def is_binary_series(s):\n",
       "    \"\"\"Check if a pandas Series contains only {0,1} (ignoring NaNs).\"\"\"\n",
       "    vals = pd.unique(s.dropna())\n",
       "    return set(vals).issubset({0, 1})\n",
       "\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# Load data\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"Loading data\")\n",
       "if not os.path.exists(CSV_PATH):\n",
       "    raise FileNotFoundError(f\"Input CSV not found at path: {CSV_PATH}\")\n",
       "\n",
       "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
       "print(f\"Loaded shape: {df.shape}\")\n",
       "print(f\"Columns: {list(df.columns)}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 1) Schema lock and dtypes enforcement\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"1) Schema lock and dtypes enforcement\")\n",
       "\n",
       "# Expected targets (keep both)\n",
       "target_label_col = \"label\"\n",
       "target_multiclass_col = \"attack_cat\"\n",
       "\n",
       "# Ensure 'label' is present; if not, raise error because it is a key target\n",
       "if target_label_col not in df.columns:\n",
       "    raise KeyError(\"Expected binary target column 'label' not found.\")\n",
       "\n",
       "# Enforce 'label' as numeric integer {0,1} if possible\n",
       "df[target_label_col] = pd.to_numeric(df[target_label_col], errors=\"coerce\").astype(\"Int64\")\n",
       "\n",
       "# Ensure 'attack_cat' exists; if not, we proceed but some checks will be skipped\n",
       "if target_multiclass_col in df.columns:\n",
       "    # Convert to string then category for consistent handling (allowing normalization later)\n",
       "    df[target_multiclass_col] = df[target_multiclass_col].astype(\"string\")\n",
       "else:\n",
       "    print(\"Warning: 'attack_cat' column not found; multiclass target-related checks will be skipped.\")\n",
       "\n",
       "# Convert all other numeric-like columns to numeric dtype when possible (coerce errors to NaN)\n",
       "non_target_cols = [c for c in df.columns if c not in [target_label_col, target_multiclass_col]]\n",
       "for col in non_target_cols:\n",
       "    # Try to detect numeric; if it contains any non-numeric, coerce to numeric (NaN) where needed.\n",
       "    if df[col].dtype == object:\n",
       "        # Heuristic: attempt to convert if majority of values are representable as numbers\n",
       "        converted = pd.to_numeric(df[col], errors=\"coerce\")\n",
       "        if converted.notna().mean() > 0.9:\n",
       "            df[col] = converted\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 2) Identifier removal\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"2) Identifier removal\")\n",
       "drop_id_cols = [c for c in [\"id\"] if c in df.columns]\n",
       "if drop_id_cols:\n",
       "    df.drop(columns=drop_id_cols, inplace=True)\n",
       "    print(f\"Dropped identifier columns: {drop_id_cols}\")\n",
       "else:\n",
       "    print(\"No identifier columns found to drop.\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 3) Exact duplicate handling\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"3) Exact duplicate handling\")\n",
       "before = len(df)\n",
       "df = df.drop_duplicates(ignore_index=True)\n",
       "after = len(df)\n",
       "print(f\"Removed {before - after} exact duplicate rows. New shape: {df.shape}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 4) Label–category consistency audit and repair\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"4) Label–category consistency audit and repair\")\n",
       "if target_multiclass_col in df.columns:\n",
       "    # Normalize attack_cat values to canonical names if recognized\n",
       "    normalized_vals = df[target_multiclass_col].apply(normalize_attack_cat)\n",
       "    df[target_multiclass_col] = normalized_vals.apply(lambda x: x[0])\n",
       "    recognized_mask = normalized_vals.apply(lambda x: x[1])\n",
       "\n",
       "    # Derive label from attack_cat when recognized\n",
       "    recognized_cats = df.loc[recognized_mask, target_multiclass_col]\n",
       "    derived_label = pd.Series(index=df.index, dtype=\"Int64\")\n",
       "    derived_label.loc[recognized_mask] = (recognized_cats != \"Normal\").astype(\"Int64\")\n",
       "\n",
       "    # Identify and repair contradictions (only where derived_label is known)\n",
       "    mismatch_mask = recognized_mask & df[target_label_col].notna() & (df[target_label_col] != derived_label)\n",
       "    mismatches = int(mismatch_mask.sum())\n",
       "\n",
       "    # For rows where label is NaN but derived_label is known, fill from derived_label\n",
       "    fill_from_cat_mask = recognized_mask & df[target_label_col].isna()\n",
       "    fills = int(fill_from_cat_mask.sum())\n",
       "\n",
       "    # Apply fixes\n",
       "    df.loc[mismatch_mask, target_label_col] = derived_label[mismatch_mask]\n",
       "    df.loc[fill_from_cat_mask, target_label_col] = derived_label[fill_from_cat_mask]\n",
       "\n",
       "    print(f\"Normalized recognized attack_cat values: {int(recognized_mask.sum())} rows.\")\n",
       "    print(f\"Repaired label to match attack_cat in {mismatches} conflicting rows.\")\n",
       "    print(f\"Filled missing labels from attack_cat in {fills} rows.\")\n",
       "\n",
       "# Ensure 'label' is strictly 0/1; if non-binary values remain, attempt to coerce or drop\n",
       "if not is_binary_series(df[target_label_col].astype(\"float\").fillna(-1)):\n",
       "    # Try a secondary coercion: any positive -> 1, zero -> 0\n",
       "    print(\"Warning: Non-binary values detected in 'label'. Coercing positive->1, zero/negative->0.\")\n",
       "    df[target_label_col] = (pd.to_numeric(df[target_label_col], errors=\"coerce\").fillna(0) > 0).astype(\"Int64\")\n",
       "\n",
       "# Drop rows where label is still NaN or not binary after coercion\n",
       "valid_label_mask = df[target_label_col].isin([0, 1])\n",
       "dropped_invalid_label = int((~valid_label_mask).sum())\n",
       "df = df.loc[valid_label_mask].reset_index(drop=True)\n",
       "if dropped_invalid_label > 0:\n",
       "    print(f\"Dropped {dropped_invalid_label} rows with invalid 'label'. New shape: {df.shape}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 5) Domain validity checks and hard constraints\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"5) Domain validity checks and hard constraints\")\n",
       "\n",
       "# Define domain rules\n",
       "non_negative_cols = [\n",
       "    \"dur\",\"sbytes\",\"dbytes\",\"spkts\",\"dpkts\",\n",
       "    \"sload\",\"dload\",\"rate\",\n",
       "    \"sinpkt\",\"dinpkt\",\"sjit\",\"djit\",\n",
       "    \"synack\",\"ackdat\",\"tcprtt\",\n",
       "    \"response_body_len\",\n",
       "    \"smean\",\"dmean\",\"smeansz\",\"dmeansz\",\n",
       "    \"stcpb\",\"dtcpb\",\n",
       "    \"trans_depth\",\n",
       "    \"sttl\",\"dttl\",\"swin\",\"dwin\",\n",
       "]\n",
       "\n",
       "# TTL plausible bounds\n",
       "ttl_cols = [\"sttl\", \"dttl\"]\n",
       "ttl_min, ttl_max = 0, 255\n",
       "\n",
       "# Known small-domain flag/category columns (will be one-hot encoded later)\n",
       "small_cat_cols = [\"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_sm_ips_ports\", \"is_ftp_login\", \"ct_ftp_cmd\"]\n",
       "\n",
       "# Enforce numeric dtypes for numeric domain columns when present\n",
       "present_nonneg_cols = safe_intersect(non_negative_cols, df.columns)\n",
       "for col in present_nonneg_cols:\n",
       "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
       "\n",
       "# Non-negativity hard filter\n",
       "neg_mask_any = pd.Series(False, index=df.index)\n",
       "for col in present_nonneg_cols:\n",
       "    neg_mask = df[col] < 0\n",
       "    neg_mask_any = neg_mask_any | (neg_mask.fillna(False))\n",
       "\n",
       "neg_count = int(neg_mask_any.sum())\n",
       "if neg_count > 0:\n",
       "    print(f\"Filtering out {neg_count} rows with negative values in non-negative constrained columns.\")\n",
       "    df = df.loc[~neg_mask_any].reset_index(drop=True)\n",
       "\n",
       "# TTL hard range check [0, 255]\n",
       "present_ttl_cols = safe_intersect(ttl_cols, df.columns)\n",
       "if present_ttl_cols:\n",
       "    out_of_range_mask_any = pd.Series(False, index=df.index)\n",
       "    for col in present_ttl_cols:\n",
       "        out_of_range_mask = ~df[col].between(ttl_min, ttl_max)\n",
       "        out_of_range_mask_any = out_of_range_mask_any | (out_of_range_mask.fillna(False))\n",
       "    out_range_count = int(out_of_range_mask_any.sum())\n",
       "    if out_range_count > 0:\n",
       "        print(f\"Filtering out {out_range_count} rows with TTL out of [{ttl_min},{ttl_max}].\")\n",
       "        df = df.loc[~out_of_range_mask_any].reset_index(drop=True)\n",
       "\n",
       "# Binary domain for is_sm_ips_ports if present (should be {0,1})\n",
       "if \"is_sm_ips_ports\" in df.columns:\n",
       "    df[\"is_sm_ips_ports\"] = pd.to_numeric(df[\"is_sm_ips_ports\"], errors=\"coerce\").astype(\"Int64\")\n",
       "    valid_binary = df[\"is_sm_ips_ports\"].isin([0, 1])\n",
       "    invalid_binary_count = int((~valid_binary).sum())\n",
       "    if invalid_binary_count > 0:\n",
       "        print(f\"Dropping {invalid_binary_count} rows with invalid 'is_sm_ips_ports' values (expect 0/1).\")\n",
       "        df = df.loc[valid_binary].reset_index(drop=True)\n",
       "\n",
       "# Ensure we removed any NaNs introduced so far in strict columns:\n",
       "strict_cols = present_nonneg_cols + present_ttl_cols + ([\"is_sm_ips_ports\"] if \"is_sm_ips_ports\" in df.columns else [])\n",
       "if strict_cols:\n",
       "    nan_mask_any = df[strict_cols].isna().any(axis=1)\n",
       "    nan_count = int(nan_mask_any.sum())\n",
       "    if nan_count > 0:\n",
       "        print(f\"Dropping {nan_count} rows with NaN in strict domain columns after enforcement.\")\n",
       "        df = df.loc[~nan_mask_any].reset_index(drop=True)\n",
       "\n",
       "print(f\"Post domain checks shape: {df.shape}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 6) Continuous feature skew mitigation (log1p)\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"6) Continuous feature skew mitigation (log1p)\")\n",
       "\n",
       "# Columns suited for log1p (only if present and non-negative by design)\n",
       "log1p_candidates = [\n",
       "    \"dur\",\"rate\",\"sload\",\"dload\",\n",
       "    \"sbytes\",\"dbytes\",\"spkts\",\"dpkts\",\n",
       "    \"sinpkt\",\"dinpkt\",\"sjit\",\"djit\",\n",
       "    \"synack\",\"ackdat\",\"tcprtt\",\n",
       "    \"response_body_len\",\n",
       "    \"smean\",\"dmean\",\"smeansz\",\"dmeansz\",\n",
       "]\n",
       "\n",
       "log1p_cols = safe_intersect(log1p_candidates, df.columns)\n",
       "\n",
       "# Ensure these columns are non-negative (should be after Step 5)\n",
       "# Apply log1p transformation in-place\n",
       "for col in log1p_cols:\n",
       "    # If any residual negatives (shouldn't happen), shift to 0 as a last resort\n",
       "    min_val = df[col].min()\n",
       "    if pd.notna(min_val) and min_val < 0:\n",
       "        shift = abs(min_val)\n",
       "        print(f\"Warning: {col} has negative values after domain checks. Shifting by {shift} before log1p.\")\n",
       "        df[col] = df[col] + shift\n",
       "    df[col] = np.log1p(df[col].astype(float))\n",
       "\n",
       "print(f\"Applied log1p to {len(log1p_cols)} columns: {log1p_cols}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 7) Extreme outlier capping (winsorization) on log-transformed features\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"7) Extreme outlier capping (winsorization)\")\n",
       "winsorize_candidates = [\n",
       "    \"rate\",\"sload\",\"dload\",\"sinpkt\",\"dinpkt\",\"sjit\",\"djit\",\n",
       "    \"synack\",\"ackdat\",\"tcprtt\",\"dur\",\"sbytes\",\"dbytes\"\n",
       "]\n",
       "winsor_cols = safe_intersect(winsorize_candidates, df.columns)\n",
       "\n",
       "for col in winsor_cols:\n",
       "    lo = df[col].quantile(LOW_Q)\n",
       "    hi = df[col].quantile(HIGH_Q)\n",
       "    if pd.isna(lo) or pd.isna(hi):\n",
       "        continue\n",
       "    if lo > hi:\n",
       "        lo, hi = hi, lo\n",
       "    df[col] = df[col].clip(lower=lo, upper=hi)\n",
       "\n",
       "print(f\"Winsorized {len(winsor_cols)} columns at [{LOW_Q*100:.1f}%, {HIGH_Q*100:.1f}%] quantiles.\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 10) Zero-inflation indicators for structural zeros (do BEFORE feature dropping)\n",
       "# Note: We place step 10 before step 8 to preserve zero-pattern info for dropped features.\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"10) Zero-inflation indicators for structural zeros\")\n",
       "\n",
       "zero_indicator_candidates = [\n",
       "    \"dur\",\"sinpkt\",\"dinpkt\",\"sjit\",\"djit\",\"synack\",\"ackdat\",\"tcprtt\"\n",
       "    # We omit rate/sload/dload indicators because they will be dropped for multicollinearity\n",
       "]\n",
       "present_zero_cols = safe_intersect(zero_indicator_candidates, df.columns)\n",
       "\n",
       "zero_ind_cols = []\n",
       "for col in present_zero_cols:\n",
       "    ind_col = f\"is_zero__{col}\"\n",
       "    # Use a tight equality check; post-log1p zero implies original was zero\n",
       "    # If column was log-transformed, values equal to 0 imply original zero.\n",
       "    zero_ind = (df[col] == 0).astype(\"int8\")\n",
       "    df[ind_col] = zero_ind\n",
       "    zero_ind_cols.append(ind_col)\n",
       "\n",
       "print(f\"Added {len(zero_ind_cols)} zero-indicator columns: {zero_ind_cols}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 8) Multicollinearity control among derived features\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"8) Multicollinearity control among derived features\")\n",
       "\n",
       "drop_for_collinearity = [\"tcprtt\", \"rate\", \"sload\", \"dload\", \"stcpb\", \"dtcpb\"]\n",
       "drop_existing = safe_intersect(drop_for_collinearity, df.columns)\n",
       "df.drop(columns=drop_existing, inplace=True, errors=\"ignore\")\n",
       "print(f\"Dropped for multicollinearity (present): {drop_existing}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 9) Categorical encoding for protocol/state flags (+ step 12 rare cat handling inside)\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"9) Categorical encoding for selected small-domain features (+ rare handling)\")\n",
       "\n",
       "ohe_base_cols = [\"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_sm_ips_ports\", \"is_ftp_login\", \"ct_ftp_cmd\"]\n",
       "ohe_cols_present = safe_intersect(ohe_base_cols, df.columns)\n",
       "\n",
       "# We'll one-hot encode these columns if present (even if numeric codes), with rare categories pooled into \"Other\"\n",
       "created_dummies = []\n",
       "for col in ohe_cols_present:\n",
       "    # Treat as strings for robust one-hot encoding; fill missing as \"Unknown\"\n",
       "    col_as_str = df[col].astype(\"Int64\") if pd.api.types.is_numeric_dtype(df[col]) else df[col].astype(\"string\")\n",
       "    col_as_str = col_as_str.astype(\"string\").fillna(\"Unknown\")\n",
       "\n",
       "    # Rare category pooling\n",
       "    vc = col_as_str.value_counts(dropna=False)\n",
       "    threshold = max(2, int(math.floor(RARE_CAT_THRESHOLD * len(df))))  # at least 2 rows to keep as unique category\n",
       "    rare_cats = set(vc[vc < threshold].index)\n",
       "    pooled = col_as_str.where(~col_as_str.isin(rare_cats), other=\"Other\")\n",
       "\n",
       "    # Create dummies with consistent naming\n",
       "    dummies = pd.get_dummies(pooled, prefix=col, prefix_sep=\"=\", dtype=np.uint8)\n",
       "    df = pd.concat([df.drop(columns=[col]), dummies], axis=1)\n",
       "    created_dummies.extend(list(dummies.columns))\n",
       "\n",
       "print(f\"One-hot encoded columns: {ohe_cols_present}\")\n",
       "print(f\"Created {len(created_dummies)} dummy columns.\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 11) Scaling of continuous features (RobustScaler)\n",
       "# - Scale only continuous numeric features (exclude targets, OHE binaries, and zero-indicator binaries)\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"11) Scaling of continuous features (RobustScaler)\")\n",
       "\n",
       "# Identify numeric columns post-encoding\n",
       "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
       "\n",
       "# Exclusions from scaling:\n",
       "exclude_from_scaling = set()\n",
       "# Targets\n",
       "exclude_from_scaling.add(target_label_col)\n",
       "# Zero indicators\n",
       "exclude_from_scaling.update(zero_ind_cols)\n",
       "# OHE dummy columns (uint8 dummies created in step 9)\n",
       "exclude_from_scaling.update(created_dummies)\n",
       "\n",
       "# Final list of continuous columns to scale\n",
       "scale_cols = [c for c in numeric_cols if c not in exclude_from_scaling]\n",
       "\n",
       "if scale_cols:\n",
       "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n",
       "    df[scale_cols] = scaler.fit_transform(df[scale_cols].astype(float))\n",
       "    print(f\"Scaled {len(scale_cols)} continuous features with RobustScaler.\")\n",
       "else:\n",
       "    print(\"No continuous columns identified for scaling.\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 12) Rare category handling (handled in step 9 via pooling before encoding)\n",
       "# -------------------------------------------------------------------\n",
       "print_step_header(\"12) Rare category handling\")\n",
       "print(\"Rare category pooling performed during step 9 before one-hot encoding.\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 13) Class distribution profiling and artifact check (no modeling)\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"13) Class distribution profiling and artifact check\")\n",
       "\n",
       "# Label distribution\n",
       "label_counts = df[target_label_col].value_counts(dropna=False)\n",
       "print(\"Label distribution:\")\n",
       "print(label_counts.to_string())\n",
       "\n",
       "# Multiclass distribution if available\n",
       "if target_multiclass_col in df.columns:\n",
       "    print(\"\\nattack_cat distribution (top 20):\")\n",
       "    print(df[target_multiclass_col].value_counts(dropna=False).head(20).to_string())\n",
       "\n",
       "# Quick leakage-like check: high correlation with label for numeric features\n",
       "print(\"\\nTop 10 absolute correlations with label (numeric features):\")\n",
       "num_for_corr = [c for c in numeric_cols if c != target_label_col and c in df.columns]\n",
       "if num_for_corr:\n",
       "    corr = df[num_for_corr].corrwith(df[target_label_col].astype(float)).abs().sort_values(ascending=False)\n",
       "    print(corr.head(10).to_string())\n",
       "else:\n",
       "    print(\"No numeric features available for correlation analysis.\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 14) Final integrity and export of clean feature matrix\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"14) Final integrity and export of clean feature matrix\")\n",
       "\n",
       "# Replace inf values (should be none) and check for NaNs\n",
       "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
       "\n",
       "# Sanity: ensure no NaNs in numeric features created by our pipeline (original dataset has no missing)\n",
       "nan_summary = df.isna().sum()\n",
       "total_nans = int(nan_summary.sum())\n",
       "if total_nans > 0:\n",
       "    # In this pipeline, we prefer to drop rows with NaNs to keep a clean matrix\n",
       "    print(f\"Warning: Found {total_nans} missing values after processing; dropping affected rows.\")\n",
       "    df = df.dropna(axis=0).reset_index(drop=True)\n",
       "\n",
       "print(f\"Final preprocessed shape: {df.shape}\")\n",
       "\n",
       "# Expose the final preprocessed DataFrame for downstream usage\n",
       "df_preprocessed = df.copy()\n",
       "\n",
       "# Attempt to save to disk (optional)\n",
       "try:\n",
       "    out_dir = str(Path(CSV_PATH).parent)\n",
       "    out_path = os.path.join(out_dir, OUTPUT_FILENAME)\n",
       "    df_preprocessed.to_csv(out_path, index=False)\n",
       "    print(f\"Saved preprocessed dataset to: {out_path}\")\n",
       "except Exception as e:\n",
       "    print(f\"Could not save preprocessed CSV due to: {e}\")\n",
       "\n",
       "# For interactive sessions, you can inspect df_preprocessed.head()\n",
       "# print(df_preprocessed.head())\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chain Step 4: Code Validation ---\n",
      "[validation] Success attempt=1 time=111.53s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Data Validation Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Data Cleaning Code - Validated\n",
       "```python\n",
       "\"\"\"\n",
       "Data Cleaning and Preprocessing for UNSW-NB15 (concatenated) dataset.\n",
       "\n",
       "VALIDATION SUMMARY (Task 4):\n",
       "- The original pipeline is largely correct and complete. It implements all plan steps, with an intentional\n",
       "  reordering where step 10 (zero-inflation indicators) is applied before step 8 (multicollinearity drops)\n",
       "  to preserve useful zero-pattern information for features that are subsequently removed.\n",
       "- One targeted change was made for robustness and to avoid unnecessary row loss:\n",
       "  CHANGE: In Final Integrity (Step 14), instead of dropping rows with NaN in any column (including raw\n",
       "          non-modeled string columns), we now only drop rows that have NaNs in numeric columns\n",
       "          (i.e., model-ready features and targets). This aligns with the step’s intent and avoids removing rows\n",
       "          due to missing values in non-numeric, uninterpreted identifiers or text columns.\n",
       "  RATIONALE: The previous \"drop if any NaN in df\" could remove a large number of rows if any unused string\n",
       "             column had NaNs. Our change keeps the feature matrix clean where it matters (numeric inputs\n",
       "             and targets) while preserving rows that are otherwise valid for modeling.\n",
       "\n",
       "All other logic is retained; additional comments were added for clarity.\n",
       "\n",
       "This script implements the cleaning plan described in Task 3:\n",
       "1) Schema lock and dtypes enforcement\n",
       "2) Identifier removal\n",
       "3) Exact duplicate handling\n",
       "4) Label–category consistency audit and repair\n",
       "5) Domain validity checks and hard constraints\n",
       "6) Continuous feature skew mitigation (log1p)\n",
       "7) Extreme outlier capping (winsorization)\n",
       "10) Zero-inflation indicators for structural zeros (applied BEFORE dropping derived features)\n",
       "8) Multicollinearity control among derived features\n",
       "9) Categorical encoding for selected protocol/state flags\n",
       "11) Scaling of continuous features\n",
       "12) Rare category handling (applied within step 9)\n",
       "13) Class distribution profiling and artifact check (non-modeling diagnostics)\n",
       "14) Final integrity checks and export\n",
       "\n",
       "Notes:\n",
       "- This code avoids splitting or modeling; it prepares a single preprocessed DataFrame.\n",
       "- It is defensive to slight schema variations by checking column existence before operations.\n",
       "- All steps print concise diagnostics to assist auditability without interrupting execution.\n",
       "\"\"\"\n",
       "\n",
       "import os\n",
       "import sys\n",
       "import math\n",
       "import warnings\n",
       "from pathlib import Path\n",
       "\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "from sklearn.preprocessing import RobustScaler\n",
       "\n",
       "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# Configuration\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "# Input CSV path (must exist)\n",
       "CSV_PATH = r\"E:\\Datasets\\UNSW-NB15\\Training and Testing Sets\\UNSW_NB15_concatenated_dropped.csv\"\n",
       "\n",
       "# Output (optional) - will attempt to write in the same directory\n",
       "OUTPUT_FILENAME = \"UNSW_NB15_preprocessed.csv\"\n",
       "\n",
       "# Robust winsorization quantiles (after log1p transformation)\n",
       "LOW_Q = 0.005\n",
       "HIGH_Q = 0.995\n",
       "\n",
       "# Rare category threshold (proportion of dataset); categories under this will be pooled into 'Other'\n",
       "RARE_CAT_THRESHOLD = 0.001  # 0.1%\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# Utilities\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "def normalize_attack_cat(val):\n",
       "    \"\"\"\n",
       "    Normalize attack category string to canonical form if recognized.\n",
       "    Returns (canonical_string, recognized_boolean)\n",
       "\n",
       "    - Handles common UNSW-NB15 attack categories and \"Normal\"\n",
       "    - Uses a lowercase map but returns canonical title-case labels.\n",
       "    \"\"\"\n",
       "    if pd.isna(val):\n",
       "        return val, False\n",
       "    s = str(val).strip().lower()\n",
       "    mapping = {\n",
       "        \"normal\": \"Normal\",\n",
       "        \"fuzzers\": \"Fuzzers\",\n",
       "        \"analysis\": \"Analysis\",\n",
       "        \"backdoor\": \"Backdoor\",\n",
       "        \"backdoors\": \"Backdoor\",  # sometimes plural\n",
       "        \"dos\": \"DoS\",\n",
       "        \"exploits\": \"Exploits\",\n",
       "        \"generic\": \"Generic\",\n",
       "        \"reconnaissance\": \"Reconnaissance\",\n",
       "        \"shellcode\": \"Shellcode\",\n",
       "        \"worms\": \"Worms\",\n",
       "    }\n",
       "    if s in mapping:\n",
       "        return mapping[s], True\n",
       "    return val, False  # return original if not recognized\n",
       "\n",
       "\n",
       "def print_step_header(step_text):\n",
       "    \"\"\"Helper to print readable step headers in the console for audit trail.\"\"\"\n",
       "    print(\"\\n\" + \"-\" * 80)\n",
       "    print(step_text)\n",
       "    print(\"-\" * 80)\n",
       "\n",
       "\n",
       "def safe_intersect(cols, df_columns):\n",
       "    \"\"\"Return the intersection of a candidate list with an existing DataFrame columns.\"\"\"\n",
       "    return [c for c in cols if c in df_columns]\n",
       "\n",
       "\n",
       "def is_binary_series(s):\n",
       "    \"\"\"Check if a pandas Series contains only {0,1} (ignoring NaNs).\"\"\"\n",
       "    vals = pd.unique(s.dropna())\n",
       "    return set(vals).issubset({0, 1})\n",
       "\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# Load data\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"Loading data\")\n",
       "if not os.path.exists(CSV_PATH):\n",
       "    raise FileNotFoundError(f\"Input CSV not found at path: {CSV_PATH}\")\n",
       "\n",
       "# low_memory=False to preserve column consistency and avoid mixed dtypes\n",
       "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
       "print(f\"Loaded shape: {df.shape}\")\n",
       "print(f\"Columns: {list(df.columns)}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 1) Schema lock and dtypes enforcement\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"1) Schema lock and dtypes enforcement\")\n",
       "\n",
       "# Targets: binary 'label' is required; multiclass 'attack_cat' is optional.\n",
       "target_label_col = \"label\"\n",
       "target_multiclass_col = \"attack_cat\"\n",
       "\n",
       "# Ensure 'label' is present; if not, raise error because it is a key target.\n",
       "if target_label_col not in df.columns:\n",
       "    raise KeyError(\"Expected binary target column 'label' not found.\")\n",
       "\n",
       "# Enforce 'label' as pandas nullable integer Int64 (allows NA during coercion) then validate later.\n",
       "df[target_label_col] = pd.to_numeric(df[target_label_col], errors=\"coerce\").astype(\"Int64\")\n",
       "\n",
       "# Ensure 'attack_cat' exists; if so, cast to string for normalization.\n",
       "if target_multiclass_col in df.columns:\n",
       "    df[target_multiclass_col] = df[target_multiclass_col].astype(\"string\")\n",
       "else:\n",
       "    print(\"Warning: 'attack_cat' column not found; multiclass target-related checks will be skipped.\")\n",
       "\n",
       "# Convert object-typed non-target columns to numeric if the vast majority are numeric-representable.\n",
       "# This prevents string categorical columns from being damaged while ensuring numeric strings become numerics.\n",
       "non_target_cols = [c for c in df.columns if c not in [target_label_col, target_multiclass_col]]\n",
       "for col in non_target_cols:\n",
       "    if df[col].dtype == object:\n",
       "        converted = pd.to_numeric(df[col], errors=\"coerce\")\n",
       "        # Convert only if >90% of values can be parsed as numbers (heuristic to preserve true categoricals).\n",
       "        if converted.notna().mean() > 0.9:\n",
       "            df[col] = converted\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 2) Identifier removal\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"2) Identifier removal\")\n",
       "# Drop only clear row-unique identifiers; keep potential features like IPs unless explicitly excluded.\n",
       "drop_id_cols = [c for c in [\"id\"] if c in df.columns]\n",
       "if drop_id_cols:\n",
       "    df.drop(columns=drop_id_cols, inplace=True)\n",
       "    print(f\"Dropped identifier columns: {drop_id_cols}\")\n",
       "else:\n",
       "    print(\"No identifier columns found to drop.\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 3) Exact duplicate handling\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"3) Exact duplicate handling\")\n",
       "before = len(df)\n",
       "df = df.drop_duplicates(ignore_index=True)\n",
       "after = len(df)\n",
       "print(f\"Removed {before - after} exact duplicate rows. New shape: {df.shape}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 4) Label–category consistency audit and repair\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"4) Label–category consistency audit and repair\")\n",
       "if target_multiclass_col in df.columns:\n",
       "    # Normalize attack_cat values to canonical names if recognized by our map\n",
       "    normalized_vals = df[target_multiclass_col].apply(normalize_attack_cat)\n",
       "    df[target_multiclass_col] = normalized_vals.apply(lambda x: x[0])\n",
       "    recognized_mask = normalized_vals.apply(lambda x: x[1])\n",
       "\n",
       "    # Derive the expected binary label from normalized attack_cat where recognized (Normal -> 0; else -> 1).\n",
       "    derived_label = pd.Series(index=df.index, dtype=\"Int64\")\n",
       "    recognized_cats = df.loc[recognized_mask, target_multiclass_col]\n",
       "    derived_label.loc[recognized_mask] = (recognized_cats != \"Normal\").astype(\"Int64\")\n",
       "\n",
       "    # Identify contradictions only on rows where both a label and a recognized cat exist.\n",
       "    mismatch_mask = recognized_mask & df[target_label_col].notna() & (df[target_label_col] != derived_label)\n",
       "    mismatches = int(mismatch_mask.sum())\n",
       "\n",
       "    # Fill missing labels directly from recognized attack_cat-derived labels.\n",
       "    fill_from_cat_mask = recognized_mask & df[target_label_col].isna()\n",
       "    fills = int(fill_from_cat_mask.sum())\n",
       "\n",
       "    # Apply fixes (mend contradictions and fill missing labels).\n",
       "    df.loc[mismatch_mask, target_label_col] = derived_label[mismatch_mask]\n",
       "    df.loc[fill_from_cat_mask, target_label_col] = derived_label[fill_from_cat_mask]\n",
       "\n",
       "    print(f\"Normalized recognized attack_cat values: {int(recognized_mask.sum())} rows.\")\n",
       "    print(f\"Repaired label to match attack_cat in {mismatches} conflicting rows.\")\n",
       "    print(f\"Filled missing labels from attack_cat in {fills} rows.\")\n",
       "\n",
       "# Ensure 'label' is strictly binary {0,1}; if non-binary values remain, coerce and enforce.\n",
       "if not is_binary_series(df[target_label_col].astype(\"float\").fillna(-1)):\n",
       "    print(\"Warning: Non-binary values detected in 'label'. Coercing positive->1, zero/negative->0.\")\n",
       "    df[target_label_col] = (pd.to_numeric(df[target_label_col], errors=\"coerce\").fillna(0) > 0).astype(\"Int64\")\n",
       "\n",
       "# Drop rows where label is still NaN or not in {0,1} after coercion to guarantee clean target.\n",
       "valid_label_mask = df[target_label_col].isin([0, 1])\n",
       "dropped_invalid_label = int((~valid_label_mask).sum())\n",
       "df = df.loc[valid_label_mask].reset_index(drop=True)\n",
       "if dropped_invalid_label > 0:\n",
       "    print(f\"Dropped {dropped_invalid_label} rows with invalid 'label'. New shape: {df.shape}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 5) Domain validity checks and hard constraints\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"5) Domain validity checks and hard constraints\")\n",
       "\n",
       "# Define domain rules for numeric columns with known physical constraints (non-negative, bounded)\n",
       "non_negative_cols = [\n",
       "    \"dur\",\"sbytes\",\"dbytes\",\"spkts\",\"dpkts\",\n",
       "    \"sload\",\"dload\",\"rate\",\n",
       "    \"sinpkt\",\"dinpkt\",\"sjit\",\"djit\",\n",
       "    \"synack\",\"ackdat\",\"tcprtt\",\n",
       "    \"response_body_len\",\n",
       "    \"smean\",\"dmean\",\"smeansz\",\"dmeansz\",\n",
       "    \"stcpb\",\"dtcpb\",\n",
       "    \"trans_depth\",\n",
       "    \"sttl\",\"dttl\",\"swin\",\"dwin\",\n",
       "]\n",
       "\n",
       "# TTL plausible bounds\n",
       "ttl_cols = [\"sttl\", \"dttl\"]\n",
       "ttl_min, ttl_max = 0, 255\n",
       "\n",
       "# Known small-domain flag/category columns (will be one-hot encoded later)\n",
       "small_cat_cols = [\"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_sm_ips_ports\", \"is_ftp_login\", \"ct_ftp_cmd\"]\n",
       "\n",
       "# Enforce numeric dtypes for numeric domain columns when present\n",
       "present_nonneg_cols = safe_intersect(non_negative_cols, df.columns)\n",
       "for col in present_nonneg_cols:\n",
       "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
       "\n",
       "# Non-negativity hard filter: drop any row with a negative value across constrained columns\n",
       "neg_mask_any = pd.Series(False, index=df.index)\n",
       "for col in present_nonneg_cols:\n",
       "    neg_mask = df[col] < 0\n",
       "    neg_mask_any = neg_mask_any | (neg_mask.fillna(False))\n",
       "\n",
       "neg_count = int(neg_mask_any.sum())\n",
       "if neg_count > 0:\n",
       "    print(f\"Filtering out {neg_count} rows with negative values in non-negative constrained columns.\")\n",
       "    df = df.loc[~neg_mask_any].reset_index(drop=True)\n",
       "\n",
       "# TTL hard range check [0, 255]; drop rows outside bounds\n",
       "present_ttl_cols = safe_intersect(ttl_cols, df.columns)\n",
       "if present_ttl_cols:\n",
       "    out_of_range_mask_any = pd.Series(False, index=df.index)\n",
       "    for col in present_ttl_cols:\n",
       "        out_of_range_mask = ~df[col].between(ttl_min, ttl_max)\n",
       "        out_of_range_mask_any = out_of_range_mask_any | (out_of_range_mask.fillna(False))\n",
       "    out_range_count = int(out_of_range_mask_any.sum())\n",
       "    if out_range_count > 0:\n",
       "        print(f\"Filtering out {out_range_count} rows with TTL out of [{ttl_min},{ttl_max}].\")\n",
       "        df = df.loc[~out_of_range_mask_any].reset_index(drop=True)\n",
       "\n",
       "# Binary domain enforcement for is_sm_ips_ports if present (must be {0,1}); drop invalid.\n",
       "if \"is_sm_ips_ports\" in df.columns:\n",
       "    df[\"is_sm_ips_ports\"] = pd.to_numeric(df[\"is_sm_ips_ports\"], errors=\"coerce\").astype(\"Int64\")\n",
       "    valid_binary = df[\"is_sm_ips_ports\"].isin([0, 1])\n",
       "    invalid_binary_count = int((~valid_binary).sum())\n",
       "    if invalid_binary_count > 0:\n",
       "        print(f\"Dropping {invalid_binary_count} rows with invalid 'is_sm_ips_ports' values (expect 0/1).\")\n",
       "        df = df.loc[valid_binary].reset_index(drop=True)\n",
       "\n",
       "# Ensure we removed any NaNs introduced so far in strict columns (domain-constrained numeric fields)\n",
       "strict_cols = present_nonneg_cols + present_ttl_cols + ([\"is_sm_ips_ports\"] if \"is_sm_ips_ports\" in df.columns else [])\n",
       "if strict_cols:\n",
       "    nan_mask_any = df[strict_cols].isna().any(axis=1)\n",
       "    nan_count = int(nan_mask_any.sum())\n",
       "    if nan_count > 0:\n",
       "        print(f\"Dropping {nan_count} rows with NaN in strict domain columns after enforcement.\")\n",
       "        df = df.loc[~nan_mask_any].reset_index(drop=True)\n",
       "\n",
       "print(f\"Post domain checks shape: {df.shape}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 6) Continuous feature skew mitigation (log1p)\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"6) Continuous feature skew mitigation (log1p)\")\n",
       "\n",
       "# Columns suited for log1p (only if present and non-negative by design)\n",
       "# These are often long-tailed in network traffic; log1p compresses scale and preserves zeros.\n",
       "log1p_candidates = [\n",
       "    \"dur\",\"rate\",\"sload\",\"dload\",\n",
       "    \"sbytes\",\"dbytes\",\"spkts\",\"dpkts\",\n",
       "    \"sinpkt\",\"dinpkt\",\"sjit\",\"djit\",\n",
       "    \"synack\",\"ackdat\",\"tcprtt\",\n",
       "    \"response_body_len\",\n",
       "    \"smean\",\"dmean\",\"smeansz\",\"dmeansz\",\n",
       "]\n",
       "\n",
       "log1p_cols = safe_intersect(log1p_candidates, df.columns)\n",
       "\n",
       "# Apply safe log1p transformation in-place (shifts up if any residual negatives appear unexpectedly)\n",
       "for col in log1p_cols:\n",
       "    min_val = df[col].min()\n",
       "    if pd.notna(min_val) and min_val < 0:\n",
       "        shift = abs(min_val)\n",
       "        print(f\"Warning: {col} has negative values after domain checks. Shifting by {shift} before log1p.\")\n",
       "        df[col] = df[col] + shift\n",
       "    df[col] = np.log1p(df[col].astype(float))\n",
       "\n",
       "print(f\"Applied log1p to {len(log1p_cols)} columns: {log1p_cols}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 7) Extreme outlier capping (winsorization) on log-transformed features\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"7) Extreme outlier capping (winsorization)\")\n",
       "# Winsorization post-log stabilizes extremes while preserving relative ranks.\n",
       "winsorize_candidates = [\n",
       "    \"rate\",\"sload\",\"dload\",\"sinpkt\",\"dinpkt\",\"sjit\",\"djit\",\n",
       "    \"synack\",\"ackdat\",\"tcprtt\",\"dur\",\"sbytes\",\"dbytes\"\n",
       "]\n",
       "winsor_cols = safe_intersect(winsorize_candidates, df.columns)\n",
       "\n",
       "for col in winsor_cols:\n",
       "    lo = df[col].quantile(LOW_Q)\n",
       "    hi = df[col].quantile(HIGH_Q)\n",
       "    if pd.isna(lo) or pd.isna(hi):\n",
       "        continue\n",
       "    if lo > hi:\n",
       "        lo, hi = hi, lo\n",
       "    df[col] = df[col].clip(lower=lo, upper=hi)\n",
       "\n",
       "print(f\"Winsorized {len(winsor_cols)} columns at [{LOW_Q*100:.1f}%, {HIGH_Q*100:.1f}%] quantiles.\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 10) Zero-inflation indicators for structural zeros (do BEFORE feature dropping)\n",
       "# NOTE (intentional reordering): We place step 10 before step 8 to preserve zero-pattern information for features\n",
       "# that are subsequently dropped for multicollinearity (e.g., tcprtt, rate, loads).\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"10) Zero-inflation indicators for structural zeros\")\n",
       "\n",
       "zero_indicator_candidates = [\n",
       "    \"dur\",\"sinpkt\",\"dinpkt\",\"sjit\",\"djit\",\"synack\",\"ackdat\",\"tcprtt\"\n",
       "    # We omit rate/sload/dload indicators because they will be dropped for multicollinearity\n",
       "]\n",
       "present_zero_cols = safe_intersect(zero_indicator_candidates, df.columns)\n",
       "\n",
       "zero_ind_cols = []\n",
       "for col in present_zero_cols:\n",
       "    ind_col = f\"is_zero__{col}\"\n",
       "    # After log1p, a value equal to 0 implies the original value was exactly 0.\n",
       "    zero_ind = (df[col] == 0).astype(\"int8\")\n",
       "    df[ind_col] = zero_ind\n",
       "    zero_ind_cols.append(ind_col)\n",
       "\n",
       "print(f\"Added {len(zero_ind_cols)} zero-indicator columns: {zero_ind_cols}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 8) Multicollinearity control among derived features\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"8) Multicollinearity control among derived features\")\n",
       "\n",
       "# Drop highly collinear or redundant features (e.g., TCP RTT variants and rates) to reduce leakage risk\n",
       "# and simplify models. We already captured zero-patterns for some of these above.\n",
       "drop_for_collinearity = [\"tcprtt\", \"rate\", \"sload\", \"dload\", \"stcpb\", \"dtcpb\"]\n",
       "drop_existing = safe_intersect(drop_for_collinearity, df.columns)\n",
       "df.drop(columns=drop_existing, inplace=True, errors=\"ignore\")\n",
       "print(f\"Dropped for multicollinearity (present): {drop_existing}\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 9) Categorical encoding for protocol/state flags (+ step 12 rare cat handling inside)\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"9) Categorical encoding for selected small-domain features (+ rare handling)\")\n",
       "\n",
       "# Selected known small-domain categoricals; will be one-hot encoded with rare category pooling.\n",
       "ohe_base_cols = [\"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_sm_ips_ports\", \"is_ftp_login\", \"ct_ftp_cmd\"]\n",
       "ohe_cols_present = safe_intersect(ohe_base_cols, df.columns)\n",
       "\n",
       "created_dummies = []\n",
       "for col in ohe_cols_present:\n",
       "    # Treat values as strings for robust one-hot encoding; ensure missing becomes explicit \"Unknown\"\n",
       "    col_as_str = df[col].astype(\"Int64\") if pd.api.types.is_numeric_dtype(df[col]) else df[col].astype(\"string\")\n",
       "    col_as_str = col_as_str.astype(\"string\").fillna(\"Unknown\")\n",
       "\n",
       "    # Rare category pooling: any category appearing fewer than threshold rows becomes \"Other\"\n",
       "    vc = col_as_str.value_counts(dropna=False)\n",
       "    threshold = max(2, int(math.floor(RARE_CAT_THRESHOLD * len(df))))  # at least 2 rows to keep as unique\n",
       "    rare_cats = set(vc[vc < threshold].index)\n",
       "    pooled = col_as_str.where(~col_as_str.isin(rare_cats), other=\"Other\")\n",
       "\n",
       "    # Create one-hot columns with deterministic naming\n",
       "    dummies = pd.get_dummies(pooled, prefix=col, prefix_sep=\"=\", dtype=np.uint8)\n",
       "    df = pd.concat([df.drop(columns=[col]), dummies], axis=1)\n",
       "    created_dummies.extend(list(dummies.columns))\n",
       "\n",
       "print(f\"One-hot encoded columns: {ohe_cols_present}\")\n",
       "print(f\"Created {len(created_dummies)} dummy columns.\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 11) Scaling of continuous features (RobustScaler)\n",
       "# - Scale only continuous numeric features (exclude targets, OHE binaries, and zero-indicator binaries)\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"11) Scaling of continuous features (RobustScaler)\")\n",
       "\n",
       "# Identify numeric columns post-encoding (includes label, zero indicators, and OHE dummies)\n",
       "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
       "\n",
       "# Build the exclusion set:\n",
       "exclude_from_scaling = set()\n",
       "# Targets\n",
       "exclude_from_scaling.add(target_label_col)\n",
       "# Zero indicators (binary features we don't scale)\n",
       "exclude_from_scaling.update(zero_ind_cols)\n",
       "# OHE dummy columns (uint8 dummies created in step 9)\n",
       "exclude_from_scaling.update(created_dummies)\n",
       "\n",
       "# Final list of continuous columns to scale\n",
       "scale_cols = [c for c in numeric_cols if c not in exclude_from_scaling]\n",
       "\n",
       "if scale_cols:\n",
       "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n",
       "    df[scale_cols] = scaler.fit_transform(df[scale_cols].astype(float))\n",
       "    print(f\"Scaled {len(scale_cols)} continuous features with RobustScaler.\")\n",
       "else:\n",
       "    print(\"No continuous columns identified for scaling.\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 12) Rare category handling (handled in step 9 via pooling before encoding)\n",
       "# -------------------------------------------------------------------\n",
       "print_step_header(\"12) Rare category handling\")\n",
       "print(\"Rare category pooling performed during step 9 before one-hot encoding.\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 13) Class distribution profiling and artifact check (no modeling)\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"13) Class distribution profiling and artifact check\")\n",
       "\n",
       "# Label distribution\n",
       "label_counts = df[target_label_col].value_counts(dropna=False)\n",
       "print(\"Label distribution:\")\n",
       "print(label_counts.to_string())\n",
       "\n",
       "# Multiclass distribution if available\n",
       "if target_multiclass_col in df.columns:\n",
       "    print(\"\\nattack_cat distribution (top 20):\")\n",
       "    print(df[target_multiclass_col].value_counts(dropna=False).head(20).to_string())\n",
       "\n",
       "# Quick leakage-like check: high correlation with label for numeric features\n",
       "print(\"\\nTop 10 absolute correlations with label (numeric features):\")\n",
       "num_for_corr = [c for c in numeric_cols if c != target_label_col and c in df.columns]\n",
       "if num_for_corr:\n",
       "    corr = df[num_for_corr].corrwith(df[target_label_col].astype(float)).abs().sort_values(ascending=False)\n",
       "    print(corr.head(10).to_string())\n",
       "else:\n",
       "    print(\"No numeric features available for correlation analysis.\")\n",
       "\n",
       "# -------------------------------------------------------------------\n",
       "# 14) Final integrity and export of clean feature matrix\n",
       "# -------------------------------------------------------------------\n",
       "\n",
       "print_step_header(\"14) Final integrity and export of clean feature matrix\")\n",
       "\n",
       "# Replace inf values (should be none) as a safeguard, then check for NaNs\n",
       "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
       "\n",
       "# VALIDATED CHANGE:\n",
       "# Previously, code dropped rows if ANY column had NaN (including unused string columns).\n",
       "# We now enforce NaN-free condition only across numeric columns (model features and targets),\n",
       "# which aligns with the step's intent and avoids unnecessary row loss.\n",
       "numeric_cols_final = df.select_dtypes(include=[np.number]).columns.tolist()\n",
       "nan_mask_numeric = df[numeric_cols_final].isna().any(axis=1)\n",
       "nan_rows_numeric = int(nan_mask_numeric.sum())\n",
       "if nan_rows_numeric > 0:\n",
       "    print(f\"Warning: Found {nan_rows_numeric} rows with NaNs in numeric columns after processing; dropping those rows.\")\n",
       "    df = df.loc[~nan_mask_numeric].reset_index(drop=True)\n",
       "\n",
       "print(f\"Final preprocessed shape: {df.shape}\")\n",
       "\n",
       "# Expose the final preprocessed DataFrame for downstream usage (single fully-prepared matrix)\n",
       "df_preprocessed = df.copy()\n",
       "\n",
       "# Attempt to save to disk (optional)\n",
       "try:\n",
       "    out_dir = str(Path(CSV_PATH).parent)\n",
       "    out_path = os.path.join(out_dir, OUTPUT_FILENAME)\n",
       "    df_preprocessed.to_csv(out_path, index=False)\n",
       "    print(f\"Saved preprocessed dataset to: {out_path}\")\n",
       "except Exception as e:\n",
       "    print(f\"Could not save preprocessed CSV due to: {e}\")\n",
       "\n",
       "# For interactive sessions, you can inspect\n",
       "# print(df_preprocessed.head())\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prompt Chain Complete ---\n",
      "Exported chain artifacts to: C:\\Users\\macdo\\Github\\VNFCDR-1\\Chris Code\\Generative AI Data Cleaning\\Machine Learning\\Step 4_GPT Prompting\\UNSW_NB15_Train_Test_Concatenated\\GPT_API_Exploratory_Analysis_Export.md\n",
      "\n",
      "=== Diagnostics Summary ===\n",
      "Model used: gpt-5\n",
      "Artifacts generated: ['analysis_md', 'plan_md', 'code_md']\n"
     ]
    }
   ],
   "source": [
    "# --- GPT Prompt Chain for Cybersecurity EDA (OpenAI GPT-5 Reasoning) ---\n",
    "\n",
    "import os, io, time, traceback\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "from typing import Dict, List\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. Environment / API Setup --------------------------------------------------\n",
    "# Expect an env file containing OPENAI_API_KEY\n",
    "#ENV_PATH = r\"/Users/sarahsetiawan/Desktop/VNFCDR-1/SarahCode/Generative_AI/OPENAI_API_KEY.env\"  # <-- update if needed\n",
    "ENV_PATH = r\"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 4_GPT Prompting\\\\OPENAI_API_KEY.env\"\n",
    "load_dotenv(ENV_PATH)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not found. Ensure the env file exists and key is set.\")\n",
    "\n",
    "# Instantiate OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "print(\"OpenAI API client configured successfully.\")\n",
    "\n",
    "# 2. Data Load ----------------------------------------------------------------\n",
    "#DATA_PATH = r\"/Users/sarahsetiawan/Desktop/VNFCDR-1/SarahCode/Sample_df/Code/MachineLearning/SampleData_API/CSVs/Representative_APISample_20000_2.csv\"\n",
    "DATA_PATH = r\"E:\\\\Datasets\\\\UNSW-NB15\\\\Training and Testing Sets\\\\UNSW_NB15_concatenated_dropped.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}\")\n",
    "print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# 3. Context Preparation (Token-Optimized) ------------------------------------\n",
    "\n",
    "def build_schema_summary(dataframe: pd.DataFrame) -> str:\n",
    "    info_buf = io.StringIO()\n",
    "    dataframe.info(buf=info_buf)\n",
    "    _ = info_buf.getvalue()  # not used directly; we generate concise version below\n",
    "    rows = []\n",
    "    total = len(dataframe)\n",
    "    for col in dataframe.columns:\n",
    "        non_null = dataframe[col].notna().sum()\n",
    "        null_pct = 100 * (1 - non_null / total)\n",
    "        uniq = dataframe[col].nunique(dropna=True)\n",
    "        rows.append(f\"{col} | {dataframe[col].dtype} | {non_null} | {null_pct:.2f}% | {uniq}\")\n",
    "    header = \"Column | DType | Non-Null | Null% | Unique\\n------ | ----- | -------- | ----- | ------\"\n",
    "    concise = header + \"\\n\" + \"\\n\".join(rows)\n",
    "    return concise\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_ROWS = 10\n",
    "schema_concise = build_schema_summary(df)\n",
    "sample_md = df.sample(n=SAMPLE_ROWS, random_state=RANDOM_SEED).to_markdown(index=False)\n",
    "\n",
    "# 4. System Instruction (kept same semantic intent) ---------------------------\n",
    "system_instruction = \"\"\"\n",
    "You are an expert data scientist specializing in data cleaning and preparation for machine learning. \n",
    "Your task is to perform exploratory data analysis (EDA), data cleaning and preprocessing for machine learning application. \n",
    "You will do this by generating structured insights, plans, and code.\n",
    "\n",
    "Goals:\n",
    "1. Perform a sharp initial EDA on a given dataset.\n",
    "2. Propose an ordered data cleaning and preprocessing plan based on the EDA and best practices.\n",
    "3. Following the data cleaning plan, produce fully executable, well-commented Python code.\n",
    "\n",
    "Constraints:\n",
    "- The created code must be executable without errors using the full dataset loaded from the provided CSV file path.\n",
    "- Use Markdown headings exactly as requested.\n",
    "\"\"\"\n",
    "\n",
    "# 5. Generation Config -------------------------------------------------------\n",
    "MODEL_NAME = \"gpt-5\"  \n",
    "MAX_COMPLETION_TOKENS = 32000  \n",
    "TEMPERATURE = 1  \n",
    "\n",
    "# 6. Retry + Wrapper (Chat Completions) --------------------------------------\n",
    "\n",
    "def call_model(messages: List[Dict[str, str]], max_completion_tokens: int = MAX_COMPLETION_TOKENS, temperature: float = TEMPERATURE):\n",
    "    \"\"\"Wrapper for OpenAI chat completion.\n",
    "\n",
    "    NOTE: Newer reasoning / frontier models use 'max_completion_tokens' instead of deprecated 'max_tokens'.\n",
    "    Some models enforce a fixed temperature (1); earlier attempt with 0.15 caused 400 error.\n",
    "    \"\"\"\n",
    "    return client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_completion_tokens=max_completion_tokens,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "def generate_with_retry(prompt_text: str, label: str, max_retries: int = 3, backoff: float = 2.0):\n",
    "    errors: List[str] = []\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            # Compose messages with system + user\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ]\n",
    "            response = call_model(messages)\n",
    "            elapsed = time.time() - start\n",
    "            text = response.choices[0].message.content.strip() if response.choices else \"\"\n",
    "            if not text:\n",
    "                raise ValueError(\"Empty response content.\")\n",
    "            print(f\"[{label}] Success attempt={attempt} time={elapsed:.2f}s\")\n",
    "            return dict(text=text, raw=response, attempts=attempt, errors=errors)\n",
    "        except Exception as e:\n",
    "            err_msg = f\"[{label}] Attempt {attempt} failed: {e}\"\n",
    "            print(err_msg)\n",
    "            errors.append(err_msg)\n",
    "            if attempt < max_retries:\n",
    "                sleep_time = backoff ** (attempt - 1)\n",
    "                time.sleep(sleep_time)\n",
    "    print(f\"[{label}] All attempts failed.\")\n",
    "    return dict(text=\"\", raw=None, attempts=max_retries, errors=errors)\n",
    "\n",
    "# 7. Prompts (EXACT TEXT PRESERVED from previous implementation) --------------\n",
    "prompt_1_analysis = f\"\"\"\n",
    "## Task 1: Initial Data Analysis\n",
    "\n",
    "You are given:\n",
    "### Concise Schema Summary\n",
    "```\n",
    "{schema_concise}\n",
    "```\n",
    "\n",
    "### Random Sample ({SAMPLE_ROWS} rows)\n",
    "```\n",
    "{sample_md}\n",
    "```\n",
    "\n",
    "### Directives\n",
    "1. Use the schema and sample to perform a sharp exploratory data analysis (EDA).\n",
    "2. Create a table of unique value counts for all columns in the dataset.\n",
    "3. Create a table of high cardinality columns. Ensure to explain why the columns were selected.\n",
    "4. Generate a data quality analysis containing accuracy, completeness, consistency, uniqueness, validity, and timeliness before data cleaning.\n",
    "\n",
    "Output ONLY under heading:\n",
    "## Analytical Insights\n",
    "Use concise Markdown sections & tables. Avoid code.\n",
    "\"\"\"\n",
    "\n",
    "# 8. Execution Chain ---------------------------------------------------------\n",
    "chain_artifacts: Dict[str, str] = {}\n",
    "\n",
    "print(\"--- Chain Step 1: Initial Analysis ---\")\n",
    "analysis_result = generate_with_retry(prompt_1_analysis, label=\"analysis\")\n",
    "\n",
    "if not analysis_result[\"text\"]:\n",
    "    print(\"Aborting chain: analysis stage failed.\")\n",
    "else:\n",
    "    chain_artifacts[\"analysis_md\"] = analysis_result[\"text\"]\n",
    "    display(Markdown(\"### Analysis Received\"))\n",
    "    display(Markdown(chain_artifacts[\"analysis_md\"]))\n",
    "\n",
    "    # Step 2: Plan (prompt uses prior output verbatim)\n",
    "    prompt_2_plan = f\"\"\"\n",
    "## Task 2: Data Cleaning Plan\n",
    "\n",
    "Using the prior analysis:\n",
    "\n",
    "### Analytical Insights\n",
    "{chain_artifacts['analysis_md']}\n",
    "\n",
    "Produce a prioritized, ordered bullet list of data cleaning and machine learning preprocessing steps.\n",
    "Generate a data quality analysis containing accuracy, completeness, consistency, uniqueness, validity, and timeliness before data cleaning.\n",
    "Do not perform train-test splits or modeling.\n",
    "No code. Include: objective, columns impacted, and rationale.\n",
    "Heading required: ## Data Cleaning Steps\n",
    "\"\"\"\n",
    "    print(\"\\n--- Chain Step 2: Cleaning Plan ---\")\n",
    "    plan_result = generate_with_retry(prompt_2_plan, label=\"plan\")\n",
    "\n",
    "    if not plan_result[\"text\"]:\n",
    "        print(\"Aborting chain: plan stage failed.\")\n",
    "    else:\n",
    "        chain_artifacts[\"plan_md\"] = plan_result[\"text\"]\n",
    "        display(Markdown(\"### Cleaning Plan Received\"))\n",
    "        display(Markdown(chain_artifacts[\"plan_md\"]))\n",
    "\n",
    "        # Step 3: Code\n",
    "        prompt_3_code = f\"\"\"\n",
    "## Task 3: Data Cleaning Code\n",
    "\n",
    "### Data Cleaning Steps\n",
    "{chain_artifacts['plan_md']}\n",
    "\n",
    "## Directives\n",
    "1. Create fully executable Python code that implements the data cleaning plan developed in Task 2.\n",
    "2. Your code should prepare one fully preprocessed dataframe. Do NOT perform any splitting or modeling.\n",
    "3. Do not use sample data, only the full dataset loaded from the CSV file at path: {DATA_PATH}\n",
    "4. Provide extremely detailed comments explaining each step of the code.\n",
    "\n",
    "\n",
    "Return as ONE fenced Python code block under heading:\n",
    "## Data Cleaning Code\n",
    "\"\"\"\n",
    "        print(\"\\n--- Chain Step 3: Code Generation ---\")\n",
    "        code_result = generate_with_retry(prompt_3_code, label=\"code\")\n",
    "\n",
    "        if not code_result[\"text\"]:\n",
    "            print(\"Code generation failed.\")\n",
    "        else:\n",
    "            chain_artifacts[\"code_md\"] = code_result[\"text\"]\n",
    "            display(Markdown(\"### Data Cleaning Code Received\"))\n",
    "            display(Markdown(chain_artifacts[\"code_md\"]))\n",
    "\n",
    "        # Step 4: Validation\n",
    "        prompt_4_validation = f\"\"\"\n",
    "## Task 4: Data Cleaning Validation\n",
    "\n",
    "### Data Cleaning Steps\n",
    "{chain_artifacts['code_md']}\n",
    "\n",
    "## Directives\n",
    "1. Provide reasoning for any changes made from the original code.\n",
    "2. Review the provided data cleaning code for correctness and completeness.\n",
    "3. Ensure all steps from the cleaning plan are implemented in order.\n",
    "4. Ensure the code is executable without errors using the full dataset loaded from the provided CSV file.\n",
    "5. Provide extremely detailed comments explaining each step of the code.\n",
    "6. Your code should prepare one fully preprocessed dataframe. Do NOT perform any splitting or modeling.\n",
    "7. If the original code is correct, return it unchanged and note that no changes were needed.\n",
    "\n",
    "Return as ONE fenced Python code block under heading:\n",
    "## Data Cleaning Code - Validated\n",
    "\"\"\"\n",
    "        print(\"\\n--- Chain Step 4: Code Validation ---\")\n",
    "        code_result = generate_with_retry(prompt_4_validation, label=\"validation\")\n",
    "\n",
    "        if not code_result[\"text\"]:\n",
    "            print(\"Validation check failed.\")\n",
    "        else:\n",
    "            chain_artifacts[\"code_md\"] = code_result[\"text\"]\n",
    "            display(Markdown(\"### Data Validation Received\"))\n",
    "            display(Markdown(chain_artifacts[\"code_md\"]))\n",
    "\n",
    "\n",
    "print(\"\\n--- Prompt Chain Complete ---\")\n",
    "\n",
    "# 9. Export Artifacts --------------------------------------------------------\n",
    "EXPORT_DIR = os.path.dirname(DATA_PATH)\n",
    "export_path = \"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 4_GPT Prompting\\\\UNSW_NB15_Train_Test_Concatenated\\\\GPT_API_Exploratory_Analysis_Export.md\"\n",
    "try:\n",
    "    with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        if chain_artifacts:\n",
    "            f.write(\"# EDA Prompt Chain Output (GPT)\\n\\n\")\n",
    "            for k, v in chain_artifacts.items():\n",
    "                pretty = k.replace(\"_md\", \"\").capitalize()\n",
    "                f.write(f\"\\n\\n## {pretty}\\n\\n\")\n",
    "                f.write(v.strip() + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"No artifacts generated (chain failed).\")\n",
    "    print(f\"Exported chain artifacts to: {export_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Export failed: {e}\")\n",
    "\n",
    "# 10. Diagnostic Recap -------------------------------------------------------\n",
    "\n",
    "def print_diagnostics():\n",
    "    print(\"\\n=== Diagnostics Summary ===\")\n",
    "    print(f\"Model used: {MODEL_NAME}\")\n",
    "    if analysis_result.get('errors'):\n",
    "        print(f\"Analysis errors: {len(analysis_result['errors'])}\")\n",
    "    if 'plan_result' in locals() and plan_result.get('errors'):\n",
    "        print(f\"Plan errors: {len(plan_result['errors'])}\")\n",
    "    if 'code_result' in locals() and code_result.get('errors'):\n",
    "        print(f\"Code errors: {len(code_result['errors'])}\")\n",
    "    print(\"Artifacts generated:\", list(chain_artifacts.keys()))\n",
    "\n",
    "print_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4fc883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API client configured successfully.\n",
      "Dataset loaded successfully. Shape: (160474, 56)\n",
      "--- Chain Step 1: Initial Analysis ---\n",
      "[analysis] Success attempt=1 time=93.47s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Analysis Received"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Analytical Insights\n",
       "\n",
       "### Dataset Snapshot\n",
       "- Source: C:\\Users\\macdo\\Github\\VNFCDR-1\\Chris Code\\Generative AI Data Cleaning\\Machine Learning\\Step 4_GPT Prompting\\UNSW_NB15_Train_Test_Concatenated\\cleaned_data.csv\n",
       "- Shape: 160,474 rows × 56 columns\n",
       "- Types: 28 float64, 27 int64, 1 object\n",
       "- Nulls: 0% across all columns (per schema)\n",
       "\n",
       "### Completeness\n",
       "- Overall completeness: 100% non-null across all columns.\n",
       "- Zero-variance columns (uninformative): \n",
       "  - is_zero__dur (unique=1; always 0)\n",
       "  - is_zero__sinpkt (unique=1; always 0)\n",
       "- Note: All one-hot columns are present; each shows 2 unique values (0/1), indicating no missing categories at schema level.\n",
       "\n",
       "Recommended actions:\n",
       "- Drop zero-variance columns: is_zero__dur, is_zero__sinpkt.\n",
       "- Confirm no hidden NaNs (e.g., inf, -inf) in floats.\n",
       "\n",
       "### Consistency\n",
       "Key cross-field rules and observations:\n",
       "- attack_cat ↔ label mapping: In sample, Normal → 0; attacks → 1. Consistent in sample; verify globally.\n",
       "- One-hot exclusivity (exactly one 1 per group):\n",
       "  - ct_state_ttl=[0,1,2,3,6,Other]\n",
       "  - ct_flw_http_mthd=[0,1,4,Other]\n",
       "  - is_ftp_login=[0,1,Other]\n",
       "  - ct_ftp_cmd=[0,1,Other]\n",
       "  Observed consistent in sample (sums to 1); verify dataset-wide.\n",
       "- Derived relationships:\n",
       "  - is_zero__{synack, ackdat, tcprtt} should reflect synack, ackdat, and synack+ackdat respectively. Sample rows appear coherent; verify systematically.\n",
       "- Train/test concatenation: ensure no data leakage in downstream modeling (e.g., re-split by time/session if applicable).\n",
       "\n",
       "Recommended actions:\n",
       "- Enforce one-hot group sum-to-one constraints; flag violations.\n",
       "- Verify label consistency: label = 0 iff attack_cat == \"Normal\", else 1.\n",
       "- Validate is_zero__tcprtt equals 1 iff synack+ackdat == 0 (within numeric tolerance).\n",
       "\n",
       "### Validity\n",
       "Schema- and domain-level checks:\n",
       "- Data types:\n",
       "  - attack_cat: object with 10 categories (expected for UNSW-NB15: Normal, Generic, Exploits, Fuzzers, DoS, Reconnaissance, Analysis, Backdoor, Shellcode, Worms).\n",
       "  - Binary/one-hot columns are int64 with values in {0,1} (unique=2), as expected.\n",
       "- Scaled features:\n",
       "  - Many originally count/time fields are float64 with negative values (standardization/robust scaling). Negative scaled values are valid; not raw units.\n",
       "  - sttl, dttl show low cardinality (13, 9 unique) consistent with limited TTL variants.\n",
       "- Outliers/heavy tails:\n",
       "  - Example: sloss shows an extreme value (46) in sample while many rows cluster near small scaled values, indicating heavy-tailed distributions. This is plausible but warrants monitoring.\n",
       "\n",
       "Recommended actions:\n",
       "- Enumerate and validate attack_cat against the expected 10-class set; correct any stray/typo categories.\n",
       "- For binary fields, assert set membership {0,1}; coerce or flag any deviations.\n",
       "- Inspect extreme values and consider robust methods (winsorization/capping) if modeling sensitivity is high.\n",
       "- Document scaling pipeline (fitted stats, version) to ensure reproducibility.\n",
       "\n",
       "### Uniqueness\n",
       "- Column-level uniqueness:\n",
       "  - No natural key column; most continuous features have high cardinality (e.g., sjit 116,392 unique), making exact row duplication unlikely.\n",
       "- Row-level duplicates: Not measurable from provided summary.\n",
       "\n",
       "Recommended actions:\n",
       "- Compute exact duplicate row rate and near-duplicate rate (hash rows or distance-based checks); drop or consolidate if >0.\n",
       "- If session/flow identifiers exist upstream, consider retaining them to support deduplication and lineage.\n",
       "\n",
       "### Accuracy\n",
       "- Ground-truth validation: Not directly assessable without external references or raw units.\n",
       "- Internal plausibility:\n",
       "  - attack_cat ↔ label mapping looks correct in the sample.\n",
       "  - is_zero__ flags appear directionally consistent with their parent variables in the sample.\n",
       "  - Some features that represent counts/times are scaled; negative values are therefore not accuracy errors.\n",
       "\n",
       "Recommended actions:\n",
       "- Back-check a sample against raw, unscaled data (if retained) to confirm scaling correctness and is_zero__ flags.\n",
       "- Quantify rate of any rule violations (e.g., one-hot sums, tcprtt construction) as a proxy for internal accuracy.\n",
       "\n",
       "### Timeliness\n",
       "- No timestamp or period-of-collection fields in schema; dataset recency and freshness cannot be assessed.\n",
       "- File represents concatenated train/test cleaned data; suitable for modeling but not for streaming freshness guarantees.\n",
       "\n",
       "Recommended actions:\n",
       "- If timeliness matters, include or join acquisition timestamps, and track file modification times and data versions.\n",
       "- Define acceptable data latency SLAs for future refreshes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported chain artifacts to: C:\\Users\\macdo\\Github\\VNFCDR-1\\Chris Code\\Generative AI Data Cleaning\\Machine Learning\\Step 4_GPT Prompting\\UNSW_NB15_Train_Test_Concatenated\\GPT_API_Exploratory_Analysis_Export.md\n",
      "\n",
      "=== Diagnostics Summary ===\n",
      "Model used: gpt-5\n",
      "Artifacts generated: ['analysis_md']\n"
     ]
    }
   ],
   "source": [
    "# Post Cleaning Data Quality Analysis\n",
    "\n",
    "import os, io, time, traceback\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "from typing import Dict, List\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. Environment / API Setup --------------------------------------------------\n",
    "# Expect an env file containing OPENAI_API_KEY\n",
    "#ENV_PATH = r\"/Users/sarahsetiawan/Desktop/VNFCDR-1/SarahCode/Generative_AI/OPENAI_API_KEY.env\"  # <-- update if needed\n",
    "ENV_PATH = r\"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 4_GPT Prompting\\\\OPENAI_API_KEY.env\"\n",
    "load_dotenv(ENV_PATH)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not found. Ensure the env file exists and key is set.\")\n",
    "\n",
    "# Instantiate OpenAI client (no custom base_url needed for official API)\n",
    "client = OpenAI(api_key=api_key)\n",
    "print(\"OpenAI API client configured successfully.\")\n",
    "\n",
    "# 2. Data Load ----------------------------------------------------------------\n",
    "#DATA_PATH = r\"/Users/sarahsetiawan/Desktop/VNFCDR-1/SarahCode/Sample_df/Code/MachineLearning/SampleData_API/CSVs/Representative_APISample_20000_2.csv\"\n",
    "DATA_PATH = r\"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 4_GPT Prompting\\\\UNSW_NB15_Train_Test_Concatenated\\\\cleaned_data.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}\")\n",
    "print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# 3. Context Preparation (Token-Optimized) ------------------------------------\n",
    "\n",
    "def build_schema_summary(dataframe: pd.DataFrame) -> str:\n",
    "    info_buf = io.StringIO()\n",
    "    dataframe.info(buf=info_buf)\n",
    "    _ = info_buf.getvalue()  # not used directly; we generate concise version below\n",
    "    rows = []\n",
    "    total = len(dataframe)\n",
    "    for col in dataframe.columns:\n",
    "        non_null = dataframe[col].notna().sum()\n",
    "        null_pct = 100 * (1 - non_null / total)\n",
    "        uniq = dataframe[col].nunique(dropna=True)\n",
    "        rows.append(f\"{col} | {dataframe[col].dtype} | {non_null} | {null_pct:.2f}% | {uniq}\")\n",
    "    header = \"Column | DType | Non-Null | Null% | Unique\\n------ | ----- | -------- | ----- | ------\"\n",
    "    concise = header + \"\\n\" + \"\\n\".join(rows)\n",
    "    return concise\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_ROWS = 10\n",
    "schema_concise = build_schema_summary(df)\n",
    "sample_md = df.sample(n=SAMPLE_ROWS, random_state=RANDOM_SEED).to_markdown(index=False)\n",
    "\n",
    "# 4. System Instruction (kept same semantic intent) ---------------------------\n",
    "system_instruction = \"\"\"\n",
    "You are an expert data scientist specializing in data cleaning and preparation for machine learning. \n",
    "Your task is to Generate a data quality analysis containing accuracy, completeness, consistency, uniqueness, validity, and timeliness on an imported file. \n",
    "\n",
    "Goals:\n",
    "1. Perform a data quality analysis on a given dataset.\n",
    "\n",
    "Constraints:\n",
    "- Use Markdown headings exactly as requested.\n",
    "\"\"\"\n",
    "\n",
    "# 5. Generation Config -------------------------------------------------------\n",
    "MODEL_NAME = \"gpt-5\"  \n",
    "MAX_COMPLETION_TOKENS = 32000  \n",
    "TEMPERATURE = 1  \n",
    "\n",
    "# 6. Retry + Wrapper (Chat Completions) --------------------------------------\n",
    "\n",
    "def call_model(messages: List[Dict[str, str]], max_completion_tokens: int = MAX_COMPLETION_TOKENS, temperature: float = TEMPERATURE):\n",
    "    \"\"\"Wrapper for OpenAI chat completion.\n",
    "\n",
    "    NOTE: Newer reasoning / frontier models use 'max_completion_tokens' instead of deprecated 'max_tokens'.\n",
    "    Some models enforce a fixed temperature (1); earlier attempt with 0.15 caused 400 error.\n",
    "    \"\"\"\n",
    "    return client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_completion_tokens=max_completion_tokens,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "def generate_with_retry(prompt_text: str, label: str, max_retries: int = 3, backoff: float = 2.0):\n",
    "    errors: List[str] = []\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            # Compose messages with system + user\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ]\n",
    "            response = call_model(messages)\n",
    "            elapsed = time.time() - start\n",
    "            text = response.choices[0].message.content.strip() if response.choices else \"\"\n",
    "            if not text:\n",
    "                raise ValueError(\"Empty response content.\")\n",
    "            print(f\"[{label}] Success attempt={attempt} time={elapsed:.2f}s\")\n",
    "            return dict(text=text, raw=response, attempts=attempt, errors=errors)\n",
    "        except Exception as e:\n",
    "            err_msg = f\"[{label}] Attempt {attempt} failed: {e}\"\n",
    "            print(err_msg)\n",
    "            errors.append(err_msg)\n",
    "            if attempt < max_retries:\n",
    "                sleep_time = backoff ** (attempt - 1)\n",
    "                time.sleep(sleep_time)\n",
    "    print(f\"[{label}] All attempts failed.\")\n",
    "    return dict(text=\"\", raw=None, attempts=max_retries, errors=errors)\n",
    "\n",
    "# 7. Prompts (EXACT TEXT PRESERVED from previous implementation) --------------\n",
    "prompt_1_analysis = f\"\"\"\n",
    "## Task 1: Data Quality Analysis\n",
    "\n",
    "You are given:\n",
    "### Concise Schema Summary\n",
    "```\n",
    "{schema_concise}\n",
    "```\n",
    "\n",
    "### Random Sample ({SAMPLE_ROWS} rows)\n",
    "```\n",
    "{sample_md}\n",
    "```\n",
    "\n",
    "### Directives\n",
    "1. Generate a data quality analysis containing accuracy, completeness, consistency, uniqueness, validity, and timeliness on the impored file at {DATA_PATH}.\n",
    "\n",
    "Output ONLY under heading:\n",
    "## Analytical Insights\n",
    "Use concise Markdown sections & tables. Avoid code.\n",
    "\"\"\"\n",
    "\n",
    "# 8. Execution Chain ---------------------------------------------------------\n",
    "chain_artifacts: Dict[str, str] = {}\n",
    "\n",
    "print(\"--- Chain Step 1: Initial Analysis ---\")\n",
    "analysis_result = generate_with_retry(prompt_1_analysis, label=\"analysis\")\n",
    "\n",
    "if not analysis_result[\"text\"]:\n",
    "    print(\"Aborting chain: analysis stage failed.\")\n",
    "else:\n",
    "    chain_artifacts[\"analysis_md\"] = analysis_result[\"text\"]\n",
    "    display(Markdown(\"### Analysis Received\"))\n",
    "    display(Markdown(chain_artifacts[\"analysis_md\"]))\n",
    "\n",
    "# 9. Export Artifacts --------------------------------------------------------\n",
    "EXPORT_DIR = os.path.dirname(DATA_PATH)\n",
    "export_path = \"C:\\\\Users\\\\macdo\\\\Github\\\\VNFCDR-1\\\\Chris Code\\\\Generative AI Data Cleaning\\\\Machine Learning\\\\Step 4_GPT Prompting\\\\UNSW_NB15_Train_Test_Concatenated\\\\GPT_API_Exploratory_Analysis_Export.md\"\n",
    "try:\n",
    "    with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        if chain_artifacts:\n",
    "            f.write(\"# EDA Prompt Chain Output (GPT)\\n\\n\")\n",
    "            for k, v in chain_artifacts.items():\n",
    "                pretty = k.replace(\"_md\", \"\").capitalize()\n",
    "                f.write(f\"\\n\\n## {pretty}\\n\\n\")\n",
    "                f.write(v.strip() + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"No artifacts generated (chain failed).\")\n",
    "    print(f\"Exported chain artifacts to: {export_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Export failed: {e}\")\n",
    "\n",
    "# 10. Diagnostic Recap -------------------------------------------------------\n",
    "\n",
    "def print_diagnostics():\n",
    "    print(\"\\n=== Diagnostics Summary ===\")\n",
    "    print(f\"Model used: {MODEL_NAME}\")\n",
    "    if analysis_result.get('errors'):\n",
    "        print(f\"Analysis errors: {len(analysis_result['errors'])}\")\n",
    "    if 'plan_result' in locals() and plan_result.get('errors'):\n",
    "        print(f\"Plan errors: {len(plan_result['errors'])}\")\n",
    "    if 'code_result' in locals() and code_result.get('errors'):\n",
    "        print(f\"Code errors: {len(code_result['errors'])}\")\n",
    "    print(\"Artifacts generated:\", list(chain_artifacts.keys()))\n",
    "\n",
    "print_diagnostics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_supervised_20250810",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
